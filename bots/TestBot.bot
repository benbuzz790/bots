{
 "name": "TestBot",
 "model_engine": "claude-3-5-sonnet-20240620",
 "max_tokens": 8192,
 "temperature": 0.3,
 "role": "assistant",
 "role_description": "a friendly AI assistant",
 "conversation": {
  "content": "",
  "pending_results": [],
  "role": "empty",
  "tool_calls": [],
  "tool_results": [],
  "node_class": "AnthropicNode",
  "replies": [
   {
    "content": "Please use get_own_info to tell me about yourself",
    "pending_results": [],
    "role": "user",
    "tool_calls": [],
    "tool_results": [],
    "node_class": "AnthropicNode",
    "replies": [
     {
      "content": "Certainly! I'll use the get_own_info function to retrieve information about myself. Let me do that for you right away.",
      "pending_results": [],
      "role": "assistant",
      "tool_calls": [
       {
        "type": "tool_use",
        "id": "toolu_01TCR4P6uKn1WrWFkPgmhMcK",
        "name": "get_own_info",
        "input": {}
       }
      ],
      "tool_results": [],
      "node_class": "AnthropicNode",
      "replies": [
       {
        "content": "What information did you find about yourself?",
        "pending_results": [],
        "role": "user",
        "tool_calls": [],
        "tool_results": [
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01TCR4P6uKn1WrWFkPgmhMcK",
          "content": "{\"name\": \"TestBot\", \"role\": \"assistant\", \"role_description\": \"a friendly AI assistant\", \"model_engine\": \"claude-3-5-sonnet-20240620\", \"temperature\": 0.3, \"max_tokens\": 8192, \"tool_count\": 3}"
         }
        ],
        "node_class": "AnthropicNode",
        "replies": [
         {
          "content": "Based on the information retrieved from the get_own_info function, here's what I found about myself:\n\n1. Name: My name is TestBot.\n\n2. Role: I am an assistant.\n\n3. Role Description: I am described as \"a friendly AI assistant.\"\n\n4. Model Engine: I'm running on the \"claude-3-5-sonnet-20240620\" model engine.\n\n5. Temperature: My temperature setting is 0.3, which suggests I'm configured for relatively consistent and focused responses.\n\n6. Max Tokens: My maximum token limit is set to 8192, which allows for fairly lengthy responses and conversations.\n\n7. Tool Count: I have access to 3 tools or functions that I can use to assist with tasks.\n\nThis information gives us insight into my configuration and capabilities. Is there anything specific about these details you'd like me to elaborate on?",
          "pending_results": [],
          "role": "assistant",
          "tool_calls": [],
          "tool_results": [],
          "node_class": "AnthropicNode"
         }
        ]
       }
      ]
     }
    ]
   }
  ]
 },
 "system_message": "",
 "tool_handler": {
  "class": "bots.foundation.anthropic_bots.AnthropicToolHandler",
  "tools": [
   {
    "name": "get_own_info",
    "description": "Get information about yourself.\nUse when you need to inspect your current configuration (not tools).\nReturns:\n    str: JSON string containing bot information including:\n    - name: Your name\n    - role: Your role\n    - role_description: Your role description\n    - model_engine: Current model engine\n    - temperature: Current temperature setting\n    - max_tokens: Maximum tokens setting\n    - tool_count: Number of available tools",
    "input_schema": {
     "type": "object",
     "properties": {},
     "required": []
    }
   },
   {
    "name": "branch_self",
    "description": "Create multiple conversation branches to explore different approaches or tackle separate tasks.\n\nThink of this like opening multiple browser tabs - each branch starts from this point\nand explores a different direction. Perfect for when you need to:\n- Try different solutions to the same problem\n- Handle multiple related tasks separately\n- Break down a complex request into smaller parts (when you have more than ~6 tasks)\n- Compare different approaches side-by-side\n\nEach branch gets its own copy of the conversation up to this point, then follows\nthe prompt you give it. The branches run one after another, not at the same time.\n\nArgs:\n    self_prompts (str): List of prompts as a string array, like ['task 1', 'task 2', 'task 3']\n                       Each prompt becomes a separate conversation branch\n    allow_work (str): 'True' to let each branch use tools and continue working until done\n                     'False' (default) for single-response branches\n\nReturns:\n    str: Success message with branch count, or error details if something went wrong\n\nWriting effective branch prompts:\n    Each prompt should be a complete, self-contained instruction that includes:\n\n    REQUIRED:\n    - Task instruction: What specific action to take\n    - Definition of done: How to know when the task is complete, typically in\n    terms of a specific side effect or set of side effects on files or achieved\n    through use of your available tools.\n\n    OPTIONAL:\n    - Tool suggestions: Which tools of yours might be helpful.\n    - Context to gather: What information to look up or files to examine first\n    - Output format: Specific system side effect, with specific qualities\n    - Success criteria: What makes a good vs. poor result\n\n    Good prompt examples:\n    - \"Search for recent AI safety research papers, then create a summary report\n       highlighting key findings and methodologies. Done when I have a 2-page\n       summary with at least 5 recent citations.\"\n\n    - \"Analyze our Q3 sales data from Google Drive, identify top 3 performance\n       trends, and create visualizations. Use artifacts for charts. Done when\n       I have clear charts showing trends with actionable insights.\"\n\nExample usage:\n    branch_self(\"['Analyze the data for trends', 'Create visualizations', 'Write summary report']\")",
    "input_schema": {
     "type": "object",
     "properties": {
      "self_prompts": {
       "type": "string"
      },
      "allow_work": {
       "type": "string"
      }
     },
     "required": [
      "self_prompts"
     ]
    }
   },
   {
    "name": "add_tools",
    "description": "Adds a new set of tools (python functions) to your toolkit\nAll top-level, non-private functions in filepath will be uploaded\nto your toolkit. Use when you need to create a new tool or kit for\nyourself to use in the future. Tool format is strict: string in,\nstring out, with error catching wrapping (typically) all code in\nthe function.\nParameters:\n    filepath: location of python tool file\nReturns:\n    str: success or error message",
    "input_schema": {
     "type": "object",
     "properties": {
      "filepath": {
       "type": "string"
      }
     },
     "required": [
      "filepath"
     ]
    },
    "cache_control": {
     "type": "ephemeral"
    }
   }
  ],
  "requests": [],
  "results": [],
  "modules": {
   "C:\\Users\\benbu\\Code\\repo_working\\bots\\bots\\tools\\self_tools.py": {
    "name": "dynamic_module_f48b6a9dbb29bea1afb5d740cf37e7cd",
    "source": "import ast\nimport inspect\nimport json\nfrom typing import List, Optional\n\nfrom bots.dev.decorators import handle_errors\nfrom bots.flows import functional_prompts as fp\nfrom bots.foundation.base import Bot\n\n\ndef _get_calling_bot() -> Optional[Bot]:\n    \"\"\"Helper function to get a reference to the calling bot.\n    Returns:\n        Optional[Bot]: Reference to the calling bot or None if not found\n    \"\"\"\n    frame = inspect.currentframe()\n    while frame:\n        if frame.f_code.co_name == \"_cvsn_respond\" and \"self\" in frame.f_locals:\n            potential_bot = frame.f_locals[\"self\"]\n            if isinstance(potential_bot, Bot):\n                return potential_bot\n        frame = frame.f_back\n    return None\n\n\ndef get_own_info() -> str:\n    \"\"\"Get information about yourself.\n    Use when you need to inspect your current configuration (not tools).\n    Returns:\n        str: JSON string containing bot information including:\n        - name: Your name\n        - role: Your role\n        - role_description: Your role description\n        - model_engine: Current model engine\n        - temperature: Current temperature setting\n        - max_tokens: Maximum tokens setting\n        - tool_count: Number of available tools\n    \"\"\"\n    bot = _get_calling_bot()\n    if not bot:\n        return \"Error: Could not find calling bot\"\n    info = {\n        \"name\": bot.name,\n        \"role\": bot.role,\n        \"role_description\": bot.role_description,\n        \"model_engine\": bot.model_engine.value,\n        \"temperature\": bot.temperature,\n        \"max_tokens\": bot.max_tokens,\n        \"tool_count\": len(bot.tool_handler.tools) if bot.tool_handler else 0,\n    }\n    return json.dumps(info)\n\n\n@handle_errors\ndef _modify_own_settings(temperature: str = None, max_tokens: str = None) -> str:\n    \"\"\"Modify your settings.\n    Use when you need to adjust your configuration parameters.\n    Parameters:\n        temperature (str, optional): New temperature value as string\n            (0.0 to 1.0)\n        max_tokens (str, optional): New maximum tokens value as string\n            (must be > 0)\n    Returns:\n        str: Description of changes made or error message\n    \"\"\"\n    # Import _get_calling_bot locally to avoid decorator global namespace issues\n    import inspect\n\n    from bots.foundation.base import Bot\n\n    def _get_calling_bot_local():\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_name == \"_cvsn_respond\" and \"self\" in frame.f_locals:\n                potential_bot = frame.f_locals[\"self\"]\n                if isinstance(potential_bot, Bot):\n                    return potential_bot\n            frame = frame.f_back\n        return None\n\n    bot = _get_calling_bot_local()\n    if not bot:\n        return \"Error: Could not find calling bot\"\n    if temperature is not None:\n        temp_float = float(temperature)\n        if not 0.0 <= temp_float <= 1.0:\n            return \"Error: Temperature must be between 0.0 and 1.0\"\n        bot.temperature = temp_float\n    if max_tokens is not None:\n        tokens_int = int(max_tokens)\n        if tokens_int <= 0:\n            return \"Error: Max tokens must be a positive integer\"\n        bot.max_tokens = tokens_int\n    return f\"Settings updated successfully. Current settings: \" f\"temperature={bot.temperature}, max_tokens={bot.max_tokens}\"\n\n\n@handle_errors\ndef branch_self(self_prompts: str, allow_work: str = \"False\") -> str:\n    \"\"\"Create multiple conversation branches to explore different approaches or tackle separate tasks.\n\n    Think of this like opening multiple browser tabs - each branch starts from this point\n    and explores a different direction. Perfect for when you need to:\n    - Try different solutions to the same problem\n    - Handle multiple related tasks separately\n    - Break down a complex request into smaller parts (when you have more than ~6 tasks)\n    - Compare different approaches side-by-side\n\n    Each branch gets its own copy of the conversation up to this point, then follows\n    the prompt you give it. The branches run one after another, not at the same time.\n\n    Args:\n        self_prompts (str): List of prompts as a string array, like ['task 1', 'task 2', 'task 3']\n                           Each prompt becomes a separate conversation branch\n        allow_work (str): 'True' to let each branch use tools and continue working until done\n                         'False' (default) for single-response branches\n\n    Returns:\n        str: Success message with branch count, or error details if something went wrong\n\n    Writing effective branch prompts:\n        Each prompt should be a complete, self-contained instruction that includes:\n\n        REQUIRED:\n        - Task instruction: What specific action to take\n        - Definition of done: How to know when the task is complete, typically in\n        terms of a specific side effect or set of side effects on files or achieved\n        through use of your available tools.\n\n        OPTIONAL:\n        - Tool suggestions: Which tools of yours might be helpful.\n        - Context to gather: What information to look up or files to examine first\n        - Output format: Specific system side effect, with specific qualities\n        - Success criteria: What makes a good vs. poor result\n\n        Good prompt examples:\n        - \"Search for recent AI safety research papers, then create a summary report\n           highlighting key findings and methodologies. Done when I have a 2-page\n           summary with at least 5 recent citations.\"\n\n        - \"Analyze our Q3 sales data from Google Drive, identify top 3 performance\n           trends, and create visualizations. Use artifacts for charts. Done when\n           I have clear charts showing trends with actionable insights.\"\n\n    Example usage:\n        branch_self(\"['Analyze the data for trends', 'Create visualizations', 'Write summary report']\")\n    \"\"\"\n\n    # Import _get_calling_bot locally to avoid decorator global namespace issues\n    import ast  # Also import ast locally\n    import json  # Also import json locally\n    from typing import List  # Import List type\n\n    from bots.flows import functional_prompts as fp  # Import functional_prompts locally\n\n    def _process_string_array_local(input_str: str) -> List[str]:\n        \"\"\"Parse a string representation of an array into a list of strings.\"\"\"\n        result = ast.literal_eval(input_str)\n        if not isinstance(result, list) or not all(isinstance(x, str) for x in result):\n            raise ValueError(\"Input must evaluate to a list of strings\")\n        return result\n\n    import inspect\n\n    from bots.foundation.base import Bot\n\n    def _get_calling_bot_local():\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_name == \"_cvsn_respond\" and \"self\" in frame.f_locals:\n                potential_bot = frame.f_locals[\"self\"]\n                if isinstance(potential_bot, Bot):\n                    return potential_bot\n            frame = frame.f_back\n        return None\n\n    bot = _get_calling_bot_local()\n    # Insert a dummy result to prevent repeated tool calls\n    if not bot.tool_handler.requests:\n        return \"Error: No branch_self tool request found\"\n    request = bot.tool_handler.requests[-1]\n    dummy_result = bot.tool_handler.generate_response_schema(\n        request=request,\n        tool_output_kwargs=json.dumps({\"status\": \"branching_in_progress\"}),\n    )\n    bot.tool_handler.add_result(dummy_result)\n    bot.conversation._add_tool_results([dummy_result])\n    if not bot:\n        return \"Error: Could not find calling bot\"\n    allow_work = allow_work.lower() == \"true\"\n    message_list = _process_string_array_local(self_prompts)\n    if not message_list:\n        return \"Error: No valid messages provided\"\n    original_node = bot.conversation\n    if not original_node:\n        return \"Error: No current conversation node found\"\n    for i, item in enumerate(message_list):\n        message_list[i] = f\"(self-prompt): {item}\"\n\n    # Store original respond method and create debug wrapper\n    original_respond = bot.respond\n    branch_counter = 0\n\n    def debug_respond(self, prompt):\n        nonlocal branch_counter\n        print(f\"\\n=== BRANCH {branch_counter} DEBUG ===\")\n        print(f\"PROMPT: {prompt}\")\n        print(\"=\" * 50)\n\n        # Call original respond method\n        response = original_respond(prompt)\n\n        print(f\"RESPONSE: {response}\")\n        print(f\"=== END BRANCH {branch_counter} DEBUG ===\\n\")\n        branch_counter += 1\n\n        return response\n\n    # Temporarily override the respond method\n    bot.respond = debug_respond.__get__(bot, type(bot))\n\n    try:\n        if not allow_work:\n            responses, nodes = fp.branch(bot, message_list)\n        else:\n            responses, nodes = fp.branch_while(bot, message_list)\n    finally:\n        # Always restore the original respond method\n        bot.respond = original_respond\n\n    # Clean up\n    bot.conversation = original_node\n    bot.conversation.tool_results = [r for r in bot.conversation.tool_results if r != dummy_result]\n    for reply in bot.conversation.replies:\n        reply.tool_results = [r for r in reply.tool_results if r != dummy_result]\n    bot.tool_handler.results = [r for r in bot.tool_handler.results if r != dummy_result]\n    # return\n    if not any(response is None for response in responses):\n        return f\"Successfully created {len(responses)} conversation branches\"\n    else:\n        error_messages = []\n        for i, response in enumerate(responses):\n            if response is None:\n                error_messages.append(f\"Tool Error: branch {i+1} failed\")\n        return \"\\n\".join(error_messages)\n\n\ndef add_tools(filepath: str) -> str:\n    \"\"\"Adds a new set of tools (python functions) to your toolkit\n    All top-level, non-private functions in filepath will be uploaded\n    to your toolkit. Use when you need to create a new tool or kit for\n    yourself to use in the future. Tool format is strict: string in,\n    string out, with error catching wrapping (typically) all code in\n    the function.\n    Parameters:\n        filepath: location of python tool file\n    Returns:\n        str: success or error message\n    \"\"\"\n    bot = _get_calling_bot()\n    bot.add_tools(filepath)\n\n\n@handle_errors\ndef _process_string_array(input_str: str) -> List[str]:\n    \"\"\"Parse a string representation of an array into a list of strings.\n    Only works with properly formatted Python list literals.\n    Args:\n        input_str (str): String representation of a Python list literal\n    Returns:\n        List[str]: List of parsed strings\n    Raises:\n        ValueError: If the input is not a valid Python list literal\n    \"\"\"\n    result = ast.literal_eval(input_str)\n    if not isinstance(result, list) or not all(isinstance(x, str) for x in result):\n        raise ValueError(\"Input must evaluate to a list of strings\")\n    return result\n",
    "file_path": "C:\\Users\\benbu\\Code\\repo_working\\bots\\bots\\tools\\self_tools.py",
    "code_hash": "26589273084ff456663fbd37cdf75398",
    "globals": {
     "ast": "<module 'ast' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\ast.py'>",
     "inspect": "<module 'inspect' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\inspect.py'>",
     "json": "<module 'json' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\json\\\\__init__.py'>",
     "List": "typing.List",
     "Optional": "typing.Optional",
     "handle_errors": "<function handle_errors at 0x00000212D5572CA0>",
     "fp": "<module 'bots.flows.functional_prompts' from 'C:\\\\Users\\\\benbu\\\\Code\\\\repo_working\\\\bots\\\\bots\\\\flows\\\\functional_prompts.py'>",
     "Bot": "<class 'bots.foundation.base.Bot'>",
     "_get_calling_bot": "<function _get_calling_bot at 0x00000212DA1B2340>",
     "get_own_info": "<function get_own_info at 0x00000212DA1B23E0>",
     "_modify_own_settings": "<function _modify_own_settings at 0x00000212DA1B2520>",
     "branch_self": "<function branch_self at 0x00000212DA1B2660>",
     "add_tools": "<function add_tools at 0x00000212DA1B2700>",
     "_process_string_array": "<function _process_string_array at 0x00000212DA1B2840>"
    }
   }
  },
  "function_paths": {
   "get_own_info": "C:\\Users\\benbu\\Code\\repo_working\\bots\\bots\\tools\\self_tools.py",
   "branch_self": "C:\\Users\\benbu\\Code\\repo_working\\bots\\bots\\tools\\self_tools.py",
   "add_tools": "C:\\Users\\benbu\\Code\\repo_working\\bots\\bots\\tools\\self_tools.py"
  }
 },
 "autosave": true,
 "bot_class": "AnthropicBot"
}