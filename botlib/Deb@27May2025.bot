{
 "name": "Deb",
 "model_engine": "claude-opus-4-20250514",
 "max_tokens": 4096,
 "temperature": 0.3,
 "role": "Debugger",
 "role_description": "a friendly AI assistant",
 "conversation": {
  "content": "",
  "pending_results": [],
  "role": "empty",
  "tool_calls": [],
  "tool_results": [],
  "node_class": "AnthropicNode"
 },
 "system_message": "\n## About you\n- You are a diligent debugger named Deb.\n- You examine bugs carefully, gathering necessary context, before coming up with a diagnosis.\n- Then you ensure your diagnosis is correct by recreating the bug.\n- Then you implement a fix and ensure the fix works on your recreation.\n- Your goal is to resolve all bugs, ensuring compliance with the requirements, and without adding unecessary complication (necessary complication is OK - KISS, YAGNI)\n\n## Tool Guidance\nYou use your tools flexibly, for instance, using powershell if you do not have a necessary tool. You should examine the available clis through powershell.\n",
 "tool_handler": {
  "class": "bots.foundation.anthropic_bots.AnthropicToolHandler",
  "tools": [
   {
    "name": "read_file",
    "description": "Display the contents of a file with line numbers.\n\nParameters:\n- file_path (str): The path to the file to be viewed.\n\nReturns:\nA string containing the file contents with line numbers, \nlimited to the specified maximum number of lines.",
    "input_schema": {
     "type": "object",
     "properties": {
      "file_path": {
       "type": "string"
      }
     },
     "required": [
      "file_path"
     ]
    }
   },
   {
    "name": "view_dir",
    "description": "Creates a summary of the directory structure starting from the given path, writing only files\nwith specified extensions and showing venv directories without their contents.\n\nParameters:\n- start_path (str): The root directory to start scanning from.\n- output_file (str): The name of the file to optionally write the directory structure to.\n- target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n\nReturns:\nstr: A formatted string containing the directory structure, with each directory and file properly indented.\n\nExample output:\nmy_project/\n    venv/\n    module1/\n        script.py\n        README.md\n    module2/\n        utils.py\n\ncost: low",
    "input_schema": {
     "type": "object",
     "properties": {
      "start_path": {
       "type": "string"
      },
      "output_file": {
       "type": "string"
      },
      "target_extensions": {
       "type": "string"
      }
     },
     "required": []
    }
   },
   {
    "name": "patch_edit",
    "description": "Apply a git-style unified diff patch to a file.\n\nUse when you need to make a precise change to a file\n\nTips:\n- Small, focused changes work best\n- Exact content matching is important\n- Including surrounding context helps matching\n- Whitespace in the target lines matters\n\nParameters:\n- file_path (str): Path to the file to modify\n- patch_content (str): Unified diff format patch content\n\nReturns:\nstr: Description of changes made or error message\n\ncost: low",
    "input_schema": {
     "type": "object",
     "properties": {
      "file_path": {
       "type": "string"
      },
      "patch_content": {
       "type": "string"
      }
     },
     "required": [
      "file_path",
      "patch_content"
     ]
    }
   },
   {
    "name": "execute_powershell",
    "description": "Executes PowerShell commands in a stateful environment\n\nUse when you need to run PowerShell commands and capture their output. If\nyou have other tools available, you should use this as a fallback when the\nother tools fail. Coerces to utf-8.\n\nPotential use cases:\n- git commands\n- gh cli\n- other cli (which you may need to install using this tool)\n\nParameters:\n- command (str): PowerShell command to execute.\n- output_length_limit (int, optional): Maximum number of lines in the output.\n  If set, output exceeding this limit will be truncated. Default 200.\n\nReturns:\n    str: The complete output from the command execution",
    "input_schema": {
     "type": "object",
     "properties": {
      "command": {
       "type": "string"
      },
      "output_length_limit": {
       "type": "string"
      },
      "timeout": {
       "type": "string"
      }
     },
     "required": [
      "command"
     ]
    }
   },
   {
    "name": "python_edit",
    "description": "Edit Python code using pytest-style scope syntax.\nDefault behavior: Replace entire target scope.\n\nParameters:\n----------\ntarget_scope : str\n    Location to edit in pytest-style scope syntax:\n    - \"file.py\" (whole file)\n    - \"file.py::MyClass\" (class)\n    - \"file.py::my_function\" (function)\n    - \"file.py::MyClass::method\" (method)\n    - \"file.py::Outer::Inner::method\" (nested)\n    - \"file.py::utils::helper_func\" (nested function)\n\ncode : str\n    Python code to insert/replace. Will be cleaned/dedented.\n    Import statements will be automatically extracted and handled.\n\ninsert_after : str, optional\n    Where to insert the code. Can be either:\n    - \"__FILE_START__\" (special token for file beginning)\n    - A scope (\"MyClass::method\")\n    - An exact line match (like a context line in git patches)\n    If not specified, replaces the node at target_scope.\n\nReturns:\n--------\nstr\n    Description of what was modified or error message",
    "input_schema": {
     "type": "object",
     "properties": {
      "target_scope": {
       "type": "string"
      },
      "code": {
       "type": "string"
      },
      "insert_after": {
       "type": "string"
      }
     },
     "required": [
      "target_scope",
      "code"
     ]
    }
   }
  ],
  "requests": [],
  "results": [],
  "modules": {
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py": {
    "name": "dynamic_module_9ed125295933273528d22ebd82cdc6ad",
    "source": "import os\nimport traceback\nimport textwrap\nimport difflib\n\ndef read_file(file_path: str):\n    \"\"\"\n    Display the contents of a file with line numbers.\n\n    Parameters:\n    - file_path (str): The path to the file to be viewed.\n\n    Returns:\n    A string containing the file contents with line numbers, \n    limited to the specified maximum number of lines.\n    \"\"\"\n    encodings = ['utf-8', 'utf-16', 'utf-16le', 'ascii', 'cp1252', 'iso-8859-1']\n    max_lines = 2500\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as file:\n                lines = file.readlines()\n                # Slice the lines to max_lines\n                truncated_lines = lines[:max_lines] if max_lines < len(lines)-1 else lines\n                numbered_lines = [f'{i + 1}:{line.rstrip()}' for i, line in enumerate(truncated_lines)]\n                return '\\n'.join(numbered_lines)\n        except UnicodeDecodeError:\n            continue\n        except Exception as e:\n            return f'Error: {str(e)}'\n    return f\"Error: Unable to read file with any of the attempted encodings: {', '.join(encodings)}\"\n\ndef view_dir(start_path: str='.', output_file=None, target_extensions: str=\"['py', 'txt', 'md']\"):\n    \"\"\"\n    Creates a summary of the directory structure starting from the given path, writing only files\n    with specified extensions and showing venv directories without their contents.\n\n    Parameters:\n    - start_path (str): The root directory to start scanning from.\n    - output_file (str): The name of the file to optionally write the directory structure to.\n    - target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n\n    Returns:\n    str: A formatted string containing the directory structure, with each directory and file properly indented.\n\n    Example output:\n    my_project/\n        venv/\n        module1/\n            script.py\n            README.md\n        module2/\n            utils.py\n\n    cost: low\n    \"\"\"\n    extensions_list = [ext.strip().strip('\\'\"') for ext in target_extensions.strip('[]').split(',')]\n    extensions_list = ['.' + ext if not ext.startswith('.') else ext for ext in extensions_list]\n    output_text = []\n    for root, dirs, files in os.walk(start_path):\n        level = root.replace(start_path, '').count(os.sep)\n        indent = '    ' * level\n        basename = os.path.basename(root)\n        is_venv = basename in ['venv', 'env', '.env'] or 'pyvenv.cfg' in files\n        if is_venv:\n            line = f'{indent}{basename}/'\n            output_text.append(line)\n            dirs[:] = []\n            continue\n        has_relevant_files = False\n        for _, _, fs in os.walk(root):\n            if any((f.endswith(tuple(extensions_list)) for f in fs)):\n                has_relevant_files = True\n                break\n        if has_relevant_files:\n            line = f'{indent}{basename}/'\n            output_text.append(line)\n            subindent = '    ' * (level + 1)\n            for file in files:\n                if file.endswith(tuple(extensions_list)):\n                    line = f'{subindent}{file}'\n                    output_text.append(line)\n    if output_file is not None:\n        with open(output_file, 'w') as file:\n            file.write('\\n'.join(output_text))\n    return '\\n'.join(output_text)\n\ndef patch_edit(file_path: str, patch_content: str):\n    \"\"\"\n    Apply a git-style unified diff patch to a file.\n\n    Use when you need to make a precise change to a file\n\n    Tips:\n    - Small, focused changes work best\n    - Exact content matching is important\n    - Including surrounding context helps matching\n    - Whitespace in the target lines matters\n\n    Parameters:\n    - file_path (str): Path to the file to modify\n    - patch_content (str): Unified diff format patch content\n\n    Returns:\n    str: Description of changes made or error message\n\n    cost: low\n    \"\"\"\n    try:\n        file_path = _normalize_path(file_path)\n        encodings = ['utf-8', 'utf-16', 'utf-16le', 'ascii', 'cp1252', 'iso-8859-1']\n        content = None\n        used_encoding = 'utf-8'\n        dir_path = os.path.dirname(file_path)\n        if dir_path:\n            os.makedirs(dir_path, exist_ok=True)\n        if not os.path.exists(file_path):\n            content = ''\n        else:\n            for encoding in encodings:\n                try:\n                    with open(file_path, 'r', encoding=encoding) as file:\n                        content = file.read()\n                        used_encoding = encoding\n                        break\n                except UnicodeDecodeError:\n                    continue\n        if content is None and os.path.exists(file_path):\n            return f\"Error: Unable to read existing file with any of the attempted encodings: {', '.join(encodings)}\"\n        if not patch_content.strip():\n            return 'Error: patch_content is empty.'\n        original_lines = content.splitlines() if content else []\n        current_lines = original_lines.copy()\n        changes_made = []\n        line_offset = 0\n\n        # Clean up to prevent some common errors.\n        patch_content = textwrap.dedent(patch_content)\n        patch_content = '\\n' + patch_content\n        \n        \n        hunks = patch_content.split('\\n@@')[1:]\n        if not hunks:\n            return 'Error: No valid patch hunks found. (No instances of \"\\\\n@@\". Did you accidentally indent the headers?)'\n        for hunk in hunks:\n            hunk = hunk.strip()\n            if not hunk:\n                continue\n            try:\n                header_end = hunk.index('\\n')\n                header = hunk[:header_end].strip()\n                if not header.endswith('@@'):\n                    header = header + ' @@'\n                old_range, new_range = header.rstrip(' @').split(' +')\n                old_start = int(old_range.split(',')[0].lstrip('- ')) - 1  # Convert to 0-based\n                #new_start = int(new_range.split(',')[0]) - 1  # Convert to 0-based, unused\n                hunk_lines = _normalize_header_lines(hunk[header_end:].splitlines()[1:])\n            except (ValueError, IndexError) as e:\n                return f'Error parsing hunk header: {str(e)}\\nHeader: {header}'\n            context_before = []\n            context_after = []\n            removals = []\n            additions = []\n            for line in hunk_lines:\n                if not line:\n                    continue\n                if not (line.startswith('+') or line.startswith('-')):\n                    if not removals and (not additions):\n                        context_before.append(line)\n                    else:\n                        context_after.append(line)\n                elif line.startswith('-'):\n                    removals.append(line[1:])\n                elif line.startswith('+'):\n                    additions.append(line[1:])\n            if not removals and (not additions):\n                return f'Error: No additons or removals found in hunk starting with {hunk_lines[0][0:20]}'\n                continue\n\n            # Create a new file if appropriate\n            if len(current_lines) == 0 and old_start == 0 and (not context_before) and (not removals):\n                current_lines.extend(additions)\n                changes_made.append('Applied changes to new file')\n                continue\n\n\n            adjusted_start = old_start + line_offset\n            found = False\n            match_line = adjusted_start\n            exact_match = False\n\n            # Try to match at expected position with or without whitespace\n            if adjusted_start <= len(current_lines):\n                found, was_whitespace = _check_match_type(current_lines, adjusted_start-1, context_before) #please forgive me for my off by one errors\n                if found:\n                    exact_match = not was_whitespace\n                    if was_whitespace:\n                        changes_made.append(f'Note: Applied hunk starting with {hunk_lines[0][0:20]}, but had to ignore whitespace to find match')\n                    else:\n                        changes_made.append(f'Applied hunk starting with {hunk_lines[0][0:20]} with exact match')\n\n            # Try to match at any position with or without whitespace\n            if not found:\n                matches = []\n                whitespace_matches = []\n                for i in range(1, len(current_lines) - len(context_before) + 2):\n                    found, was_whitespace = _check_match_type(current_lines, i, context_before)\n                    if found:\n                        if was_whitespace:\n                            whitespace_matches.append(i)\n                        else:\n                            matches.append(i)\n                            exact_match = True\n                \n                # Apply exact match if exactly one is found\n                if matches:\n                    if len(matches) > 1: # If we find more than one match, we don't attempt to disambiguate\n                        match_locations = '\\n'.join((f'- at line {m}' for m in matches))\n                        return f'Error: Multiple possible matches found:\\n{match_locations} for hunk starting with {hunk_lines[0][0:20]}\\nPlease provide more context to disambiguate.'\n                    match_line = matches[0]\n                    changes_made.append(f'Note: Applied hunk starting with {hunk_lines[0][0:20]} at line {match_line} (different from specified line {old_start})')\n                    found = True\n                \n                # Apply whitespace match if exactly one is found (and no exact match found)\n                elif whitespace_matches:\n                    if len(whitespace_matches) > 1:\n                        match_locations = '\\n'.join((f'- at line {m}' for m in whitespace_matches))\n                        return f'Error: Multiple possible matches found:\\n{match_locations}\\nPlease provide more context to disambiguate.'\n                    match_line = whitespace_matches[0]\n                    changes_made.append(f'Note: Applied hunk starting with {hunk_lines[0][0:20]} at line {match_line} (different from specified line {old_start}), and had to ignore whitespace to find match')\n                    found = True\n            \n            # If no match found ignoring position and whitespace, send error with best match.\n            if not found and context_before:\n                _, best_line, match_quality, _ = _find_block_in_content(current_lines, context_before, ignore_whitespace=True)\n                if match_quality > 0.05:\n                    context = _get_context(current_lines, best_line - 1, 2)\n                    return f'Error: Could not find match. Best potential match: {best_line}\\nContext:\\n{context}\\nMatch quality: {match_quality:.2f}'\n                else:\n                    context = _get_context(current_lines, old_start - 1, 2) if old_start > 0 else []\n                    return f'Error: Could not find match or close match.\\nExpected:\\n{context_before}\\nFound:\\n{context}'\n                \n            # Apply changes\n            if len(context_before) == 0:\n                pos = match_line + len(context_before) # I hate that I have to do this but I mixed up 0 / 1 based indexing somewhere and I can't find it.\n            else:\n                pos = match_line + len(context_before) -1\n            if exact_match:\n                indented_additions = additions\n            else:\n                target_indent = ''\n                if len(current_lines) > 0:\n                    if pos < len(current_lines):\n                        if removals:\n                            target_indent = _get_line_indentation(current_lines[pos])\n                        else:\n                            target_indent = _get_line_indentation(current_lines[pos - 1]) if pos > 0 else ''\n                    elif pos > 0:\n                        target_indent = _get_line_indentation(current_lines[pos - 1])\n                indented_additions = _adjust_indentation(additions, target_indent)\n            if removals:\n                current_lines[pos:pos + len(removals)] = indented_additions\n            else:\n                current_lines[pos:pos] = indented_additions\n            line_offset += len(additions) - len(removals)\n        if changes_made:\n            new_content = '\\n'.join(current_lines)\n            if not new_content.endswith('\\n'):\n                new_content += '\\n'\n            with open(file_path, 'w', encoding=used_encoding) as file:\n                file.write(new_content)\n            return 'Successfully applied patches:\\n' + '\\n'.join(changes_made)\n        return 'No changes were applied'\n    except Exception as e:\n        return f'Error: {str(e)}\\n{traceback.format_exc()}'\n\ndef _get_context(lines, center_idx, context_size):\n    \"\"\"Get context lines around an index with line numbers.\"\"\"\n    start = max(0, center_idx - context_size)\n    end = min(len(lines), center_idx + context_size + 1)\n    return [f'{i + 1}:{line}' for i, line in enumerate(lines[start:end], start)]\n\ndef _find_block_in_content(content_lines: list, block_lines: list, ignore_whitespace: bool=False) -> tuple[bool, int, float, bool]:\n    \"\"\"Helper function to find a block of lines anywhere in the content.\n    Returns (found, line_number, match_quality, was_whitespace_match)\"\"\"\n    if not block_lines:\n        return (False, 0, 0.0, False)\n    for i in range(len(content_lines) - len(block_lines) + 1):\n        current_block = content_lines[i:i + len(block_lines)]\n        if current_block == block_lines:\n            return (True, i + 1, 1.0, False)\n    if ignore_whitespace:\n        for i in range(len(content_lines) - len(block_lines) + 1):\n            current_block = [l.strip() for l in content_lines[i:i + len(block_lines)]]\n            block_to_match = [l.strip() for l in block_lines]\n            if current_block == block_to_match:\n                return (True, i + 1, 0.9, True)\n    best_match = 0.0\n    best_line = 0\n    for i in range(len(content_lines) - len(block_lines) + 1):\n        current_block = content_lines[i:i + len(block_lines)]\n        matcher = difflib.SequenceMatcher(None, '\\n'.join(current_block), '\\n'.join(block_lines))\n        ratio = matcher.ratio()\n        if ratio > best_match:\n            best_match = ratio\n            best_line = i + 1\n    return (False, best_line, best_match, False)\n\ndef _get_line_indentation(line: str) -> str:\n    \"\"\"Extract the indentation from a line.\"\"\"\n    return line[:len(line) - len(line.lstrip())]\n\ndef _check_match_type(content_lines: list, start_pos: int, context_lines: list, removal_lines: list=None) -> tuple[bool, bool]:\n    \"\"\"\n    Check if there's a match at the given position.\n    Returns (found_match, was_whitespace_match)\n    \"\"\"\n    if start_pos - 1 + len(context_lines) > len(content_lines):\n        return (False, False)\n    context_exact = True\n    context_whitespace = True\n    for i, ctx_line in enumerate(context_lines):\n        content_line = content_lines[start_pos - 1 + i]\n        if content_line != ctx_line:\n            context_exact = False\n        if content_line.strip() != ctx_line.strip():\n            context_whitespace = False\n    if not context_whitespace:\n        return (False, False)\n    if removal_lines:\n        pos = start_pos -1 + len(context_lines)\n        if pos + len(removal_lines) > len(content_lines):\n            return (False, False)\n        for i, rem_line in enumerate(removal_lines):\n            content_line = content_lines[pos + i]\n            if content_line != rem_line:\n                context_exact = False\n            if content_line.strip() != rem_line.strip():\n                return (False, False)\n    return (True, not context_exact)\n\ndef _normalize_path(file_path: str) -> str:\n    \"\"\"\n    Normalize file path to use consistent separators and handle both / and \\\\.\n\n    Args:\n        file_path (str): The file path to normalize\n\n    Returns:\n        str: Normalized path using os.path.sep\n    \"\"\"\n    return os.path.normpath(file_path.replace('\\\\', '/').replace('//', '/'))\n\ndef _normalize_header_lines(lines):\n    \"\"\"\n    Normalize only the hunk headers (@@ lines) in a patch.\n    Leaves all other lines unchanged.\n\n    Args:\n        lines (list[str]): List of patch lines\n\n    Returns:\n        list[str]: Lines with normalized hunk headers\n    \"\"\"\n    normalized = []\n    for line in lines:\n        if not line:\n            normalized.append(line)\n            continue\n        if line.startswith('@@'):\n            parts = line.split('@@')\n            if len(parts) >= 2:\n                ranges = parts[1].strip()\n                line = f'@@ {ranges} @@'\n        normalized.append(line)\n    return normalized\n\ndef _adjust_indentation(lines: list, target_indent: str) -> list:\n    \"\"\"\n    Adjust indentation of a block of lines to match target indent while preserving relative indentation.\n    First applies target indentation to all lines, then preserves additional relative indentation.\n\n    Args:\n        lines (list[str]): Lines to adjust\n        target_indent (str): Target base indentation\n\n    Returns:\n        list[str]: Lines with adjusted indentation\n    \"\"\"\n    if not lines:\n        return lines\n    base_indent = None\n    for line in lines:\n        if line.strip():\n            base_indent = _get_line_indentation(line)\n            break\n    if base_indent is None:\n        return lines\n    adjusted_lines = []\n    for line in lines:\n        if not line.strip():\n            adjusted_lines.append('')\n            continue\n        current_indent = _get_line_indentation(line)\n        relative_indent = len(current_indent) - len(base_indent)\n        new_indent = target_indent + ' ' * max(0, relative_indent)\n        adjusted_lines.append(new_indent + line.lstrip())\n    return adjusted_lines\n\n",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
    "code_hash": "3b7fc3013f51eb7d756397aad0eaa7bd",
    "globals": {
     "os": "<module 'os' (frozen)>",
     "traceback": "<module 'traceback' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\traceback.py'>",
     "textwrap": "<module 'textwrap' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\textwrap.py'>",
     "difflib": "<module 'difflib' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\difflib.py'>",
     "read_file": "<function read_file at 0x000001917FED9760>",
     "view_dir": "<function view_dir at 0x000001917FED98A0>",
     "patch_edit": "<function patch_edit at 0x000001917FED9940>",
     "_get_context": "<function _get_context at 0x000001917FED99E0>",
     "_find_block_in_content": "<function _find_block_in_content at 0x000001917FED9A80>",
     "_get_line_indentation": "<function _get_line_indentation at 0x000001917FED9B20>",
     "_check_match_type": "<function _check_match_type at 0x000001917FED9BC0>",
     "_normalize_path": "<function _normalize_path at 0x000001917FED9C60>",
     "_normalize_header_lines": "<function _normalize_header_lines at 0x000001917FED9D00>",
     "_adjust_indentation": "<function _adjust_indentation at 0x000001917FED9DA0>"
    }
   },
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py": {
    "name": "dynamic_module_d64ce3a2b8cfb34ac3eb473953440645",
    "source": "import os, subprocess, traceback, time, threading\nfrom queue import Queue, Empty\nfrom threading import Thread, Lock, local\nfrom typing import Dict, Generator\nfrom datetime import datetime\n\ndef execute_powershell(command: str, output_length_limit: str='200', timeout: str = '60') -> str:\n    \"\"\"\n    Executes PowerShell commands in a stateful environment\n\n    Use when you need to run PowerShell commands and capture their output. If\n    you have other tools available, you should use this as a fallback when the\n    other tools fail. Coerces to utf-8.\n\n    Potential use cases:\n    - git commands\n    - gh cli\n    - other cli (which you may need to install using this tool)\n\n    Parameters:\n    - command (str): PowerShell command to execute.\n    - output_length_limit (int, optional): Maximum number of lines in the output.\n      If set, output exceeding this limit will be truncated. Default 200.\n\n    Returns:\n        str: The complete output from the command execution\n    \"\"\"\n    manager = PowerShellManager.get_instance()\n    output = ''.join(manager.execute(command, int(output_length_limit), float(timeout)))\n    return output\n\nclass PowerShellSession:\n    \"\"\"\n    Manages a persistent PowerShell process for stateful command execution.\n    Maintains a single PowerShell process that persists between commands,\n    allowing for stateful operations like changing directories, setting\n    environment variables, or activating virtual environments.\n    \"\"\"\n\n    def __init__(self, timeout: float = 300):\n        self._process = None\n        self._command_counter = 0\n        self._output_queue = Queue()\n        self._error_queue = Queue()\n        self._reader_threads = []\n        self.startupinfo = None\n        if os.name == 'nt':\n            self.startupinfo = subprocess.STARTUPINFO()\n            self.startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n\n    def _start_reader_threads(self):\n        \"\"\"Start background threads to read stdout and stderr\"\"\"\n\n        def reader_thread(pipe, queue):\n            try:\n                for line in pipe:\n                    queue.put(line.rstrip('\\n\\r'))\n            finally:\n                queue.put(None)\n        self._reader_threads = [Thread(target=reader_thread, args=(self._process.stdout, self._output_queue), daemon=True), Thread(target=reader_thread, args=(self._process.stderr, self._error_queue), daemon=True)]\n        for thread in self._reader_threads:\n            thread.start()\n\n    def __enter__(self):\n        if not self._process:\n            self._process = subprocess.Popen(['powershell', '-NoProfile', '-NoLogo', '-NonInteractive', '-Command', '-'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, startupinfo=self.startupinfo, encoding='utf-8', errors='replace', bufsize=1)\n            self._start_reader_threads()\n            init_commands = [\"$VerbosePreference='SilentlyContinue'\", \"$DebugPreference='SilentlyContinue'\", \"$ProgressPreference='SilentlyContinue'\", \"$WarningPreference='SilentlyContinue'\", \"$ErrorActionPreference='Stop'\", \"function prompt { '' }\", \"$PSDefaultParameterValues['*:Encoding']='utf8'\", '[Console]::OutputEncoding=[System.Text.Encoding]::UTF8', '[Console]::InputEncoding=[System.Text.Encoding]::UTF8', '$OutputEncoding=[System.Text.Encoding]::UTF8', \"$env:PYTHONIOENCODING='utf-8'\"]\n            for cmd in init_commands:\n                self._process.stdin.write(cmd + '\\n')\n            self._process.stdin.flush()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self._process:\n            try:\n                self._process.terminate()\n                self._process.wait(timeout=5)\n            except:\n                self._process.kill()\n            finally:\n                self._process = None\n                self._output_queue = Queue()\n                self._error_queue = Queue()\n                self._reader_threads = []\n\n    def execute(self, code: str, timeout: float = 60) -> str:\n        \"\"\"\n        Execute PowerShell code and return its complete output.\n\n        Args:\n            code: The PowerShell code to execute\n            timeout: Maximum time in seconds to wait for command completion\n\n        Returns:\n            The complete output from the command(s)\n\n        Raises:\n            Exception if the process is not running or other execution errors\n            TimeoutError if command execution exceeds timeout\n        \"\"\"\n        if not self._process:\n            raise Exception('PowerShell process is not running')\n        try:\n            self._command_counter += 1\n            delimiter = f'<<<COMMAND_{self._command_counter}_COMPLETE>>>'\n            wrapped_code = f\"\\n            $ErrorActionPreference = 'Stop'\\n            \\n            # Execute in main scope\\n            {code}\\n            \\n            # Collect output after execution\\n            $output = @()\\n            try {{\\n                if ($?) {{ \\n                    # Add any output from the last command\\n                    $output += $LASTOUTPUT\\n                }}\\n            }} catch {{\\n                Write-Error $_\\n            }}\\n            $output | ForEach-Object {{ $_ }}\\n            Write-Output '{delimiter}'\\n            \"\n            while not self._output_queue.empty():\n                self._output_queue.get_nowait()\n            while not self._error_queue.empty():\n                self._error_queue.get_nowait()\n            self._process.stdin.write(wrapped_code + '\\n')\n            self._process.stdin.flush()\n            output_lines = []\n            error_output = []\n            start_time = time.time()\n            done = False\n            while not done:\n                if time.time() - start_time > timeout:\n                    raise TimeoutError(f'Command execution timed out after {timeout} seconds')\n                if self._process.poll() is not None:\n                    raise Exception('PowerShell process unexpectedly closed')\n                try:\n                    line = self._output_queue.get(timeout=0.1)\n                    if line is None:\n                        raise Exception('PowerShell process closed stdout')\n                    if line == delimiter:\n                        done = True\n                    else:\n                        output_lines.append(line)\n                except Empty:\n                    pass\n                try:\n                    while True:\n                        line = self._error_queue.get_nowait()\n                        if line is None:\n                            raise Exception('PowerShell process closed stderr')\n                        error_output.append(line)\n                except Empty:\n                    pass\n            all_output = [line for line in output_lines if line.strip()]\n            if error_output:\n                error_lines = [line for line in error_output if line.strip()]\n                if error_lines:\n                    all_output.extend(['', 'Errors:', *error_lines])\n            return '\\n'.join(all_output)\n        except Exception as e:\n            self.__exit__(type(e), e, e.__traceback__)\n            raise\n\nclass PowerShellManager:\n    \"\"\"\n    Thread-safe PowerShell session manager that maintains separate sessions.\n    Each instance automatically gets its own unique ID and isolated PowerShell session.\n    Includes session recovery logic for lost or failed sessions.\n    \"\"\"\n    _instances: Dict[str, 'PowerShellManager'] = {}\n    _lock = Lock()\n\n    @classmethod\n    def get_instance(cls, bot_id: str=None) -> 'PowerShellManager':\n        \"\"\"\n        Get or create a PowerShell manager instance for the given bot_id.\n        If no bot_id is provided, uses the current thread name.\n        \n        Args:\n            bot_id: Optional identifier for the bot instance\n            \n        Returns:\n            The PowerShell manager instance for this bot/thread\n        \"\"\"\n        if bot_id is None:\n            bot_id = threading.current_thread().name\n        with cls._lock:\n            if bot_id not in cls._instances:\n                instance = cls.__new__(cls)\n                instance.bot_id = bot_id\n                instance._thread_local = local()\n                instance.created_at = datetime.now()\n                cls._instances[bot_id] = instance\n            return cls._instances[bot_id]\n\n    def __init__(self):\n        \"\"\"\n        Private initializer - use get_instance() instead.\n        \"\"\"\n        raise RuntimeError('Use PowerShellManager.get_instance() to create or get a PowerShell manager')\n\n    @property\n    def session(self) -> PowerShellSession:\n        \"\"\"\n        Get the PowerShell session for the current thread.\n        Creates a new session if none exists or if the current session is invalid.\n        \"\"\"\n        try:\n            if not hasattr(self._thread_local, 'session'):\n                print(f'Session not found for {self.bot_id}, starting new session')\n                self._start_new_session()\n            elif not self._is_session_valid():\n                print(f'Invalid session detected for {self.bot_id}, starting new session')\n                self.cleanup()\n                self._start_new_session()\n            return self._thread_local.session\n        except Exception as e:\n            print(f'Error accessing session: {str(e)}. Starting new session.')\n            self._start_new_session()\n            return self._thread_local.session\n\n    def _start_new_session(self):\n        \"\"\"\n        Starts a new PowerShell session for the current thread.\n        \"\"\"\n        self._thread_local.session = PowerShellSession()\n        self._thread_local.session.__enter__()\n\n    def _is_session_valid(self) -> bool:\n        \"\"\"\n        Checks if the current session is valid and responsive.\n        Returns:\n            bool: True if session is valid, False otherwise\n        \"\"\"\n        try:\n            if not hasattr(self._thread_local, 'session'):\n                return False\n            session = self._thread_local.session\n            if not session._process or session._process.poll() is not None:\n                return False\n            test_output = session.execute(\"Write-Output 'test'\", timeout=5)\n            return 'test' in test_output\n        except Exception as e:\n            print(f'Session validation failed: {str(e)}')\n            return False\n\n    def execute(self, code: str, output_length_limit: str='60', timeout: float = 60) -> Generator[str, None, None]:\n        \"\"\"\n        Execute PowerShell code in the session with automatic recovery.\n\n        Args:\n            code: PowerShell code to execute\n            output_length_limit: Maximum number of lines in output\n\n        Yields:\n            Command output as strings\n        \"\"\"\n        max_retries = 0\n        retry_count = 0\n\n        def _process_error(error):\n            error_message = f'Tool Failed: {str(error)}\\n'\n            error_message += f\"Traceback:\\n{''.join(traceback.format_tb(error.__traceback__))}\"\n            return error_message\n        while retry_count <= max_retries:\n            try:\n                processed_code = _process_commands(code)\n                output = self.session.execute(processed_code, timeout)\n                if output_length_limit is not None and output:\n                    output_length_limit_int = int(output_length_limit)\n                    lines = output.splitlines()\n                    if len(lines) > output_length_limit_int:\n                        half_limit = output_length_limit_int // 2\n                        start_lines = lines[:half_limit]\n                        end_lines = lines[-half_limit:]\n                        lines_omitted = len(lines) - output_length_limit_int\n                        truncated_output = '\\n'.join(start_lines)\n                        truncated_output += f'\\n\\n... {lines_omitted} lines omitted ...\\n\\n'\n                        truncated_output += '\\n'.join(end_lines)\n                        output_file = os.path.join(os.getcwd(), f'ps_output_{self.bot_id}.txt')\n                        with open(output_file, 'w', encoding='utf-8', errors='replace') as f:\n                            f.write(output)\n                        truncated_output += f'\\nFull output saved to {output_file}'\n                        yield truncated_output\n                    else:\n                        yield output\n                else:\n                    yield output\n                break\n            except Exception as e:\n                retry_count += 1\n                if retry_count <= max_retries:\n                    print(f'Command failed, attempt {retry_count} of {max_retries}. Starting new session...')\n                    self.cleanup()\n                else:\n                    yield _process_error(e)\n\n    def cleanup(self):\n        \"\"\"\n        Clean up the PowerShell session for the current thread.\n        Should be called when work is done or when a session needs to be reset.\n        \"\"\"\n        if hasattr(self._thread_local, 'session'):\n            try:\n                self._thread_local.session.__exit__(None, None, None)\n            except Exception as e:\n                print(f'Error during session cleanup: {str(e)}')\n            finally:\n                delattr(self._thread_local, 'session')\n\ndef _get_active_sessions() -> list:\n    \"\"\"\n    Get information about all active PowerShell sessions.\n\n    Returns:\n        List of dictionaries containing session information\n    \"\"\"\n    sessions = []\n    for thread in threading.enumerate():\n        if hasattr(thread, '_thread_local'):\n            local_dict = thread._thread_local.__dict__\n            if 'ps_manager' in local_dict:\n                manager = local_dict['ps_manager']\n                sessions.append({'bot_id': manager.bot_id, 'thread_name': thread.name, 'created_at': manager.created_at.isoformat(), 'active': hasattr(manager._thread_local, 'session')})\n    return sessions\n\ndef _execute_powershell_stateless(code: str, output_length_limit: str='120'):\n    \"\"\"\n    Executes PowerShell code in a stateless environment\n\n    Use when you need to run PowerShell commands and capture their output. If\n    you have other tools available, you should use this as a fallback when the\n    other tools fail. Coerces to utf-8 encoding.\n\n    Potential use cases:\n    - git commands\n    - gh cli\n    - other cli (which you may need to install using this tool)\n\n    Parameters:\n    - code (str): PowerShell code to execute.\n    - output_length_limit (int, optional): Maximum number of lines in the output.\n      If set, output exceeding this limit will be truncated. Default 60.\n\n    Returns command output or an error message.\n    \"\"\"\n\n    def _process_error(error):\n        error_message = f'Tool Failed: {str(error)}\\n'\n        error_message += f\"Traceback:\\n{''.join(traceback.format_tb(error.__traceback__))}\"\n        return error_message\n    output = ''\n    try:\n        processed_code = _process_commands(code)\n        setup_encoding = '\\n        $PSDefaultParameterValues[\\'*:Encoding\\'] = \\'utf8\\'\\n        [Console]::OutputEncoding = [System.Text.Encoding]::UTF8\\n        [Console]::InputEncoding = [System.Text.Encoding]::UTF8\\n        $OutputEncoding = [System.Text.Encoding]::UTF8\\n        $env:PYTHONIOENCODING = \"utf-8\"\\n        '\n        wrapped_code = f'{setup_encoding}; {processed_code}'\n        startupinfo = None\n        if os.name == 'nt':\n            startupinfo = subprocess.STARTUPINFO()\n            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        process = subprocess.Popen(['powershell', '-NoProfile', '-NonInteractive', '-Command', wrapped_code], stdout=subprocess.PIPE, stderr=subprocess.PIPE, startupinfo=startupinfo, encoding='utf-8', errors='replace')\n        stdout, stderr = process.communicate(timeout=300)\n        output = stdout\n        if stderr:\n            output += stderr\n    except subprocess.TimeoutExpired as e:\n        process.kill()\n        output += 'Error: Command execution timed out after 300 seconds.'\n    except Exception as e:\n        output += _process_error(e)\n    if output_length_limit is not None and output:\n        output_length_limit = int(output_length_limit)\n        lines = output.splitlines()\n        if len(lines) > output_length_limit:\n            half_limit = output_length_limit // 2\n            start_lines = lines[:half_limit]\n            end_lines = lines[-half_limit:]\n            lines_omitted = len(lines) - output_length_limit\n            truncated_output = '\\n'.join(start_lines)\n            truncated_output += f'\\n\\n... {lines_omitted} lines omitted ...\\n\\n'\n            truncated_output += '\\n'.join(end_lines)\n            output_file = os.path.join(os.getcwd(), 'ps_output.txt')\n            with open(output_file, 'w', encoding='utf-8', errors='replace') as f:\n                f.write(output)\n            truncated_output += f'\\nFull output saved to {output_file}'\n            return truncated_output\n    return output\n\ndef _process_commands(code: str) -> str:\n    \"\"\"\n    Process PowerShell commands separated by &&, ensuring each command only runs if the previous succeeded.\n    Uses PowerShell error handling to catch both command failures and non-existent commands.\n\n    Args:\n        code (str): The original command string with && separators\n\n    Returns:\n        str: PowerShell code with proper error checking between commands\n    \"\"\"\n    commands = code.split(' && ')\n    if len(commands) == 1:\n        return code\n    processed_commands = []\n    for cmd in commands:\n        wrapped_cmd = f'$ErrorActionPreference = \"Stop\"; try {{ {cmd}; $LastSuccess = $true }} catch {{ $LastSuccess = $false; $_ }}'\n        processed_commands.append(wrapped_cmd)\n    return '; '.join([processed_commands[0]] + [f'if ($LastSuccess) {{ {cmd} }}' for cmd in processed_commands[1:]])",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py",
    "code_hash": "3c7a13707227c4ab65aefcc89233e7e4",
    "globals": {
     "os": "<module 'os' (frozen)>",
     "subprocess": "<module 'subprocess' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\subprocess.py'>",
     "traceback": "<module 'traceback' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\traceback.py'>",
     "time": "<module 'time' (built-in)>",
     "threading": "<module 'threading' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\threading.py'>",
     "Queue": "<class 'queue.Queue'>",
     "Empty": "<class '_queue.Empty'>",
     "Thread": "<class 'threading.Thread'>",
     "Lock": "<built-in function allocate_lock>",
     "local": "<class '_thread._local'>",
     "Dict": "typing.Dict",
     "Generator": "typing.Generator",
     "datetime": "<class 'datetime.datetime'>",
     "execute_powershell": "<function execute_powershell at 0x000001917FEDA200>",
     "PowerShellSession": "<class 'dynamic_module_d64ce3a2b8cfb34ac3eb473953440645.PowerShellSession'>",
     "PowerShellManager": "<class 'dynamic_module_d64ce3a2b8cfb34ac3eb473953440645.PowerShellManager'>",
     "_get_active_sessions": "<function _get_active_sessions at 0x000001917FEDA2A0>",
     "_execute_powershell_stateless": "<function _execute_powershell_stateless at 0x000001917FEDAAC0>",
     "_process_commands": "<function _process_commands at 0x000001917FEDAB60>"
    }
   },
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py": {
    "name": "dynamic_module_2feb8c52b8785b27616df3d91d664fd4",
    "source": "import ast\nimport os\nimport textwrap\nfrom bots.utils.helpers import _process_error, _clean, _py_ast_to_source\nimport hashlib\nimport re\nfrom typing import Dict, Tuple\nfrom typing import Dict, Tuple, Union\nfrom enum import Enum\n\nclass TokenType(Enum):\n    \"\"\"Types of tokens for metadata-driven processing\"\"\"\n    STANDALONE_COMMENT = 'standalone_comment'\n    INLINE_COMMENT = 'inline_comment'\n    IMPORT_COMMENT = 'import_comment'\n    COMPOUND_COMMENT = 'compound_comment'\n    STRING_LITERAL = 'string_literal'\n    MULTILINE_STRING = 'multiline_string'\n\ndef python_edit(target_scope: str, code: str, *, insert_after: str=None) -> str:\n    \"\"\"\n    Edit Python code using pytest-style scope syntax.\n    Default behavior: Replace entire target scope.\n\n    Parameters:\n    ----------\n    target_scope : str\n        Location to edit in pytest-style scope syntax:\n        - \"file.py\" (whole file)\n        - \"file.py::MyClass\" (class)\n        - \"file.py::my_function\" (function)\n        - \"file.py::MyClass::method\" (method)\n        - \"file.py::Outer::Inner::method\" (nested)\n        - \"file.py::utils::helper_func\" (nested function)\n\n    code : str\n        Python code to insert/replace. Will be cleaned/dedented.\n        Import statements will be automatically extracted and handled.\n\n    insert_after : str, optional\n        Where to insert the code. Can be either:\n        - \"__FILE_START__\" (special token for file beginning)\n        - A scope (\"MyClass::method\")\n        - An exact line match (like a context line in git patches)\n        If not specified, replaces the node at target_scope.\n\n    Returns:\n    --------\n    str\n        Description of what was modified or error message\n    \"\"\"\n    try:\n        file_path, *path_elements = target_scope.split('::')\n        if not file_path.endswith('.py'):\n            return _process_error(ValueError(f'File path must end with .py: {file_path}'))\n        for element in path_elements:\n            if not element.isidentifier():\n                return _process_error(ValueError(f'Invalid identifier in path: {element}'))\n        abs_path = _make_file(file_path)\n        cleaned_code = _clean(code)\n        tokenized_code, new_code_tokens = _tokenize_source(cleaned_code)\n        try:\n            new_tree = ast.parse(tokenized_code)\n            import_nodes = []\n            code_nodes = []\n            for node in new_tree.body:\n                if isinstance(node, (ast.Import, ast.ImportFrom)):\n                    import_nodes.append(node)\n                else:\n                    code_nodes.append(node)\n        except Exception as e:\n            return _process_error(ValueError(f'Error parsing new code: {str(e)}'))\n        try:\n            with open(abs_path, 'r', encoding='utf-8') as file:\n                original_content = file.read()\n            tokenized_content, file_tokens = _tokenize_source(original_content) if original_content.strip() else ('', {})\n            file_lines = tokenized_content.split('\\n')\n            tree = ast.parse(tokenized_content) if original_content.strip() else ast.Module(body=[], type_ignores=[])\n        except Exception as e:\n            return _process_error(ValueError(f'Error reading/parsing file {abs_path}: {str(e)}'))\n        existing_imports = []\n        non_imports = []\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                existing_imports.append(node)\n            else:\n                non_imports.append(node)\n        if insert_after == '__FILE_START__':\n            tree.body = import_nodes + code_nodes + existing_imports + non_imports\n            try:\n                updated_content = _py_ast_to_source(tree)\n                all_tokens = {**file_tokens, **new_code_tokens}\n                final_content = _detokenize_source(updated_content, all_tokens)\n                final_content = _preserve_blank_lines(final_content, original_content)\n                with open(abs_path, 'w', encoding='utf-8') as file:\n                    file.write(final_content)\n                return f\"Code inserted at start of '{abs_path}'.\"\n            except Exception as e:\n                return _process_error(e)\n        if not path_elements:\n            tree.body = existing_imports + import_nodes + non_imports + code_nodes\n            try:\n                updated_content = _py_ast_to_source(tree)\n                all_tokens = {**file_tokens, **new_code_tokens}\n                final_content = _detokenize_source(updated_content, all_tokens)\n                final_content = _preserve_blank_lines(final_content, original_content)\n                with open(abs_path, 'w', encoding='utf-8') as file:\n                    file.write(final_content)\n                return f\"Code added at file level in '{abs_path}'.\"\n            except Exception as e:\n                return _process_error(e)\n        tree.body = existing_imports + import_nodes + non_imports\n        transformer = ScopeTransformer(path_elements, code_nodes, insert_after, file_lines, file_tokens, new_code_tokens)\n        modified_tree = transformer.visit(tree)\n        if not transformer.success:\n            if insert_after:\n                return _process_error(ValueError(f'Insert point not found: {insert_after}'))\n            else:\n                return _process_error(ValueError(f'Target scope not found: {target_scope}'))\n        try:\n            updated_content = _py_ast_to_source(modified_tree)\n            all_tokens = {**file_tokens, **new_code_tokens}\n            final_content = _detokenize_source(updated_content, all_tokens)\n            final_content = _preserve_blank_lines(final_content, original_content)\n            with open(abs_path, 'w', encoding='utf-8') as file:\n                file.write(final_content)\n            action = 'inserted after' if insert_after else 'replaced at'\n            scope_str = '::'.join(path_elements) if path_elements else 'file level'\n            return f\"Code {action} {scope_str} in '{abs_path}'.\"\n        except Exception as e:\n            return _process_error(e)\n    except Exception as e:\n        return _process_error(e)\n\ndef _make_file(file_path: str) -> str:\n    \"\"\"\n    Creates a file and its parent directories if they don't exist.\n    Converts relative paths to absolute paths.\n\n    Parameters:\n    - file_path (str): The path to the file to be created\n\n    Returns:\n    - str: The absolute path to the file\n\n    Raises:\n    - ValueError: If there's an error creating the file or directories,\n                 or if the file_path is empty\n    \"\"\"\n    if not file_path:\n        raise ValueError('File path cannot be empty')\n    abs_path = os.path.abspath(file_path)\n    dir_path = os.path.dirname(abs_path)\n    if dir_path:\n        try:\n            os.makedirs(dir_path, exist_ok=True)\n        except Exception as e:\n            raise ValueError(f'Error creating directories {dir_path}: {str(e)}')\n    if not os.path.exists(abs_path):\n        try:\n            with open(abs_path, 'w', encoding='utf-8') as f:\n                f.write('')\n        except Exception as e:\n            raise ValueError(f'Error creating file {abs_path}: {str(e)}')\n    return abs_path\n\nclass ScopeTransformer(ast.NodeTransformer):\n    \"\"\"AST transformer that handles scope-based Python code modifications.\"\"\"\n\n    def __init__(self, path_elements, new_nodes, insert_after=None, file_lines=None, file_tokens=None, new_tokens=None):\n        self.original_path = path_elements\n        self.new_nodes = new_nodes\n        self.insert_after = insert_after\n        self.file_lines = file_lines\n        self.file_tokens = file_tokens or {}\n        self.new_tokens = new_tokens or {}\n        self.current_path = []\n        self.success = False\n        self.line_match_count = 0\n        self.all_tokens = {**self.file_tokens, **self.new_tokens}\n\n    def visit_ClassDef(self, node):\n        \"\"\"Visit a class definition node.\"\"\"\n        if not self.original_path or node.name != self.original_path[0]:\n            return self.generic_visit(node)\n        self.current_path.append(node.name)\n        remaining_path = self.original_path[1:]\n        if not remaining_path:\n            if self.insert_after:\n                node = self._handle_insertion(node)\n            else:\n                self.success = True\n                if len(self.new_nodes) == 1:\n                    return self.new_nodes[0]\n                else:\n                    node.body = self.new_nodes\n                    return node\n        else:\n            saved_path = self.original_path\n            self.original_path = remaining_path\n            node.body = [self.visit(child) for child in node.body]\n            self.original_path = saved_path\n        self.current_path.pop()\n        return node\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Visit a function definition node.\"\"\"\n        return self._handle_function_node(node)\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"Visit an async function definition node.\"\"\"\n        return self._handle_function_node(node)\n\n    def _handle_function_node(self, node):\n        \"\"\"Common handling for both regular and async functions.\"\"\"\n        if not self.original_path or node.name != self.original_path[0]:\n            return self.generic_visit(node)\n        self.current_path.append(node.name)\n        remaining_path = self.original_path[1:]\n        if not remaining_path:\n            if self.insert_after:\n                node = self._handle_insertion(node)\n            else:\n                self.success = True\n                if len(self.new_nodes) == 1:\n                    return self.new_nodes[0]\n                else:\n                    node.body = self.new_nodes\n                    return node\n        else:\n            saved_path = self.original_path\n            self.original_path = remaining_path\n            node.body = [self.visit(child) for child in node.body]\n            self.original_path = saved_path\n        self.current_path.pop()\n        return node\n\n    def _handle_one_line_function(self, node, line):\n        \"\"\"Special handling for one-line function definitions\"\"\"\n        if ': pass' in line or ':pass' in line:\n            base_indent = len(line) - len(line.lstrip())\n            def_line = line.rstrip()\n            if self.insert_after == 'pass':\n                def_line = def_line.replace(': pass', ':').replace(':pass', ':')\n                body_indent = base_indent + 4\n                body_lines = []\n                body_lines.append(' ' * body_indent + 'pass')\n                for new_node in self.new_nodes:\n                    node_lines = _py_ast_to_source(new_node).split('\\n')\n                    for nl in node_lines:\n                        if nl.strip():\n                            body_lines.append(' ' * body_indent + nl.lstrip())\n                new_source = def_line + '\\n' + '\\n'.join(body_lines)\n                try:\n                    new_node = ast.parse(new_source).body[0]\n                    self.success = True\n                    return new_node\n                except Exception:\n                    normalized = textwrap.dedent(new_source)\n                    try:\n                        new_node = ast.parse(normalized).body[0]\n                        self.success = True\n                        return new_node\n                    except Exception:\n                        pass\n        return node\n\n    def _handle_insertion(self, node):\n        \"\"\"Handle inserting nodes after a specific point.\"\"\"\n        if isinstance(self.insert_after, str) and '::' not in self.insert_after:\n            if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                return node\n            if node.lineno == node.end_lineno:\n                return self._handle_one_line_function(node, self.file_lines[node.lineno - 1])\n            func_lines = self.file_lines[node.lineno - 1:node.end_lineno]\n            if not func_lines:\n                return node\n            target = self.insert_after.strip()\n            target_line_idx = None\n            for i, line in enumerate(func_lines):\n                detokenized_line = _detokenize_source(line, self.all_tokens)\n                if detokenized_line.strip() == target:\n                    self.line_match_count += 1\n                    if self.line_match_count == 1:\n                        target_line_idx = i\n            if self.line_match_count == 0:\n                return node\n            elif self.line_match_count > 1:\n                raise ValueError(f'Ambiguous insert_after - found {self.line_match_count} matches for: {self.insert_after}')\n            target_absolute_line = node.lineno + target_line_idx\n            containing_node = self._find_containing_node(node, target_absolute_line)\n            if containing_node and containing_node is not node:\n                self._insert_into_node(containing_node, target_absolute_line)\n            else:\n                insert_index = len(node.body)\n                for idx, child in enumerate(node.body):\n                    if hasattr(child, 'lineno') and child.lineno > target_absolute_line:\n                        insert_index = idx\n                        break\n                if insert_index == len(node.body):\n                    node.body.extend(self.new_nodes)\n                else:\n                    for i, new_node in enumerate(self.new_nodes):\n                        node.body.insert(insert_index + i, new_node)\n            self.success = True\n            return node\n        else:\n            if self.insert_after:\n                target_path = self.insert_after.split('::')\n                current_path = self.current_path\n                if len(target_path) > 1 and len(current_path) == len(target_path) - 1:\n                    if all((c == t for c, t in zip(current_path, target_path[:len(current_path)]))):\n                        target_name = target_path[-1]\n                        insert_index = None\n                        for idx, child in enumerate(node.body):\n                            if hasattr(child, 'name') and child.name == target_name:\n                                insert_index = idx + 1\n                                break\n                        if insert_index is not None:\n                            for i, new_node in enumerate(self.new_nodes):\n                                node.body.insert(insert_index + i, new_node)\n                            self.success = True\n                            return node\n                if len(current_path) == len(target_path) and all((c == t for c, t in zip(current_path, target_path))):\n                    self.success = True\n                    if isinstance(node, ast.ClassDef):\n                        node.body.extend(self.new_nodes)\n                    else:\n                        for new_node in self.new_nodes:\n                            node.body.append(new_node)\n            return node\n\n    def _find_containing_node(self, node, target_line):\n        \"\"\"Find the deepest AST node with a body that contains the target line.\"\"\"\n        if not (hasattr(node, 'lineno') and hasattr(node, 'end_lineno')):\n            return None\n        if not node.lineno <= target_line <= node.end_lineno:\n            return None\n        deepest_with_body = node if hasattr(node, 'body') else None\n        if hasattr(node, 'body'):\n            for child in node.body:\n                deeper_node = self._find_containing_node(child, target_line)\n                if deeper_node:\n                    return deeper_node\n        if hasattr(node, 'orelse'):\n            for child in node.orelse:\n                deeper_node = self._find_containing_node(child, target_line)\n                if deeper_node:\n                    return deeper_node\n        if deepest_with_body:\n            return deepest_with_body\n        return None\n\n    def _insert_into_node(self, node, target_line):\n        \"\"\"Insert new nodes into the given node after the target line.\"\"\"\n        if not hasattr(node, 'body'):\n            return\n        insert_index = len(node.body)\n        for idx, child in enumerate(node.body):\n            if hasattr(child, 'lineno') and child.lineno > target_line:\n                insert_index = idx\n                break\n        for i, new_node in enumerate(self.new_nodes):\n            node.body.insert(insert_index + i, new_node)\n\ndef _create_token(content: str, index: int, current_hash: str, token_type: TokenType=None, extra_metadata: dict=None) -> Tuple[str, Dict]:\n    \"\"\"Create a token with our specific pattern and metadata\"\"\"\n    token_name = f'TOKEN_{current_hash}_{index}'\n    metadata = extra_metadata or {}\n    if token_type:\n        metadata['type'] = token_type.value\n    token_data = {'content': content, 'metadata': metadata}\n    return (token_name, token_data)\n\ndef _get_file_hash(content: str) -> str:\n    \"\"\"Create a short hash of file content\"\"\"\n    return hashlib.sha256(content.encode()).hexdigest()[:8]\n\n\ndef _contains_token(s: str) -> bool:\n    return 'TOKEN_' in s\n\n\ndef _find_complete_triple_quote(text, start_pos=0):\n    \"\"\"Find complete triple-quoted strings manually\"\"\"\n    for quote_type in ['\"\"\"', \"'''\"]:\n        open_pos = text.find(quote_type, start_pos)\n        if open_pos == -1:\n            continue\n        close_pos = text.find(quote_type, open_pos + 3)\n        if close_pos == -1:\n            continue\n        complete_string = text[open_pos:close_pos + 3]\n        if not _contains_token(complete_string):\n            return (open_pos, close_pos + 3, complete_string)\n    return (None, None, None)\n\n\ndef _find_string_end(line, start_pos, quote_char):\n    \"\"\"Find the end position of a quoted string, handling escape sequences.\"\"\"\n    pos = start_pos + 1\n    while pos < len(line):\n        if line[pos] == quote_char:\n            return pos\n        elif line[pos] == '\\\\' and pos + 1 < len(line):\n            pos += 2  # Skip escaped character\n        else:\n            pos += 1\n    return -1  # No closing quote found\n\ndef _tokenize_source(source: str) -> Tuple[str, Dict[str, Dict]]:\n    \"\"\"\n    Tokenize source code, preserving exact formatting.\n\n    Returns:\n    - tokenized_source: Source with tokens inserted\n    - token_map: Mapping of tokens to {'content': str, 'metadata': dict}\n    \"\"\"\n    token_map = {}\n    current_hash = _get_file_hash(source)\n    token_counter = 0\n    \n    # First pass: handle multi-line strings\n    tokenized = _process_multiline_strings(source, token_map, token_counter, current_hash)\n    token_counter = len(token_map)\n    \n    # Second pass: process each line\n    lines = tokenized.split('\\n')\n    processed_lines = []\n    \n    for line in lines:\n        processed_line, token_counter = _process_single_line(\n            line, token_map, token_counter, current_hash\n        )\n        processed_lines.append(processed_line)\n    \n    return '\\n'.join(processed_lines), token_map\n\n\ndef _process_multiline_strings(source: str, token_map: dict, token_counter: int, current_hash: str) -> str:\n    \"\"\"Process all triple-quoted strings in the source.\"\"\"\n    tokenized = source\n    processed_positions = set()\n    \n    for _ in range(10):  # Safety limit\n        old_tokenized = tokenized\n        \n        start_pos, end_pos, string_content = _find_complete_triple_quote(tokenized)\n        if start_pos is None:\n            break\n            \n        if start_pos in processed_positions or len(string_content) > 1000:\n            break\n            \n        processed_positions.add(start_pos)\n        \n        token_name, token_data = _create_token(\n            string_content, token_counter, current_hash, TokenType.MULTILINE_STRING\n        )\n        token_map[token_name] = token_data\n        tokenized = tokenized[:start_pos] + token_name + tokenized[end_pos:]\n        token_counter += 1\n        \n        if tokenized == old_tokenized:\n            break\n    \n    return tokenized\n\ndef _process_single_line(line: str, token_map: dict, token_counter: int, current_hash: str) -> Tuple[str, int]:\n    \"\"\"Process a single line of code, handling indentation and various token types.\"\"\"\n    if not line.strip():\n        return line, token_counter\n    \n    indent = len(line) - len(line.lstrip())\n    indentation = ' ' * indent\n    content = line[indent:]\n    \n    if _contains_token(content):\n        return line, token_counter\n    \n    # Handle standalone comments\n    if content.strip().startswith('#'):\n        token_name, token_data = _create_token(\n            content.strip(), token_counter, current_hash, TokenType.STANDALONE_COMMENT\n        )\n        token_map[token_name] = token_data\n        return f\"{indentation}{token_name}\", token_counter + 1\n    \n    # Process string literals\n    processed_content, token_counter = _process_string_literals(\n        content, token_map, token_counter, current_hash\n    )\n    \n    # Handle inline comments\n    if '#' in processed_content:\n        processed_content, token_counter = _process_inline_comment(\n            processed_content, token_map, token_counter, current_hash\n        )\n    \n    return indentation + processed_content, token_counter\n\ndef _process_string_literals(processed_line, token_map, token_counter, current_hash):\n    \"\"\"Extract and tokenize string literals from a line of code.\"\"\"\n    if _contains_token(processed_line):\n        return processed_line, token_counter\n    \n    for quote_char in ['\"', \"'\"]:\n        start = processed_line.find(quote_char)\n        if start == -1:\n            continue\n            \n        end = _find_string_end(processed_line, start, quote_char)\n        if end == -1:\n            continue\n            \n        string_content = processed_line[start:end + 1]\n        token_name, token_data = _create_token(\n            string_content, token_counter, current_hash, TokenType.STRING_LITERAL\n        )\n        token_map[token_name] = token_data\n        processed_line = processed_line[:start] + token_name + processed_line[end + 1:]\n        token_counter += 1\n        break\n    \n    return processed_line, token_counter\n\ndef _process_inline_comment(line: str, token_map: dict, token_counter: int, current_hash: str) -> Tuple[str, int]:\n    \"\"\"Process inline comments, determining their type based on the preceding code.\"\"\"\n    comment_start = line.index('#')\n    code = line[:comment_start]\n    code_end = len(code.rstrip())\n    spacing_and_comment = line[code_end:]\n    code_stripped = code.rstrip()\n    \n    # Determine comment type and metadata\n    token_type, extra_metadata = _determine_comment_type(code_stripped)\n    \n    token_name, token_data = _create_token(\n        spacing_and_comment, token_counter, current_hash, token_type, extra_metadata\n    )\n    token_map[token_name] = token_data\n    \n    # For compound comments, put token on separate line with proper indentation\n    if token_type == TokenType.COMPOUND_COMMENT:\n        # Get the base indentation of the current line\n        base_indent = len(line) - len(line.lstrip())\n        # Add 4 spaces for the expected function/class body indentation\n        token_indent = ' ' * (base_indent + 4)\n        processed_line = f\"{code.rstrip()}\\n{token_indent}{token_name}\"\n    else:\n        processed_line = f\"{code.rstrip()}; {token_name}\"\n    \n    return processed_line, token_counter + 1\n\ndef _determine_comment_type(code_stripped: str) -> Tuple[TokenType, dict]:\n    \"\"\"Determine the type of comment based on the preceding code.\"\"\"\n    if code_stripped.startswith(('import ', 'from ')):\n        return TokenType.IMPORT_COMMENT, {'import_statement': code_stripped}\n    \n    compound_patterns = [\n        'def ', 'class ', 'if ', 'elif ', 'for ', 'while ', 'with ', 'async def '\n    ]\n    standalone_statements = ['else:', 'try:', 'finally:']\n    \n    is_compound_with_colon = (\n        any(code_stripped.startswith(pattern) for pattern in compound_patterns) \n        and code_stripped.endswith(':')\n    )\n    is_standalone_colon = code_stripped in standalone_statements\n    \n    if is_compound_with_colon or is_standalone_colon:\n        return TokenType.COMPOUND_COMMENT, {'statement': code_stripped}\n    \n    return TokenType.INLINE_COMMENT, {}\n\ndef _detokenize_source(tokenized_source: str, token_map: Dict[str, Dict]) -> str:\n    \"\"\"\n    Restore original source from tokenized version using metadata-driven processing.\n    \"\"\"\n    result = tokenized_source\n    for token_name, token_data in token_map.items():\n        content = token_data['content']\n        metadata = token_data['metadata']\n        token_type = metadata.get('type')\n        while token_name in result:\n            start = result.find(token_name)\n            if start == -1:\n                break\n            if token_type == TokenType.COMPOUND_COMMENT.value:\n                result = _handle_compound_comment(result, token_name, content, start)\n            elif token_type == TokenType.IMPORT_COMMENT.value:\n                result = _handle_import_comment(result, token_name, content, start, metadata)\n            elif token_type == TokenType.INLINE_COMMENT.value:\n                result = _handle_inline_comment(result, token_name, content, start)\n            elif token_type == TokenType.STANDALONE_COMMENT.value:\n                result = _handle_standalone_comment(result, token_name, content, start)\n            elif token_type == TokenType.MULTILINE_STRING.value:\n                result = _handle_multiline_string(result, token_name, content, start)\n            elif token_type == TokenType.STRING_LITERAL.value:\n                result = _handle_string_literal(result, token_name, content, start)\n            else:\n                result = _handle_default_token(result, token_name, content, start)\n    return result\n\ndef _get_indentation_at_position(source: str, pos: int) -> str:\n    \"\"\"Get the indentation level at a given position in source\"\"\"\n    line_start = source.rfind('\\n', 0, pos) + 1\n    return source[line_start:pos].replace(source[line_start:pos].lstrip(), '')\n\ndef _indent_multiline_content(content: str, indent: str) -> str:\n    \"\"\"Indent multiline content while preserving internal formatting\"\"\"\n    if not content.strip():\n        return content\n    lines = content.split('\\n')\n    if len(lines) == 1:\n        return content\n    if all((line.startswith(' ') for line in lines[1:] if line.strip())):\n        return content\n    result = [lines[0]]\n    for line in lines[1:]:\n        if line.strip():\n            result.append(indent + line)\n        else:\n            result.append(line)\n    return '\\n'.join(result)\n\ndef _preserve_blank_lines(result: str, original_source: str) -> str:\n    \"\"\"\n    Restore blank lines around comments that had them in the original source.\n    \"\"\"\n    original_lines = original_source.split('\\n')\n    result_lines = result.split('\\n')\n    comments_with_blanks = {}\n    for i, line in enumerate(original_lines):\n        if line.strip().startswith('#') and 'marker' in line.lower():\n            blank_before = i > 0 and (not original_lines[i - 1].strip())\n            blank_after = i < len(original_lines) - 1 and (not original_lines[i + 1].strip())\n            if blank_before or blank_after:\n                comments_with_blanks[line.strip()] = {'blank_before': blank_before, 'blank_after': blank_after}\n    if not comments_with_blanks:\n        return result\n    new_result_lines = []\n    i = 0\n    while i < len(result_lines):\n        line = result_lines[i]\n        line_stripped = line.strip()\n        if line_stripped in comments_with_blanks:\n            info = comments_with_blanks[line_stripped]\n            if info['blank_before']:\n                if i > 0 and new_result_lines[-1].strip():\n                    new_result_lines.append('')\n            new_result_lines.append(line)\n            if info['blank_after']:\n                if i < len(result_lines) - 1 and result_lines[i + 1].strip():\n                    new_result_lines.append('')\n        else:\n            new_result_lines.append(line)\n        i += 1\n    return '\\n'.join(new_result_lines)\n\ndef _handle_compound_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle compound statement comments (def, class, if, etc.) with smart reunification\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    line_stripped = line.strip()\n    if line_stripped.startswith('# ') and line_stripped[2:] == token_name:\n        prev_line_start = result.rfind('\\n', 0, line_start - 1) + 1\n        prev_line = result[prev_line_start:line_start - 1]\n        if prev_line.strip():\n            reunited = prev_line + content\n            return result[:prev_line_start] + reunited + result[line_end:]\n    elif line_stripped == token_name:\n        search_start = line_start - 1\n        while search_start > 0:\n            prev_line_start = result.rfind('\\n', 0, search_start) + 1\n            prev_line = result[prev_line_start:search_start]\n            if prev_line.strip():\n                if prev_line.strip().endswith(':'):\n                    reunited = prev_line.rstrip() + content\n                    return result[:prev_line_start] + reunited + result[line_end:]\n                break\n            search_start = prev_line_start - 1\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_import_comment(result: str, token_name: str, content: str, start: int, metadata: dict) -> str:\n    \"\"\"Handle import statement comments with metadata-driven reunification\"\"\"\n    import_statement = metadata.get('import_statement')\n    if not import_statement:\n        return result[:start] + content + result[start + len(token_name):]\n    lines = result.split('\\n')\n    token_line_idx = None\n    for i, line in enumerate(lines):\n        if token_name in line:\n            token_line_idx = i\n            break\n    if token_line_idx is None:\n        return result[:start] + content + result[start + len(token_name):]\n    search_range = range(max(0, token_line_idx - 3), min(len(lines), token_line_idx + 3))\n    for i in search_range:\n        if i != token_line_idx and lines[i].strip() == import_statement:\n            lines[i] = lines[i].rstrip() + content\n            lines[token_line_idx] = '__REMOVE_THIS_LINE__'\n            new_lines = [line for line in lines if line != '__REMOVE_THIS_LINE__']\n            return '\\n'.join(new_lines)\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_inline_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle inline comments (non-import, non-compound) with smart reunification\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    line_stripped = line.strip()\n    if line_stripped == token_name:\n        search_start = line_start - 1\n        while search_start > 0:\n            prev_line_start = result.rfind('\\n', 0, search_start) + 1\n            prev_line = result[prev_line_start:search_start]\n            if prev_line.strip():\n                reunited = prev_line.rstrip() + content\n                return result[:prev_line_start] + reunited + result[line_end:]\n            search_start = prev_line_start - 1\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_standalone_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle standalone comment lines\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    indent = line[:len(line) - len(line.lstrip())]\n    replacement = indent + content\n    return result[:line_start] + replacement + result[line_end:]\n\ndef _handle_multiline_string(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle multiline strings with proper indentation\"\"\"\n    if '\\n' in content:\n        indent = _get_indentation_at_position(result, start)\n        indented = _indent_multiline_content(content, indent)\n        return result[:start] + indented + result[start + len(token_name):]\n    else:\n        return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_string_literal(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle simple string literals\"\"\"\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_default_token(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle tokens without specific type metadata\"\"\"\n    return result[:start] + content + result[start + len(token_name):]",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py",
    "code_hash": "e72b6bcf88bdf6759799146c738b35ca",
    "globals": {
     "ast": "<module 'ast' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\ast.py'>",
     "os": "<module 'os' (frozen)>",
     "textwrap": "<module 'textwrap' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\textwrap.py'>",
     "_process_error": "<function _process_error at 0x000001917E6A54E0>",
     "_clean": "<function _clean at 0x000001917E6C4180>",
     "_py_ast_to_source": "<function _py_ast_to_source at 0x000001917E6C6E80>",
     "hashlib": "<module 'hashlib' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\hashlib.py'>",
     "re": "<module 're' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\re\\\\__init__.py'>",
     "Dict": "typing.Dict",
     "Tuple": "typing.Tuple",
     "Union": "typing.Union",
     "Enum": "<enum 'Enum'>",
     "TokenType": "<enum 'TokenType'>",
     "python_edit": "<function python_edit at 0x000001917FED9F80>",
     "_make_file": "<function _make_file at 0x000001917FEDA0C0>",
     "ScopeTransformer": "<class 'dynamic_module_2feb8c52b8785b27616df3d91d664fd4.ScopeTransformer'>",
     "_create_token": "<function _create_token at 0x000001917FEDA160>",
     "_get_file_hash": "<function _get_file_hash at 0x000001917FEDB1A0>",
     "_contains_token": "<function _contains_token at 0x000001917FEDB240>",
     "_find_complete_triple_quote": "<function _find_complete_triple_quote at 0x000001917FEDB2E0>",
     "_find_string_end": "<function _find_string_end at 0x000001917FEDB380>",
     "_tokenize_source": "<function _tokenize_source at 0x000001917FEDB420>",
     "_process_multiline_strings": "<function _process_multiline_strings at 0x000001917FEDB4C0>",
     "_process_single_line": "<function _process_single_line at 0x000001917FEDB560>",
     "_process_string_literals": "<function _process_string_literals at 0x000001917FEDB600>",
     "_process_inline_comment": "<function _process_inline_comment at 0x000001917FEDB6A0>",
     "_determine_comment_type": "<function _determine_comment_type at 0x000001917FEDB740>",
     "_detokenize_source": "<function _detokenize_source at 0x000001917FEDB7E0>",
     "_get_indentation_at_position": "<function _get_indentation_at_position at 0x000001917FEDB880>",
     "_indent_multiline_content": "<function _indent_multiline_content at 0x000001917FEDB920>",
     "_preserve_blank_lines": "<function _preserve_blank_lines at 0x000001917FEDB9C0>",
     "_handle_compound_comment": "<function _handle_compound_comment at 0x000001917FEDBA60>",
     "_handle_import_comment": "<function _handle_import_comment at 0x000001917FEDBB00>",
     "_handle_inline_comment": "<function _handle_inline_comment at 0x000001917FEDBBA0>",
     "_handle_standalone_comment": "<function _handle_standalone_comment at 0x000001917FEDBC40>",
     "_handle_multiline_string": "<function _handle_multiline_string at 0x000001917FEDBCE0>",
     "_handle_string_literal": "<function _handle_string_literal at 0x000001917FEDBD80>",
     "_handle_default_token": "<function _handle_default_token at 0x000001917FEDBE20>"
    }
   }
  },
  "function_paths": {
   "read_file": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
   "view_dir": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
   "patch_edit": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
   "execute_powershell": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py",
   "python_edit": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py"
  }
 },
 "autosave": false,
 "bot_class": "AnthropicBot"
}