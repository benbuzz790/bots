{
 "name": "Claude",
 "model_engine": "claude-sonnet-4-20250514",
 "max_tokens": 8192,
 "temperature": 0.3,
 "role": "assistant",
 "role_description": "a friendly AI assistant",
 "conversation": {
  "content": "",
  "pending_results": [],
  "role": "empty",
  "tool_calls": [],
  "tool_results": [],
  "node_class": "AnthropicNode",
  "replies": [
   {
    "content": "First, understand the requirements",
    "pending_results": [],
    "role": "user",
    "tool_calls": [],
    "tool_results": [],
    "node_class": "AnthropicNode",
    "replies": [
     {
      "content": "I'll help you understand the requirements. To do this effectively, I need to see what requirements document or specification you're referring to. Could you please:\n\n1. Share the requirements document or file that you'd like me to analyze, or\n2. Tell me the location/path of the requirements file so I can view it, or\n3. Provide the specific requirements text you'd like me to help understand\n\nOnce I can see the requirements, I'll be able to:\n- Break down the key components\n- Identify any dependencies or constraints\n- Clarify any ambiguous points\n- Summarize the main objectives\n- Help you understand the scope and expectations\n\nWhat would you like me to examine?",
      "pending_results": [],
      "role": "assistant",
      "tool_calls": [],
      "tool_results": [],
      "node_class": "AnthropicNode",
      "replies": [
       {
        "content": "Then, design the architecture",
        "pending_results": [],
        "role": "user",
        "tool_calls": [],
        "tool_results": [],
        "node_class": "AnthropicNode",
        "replies": [
         {
          "content": "I understand you'd like me to help design an architecture, but I need more context first. To provide you with a meaningful architectural design, I'll need to understand:\n\n1. **The requirements** - What system or application are we building? What are the functional and non-functional requirements?\n\n2. **The scope** - Are we designing:\n   - A software application architecture?\n   - A system architecture?\n   - A network architecture?\n   - A database architecture?\n   - Something else?\n\n3. **Key constraints** - What are the:\n   - Performance requirements?\n   - Scalability needs?\n   - Security requirements?\n   - Technology preferences or restrictions?\n   - Budget/resource constraints?\n\nCould you please:\n- Share the requirements document or specification\n- Tell me what type of system we're architecting\n- Provide any existing documentation or context\n\nOnce I have this information, I can help you design a comprehensive architecture that includes:\n- High-level system overview\n- Component breakdown\n- Data flow diagrams\n- Technology stack recommendations\n- Deployment considerations\n- Security architecture\n- Scalability planning\n\nWhat requirements or project details can you share with me?",
          "pending_results": [],
          "role": "assistant",
          "tool_calls": [],
          "tool_results": [],
          "node_class": "AnthropicNode",
          "replies": [
           {
            "content": "Finally, implement the solution",
            "pending_results": [],
            "role": "user",
            "tool_calls": [],
            "tool_results": [],
            "node_class": "AnthropicNode",
            "replies": [
             {
              "content": "I understand you'd like me to help implement a solution, but to provide you with effective implementation, I need to understand what we're building first. \n\nTo properly help you with implementation, I need:\n\n1. **Requirements Understanding**\n   - What problem are we solving?\n   - What are the functional requirements?\n   - What are the technical constraints?\n\n2. **Architecture Design**\n   - What system components do we need?\n   - How do they interact?\n   - What technologies should we use?\n\n3. **Implementation Scope**\n   - What programming language(s)?\n   - What frameworks or libraries?\n   - What's the target environment?\n\nCould you please provide:\n- The requirements document or specification\n- Any existing code or project files\n- The project directory structure\n- Technology preferences or constraints\n\nOnce I have this context, I can help you implement:\n- Core application logic\n- Database schemas and queries\n- API endpoints\n- User interfaces\n- Configuration files\n- Tests\n- Documentation\n- Deployment scripts\n\nWould you like to:\n1. Share your project files so I can examine the current state?\n2. Provide the requirements document?\n3. Tell me about the specific problem you're trying to solve?\n\nWhat would be most helpful to get started?",
              "pending_results": [],
              "role": "assistant",
              "tool_calls": [],
              "tool_results": [],
              "node_class": "AnthropicNode"
             }
            ]
           }
          ]
         }
        ]
       }
      ]
     }
    ]
   }
  ]
 },
 "system_message": "",
 "tool_handler": {
  "class": "bots.foundation.anthropic_bots.AnthropicToolHandler",
  "tools": [
   {
    "name": "execute_powershell",
    "description": "Executes PowerShell commands in a stateful environment\n\nUse when you need to run PowerShell commands and capture their output. If\nyou have other tools available, you should use this as a fallback when the\nother tools fail. Coerces to utf-8.\n\nPotential use cases:\n- git commands\n- gh cli\n- other cli (which you may need to install using this tool)\n\nParameters:\n- command (str): PowerShell command to execute.\n- output_length_limit (int, optional): Maximum number of lines in the output.\n  If set, output exceeding this limit will be truncated. Default 200.\n\nReturns:\n    str: The complete output from the command execution",
    "input_schema": {
     "type": "object",
     "properties": {
      "command": {
       "type": "string"
      },
      "output_length_limit": {
       "type": "string"
      },
      "timeout": {
       "type": "string"
      }
     },
     "required": [
      "command"
     ]
    }
   },
   {
    "name": "python_edit",
    "description": "Edit Python code using pytest-style scope syntax.\nDefault behavior: Replace entire target scope.\n\nParameters:\n----------\ntarget_scope : str\n    Location to edit in pytest-style scope syntax:\n    - \"file.py\" (whole file)\n    - \"file.py::MyClass\" (class)\n    - \"file.py::my_function\" (function)\n    - \"file.py::MyClass::method\" (method)\n    - \"file.py::Outer::Inner::method\" (nested)\n    - \"file.py::utils::helper_func\" (nested function)\n\ncode : str\n    Python code to insert/replace. Will be cleaned/dedented.\n    Import statements will be automatically extracted and handled.\n\ninsert_after : str, optional\n    Where to insert the code. Can be either:\n    - \"__FILE_START__\" (special token for file beginning)\n    - A scope (\"MyClass::method\")\n    - An exact line match (like a context line in git patches)\n    If not specified, replaces the node at target_scope.\n\nReturns:\n--------\nstr\n    Description of what was modified or error message",
    "input_schema": {
     "type": "object",
     "properties": {
      "target_scope": {
       "type": "string"
      },
      "code": {
       "type": "string"
      },
      "insert_after": {
       "type": "string"
      }
     },
     "required": [
      "target_scope",
      "code"
     ]
    }
   },
   {
    "name": "view",
    "description": "Display the contents of a file with line numbers.\nParameters:\n- file_path (str): The path to the file to be viewed.\n- start_line (int, optional): Starting line number (1-indexed). If None, starts from beginning.\n- end_line (int, optional): Ending line number (1-indexed). If None, goes to end of file.\n- around_str_match (str, optional): String to search for. If provided, shows lines around matches.\n- dist_from_match (int, optional): Number of lines to show before and after each match. Defaults to 10.\nReturns:\nA string containing the file contents with line numbers,\nfiltered according to the specified parameters.",
    "input_schema": {
     "type": "object",
     "properties": {
      "file_path": {
       "type": "string"
      },
      "start_line": {
       "type": "string"
      },
      "end_line": {
       "type": "string"
      },
      "around_str_match": {
       "type": "string"
      },
      "dist_from_match": {
       "type": "string"
      }
     },
     "required": [
      "file_path"
     ]
    }
   },
   {
    "name": "view_dir",
    "description": "Creates a summary of the directory structure starting from the given path, writing only files\nwith specified extensions and showing venv directories without their contents.\nParameters:\n- start_path (str): The root directory to start scanning from.\n- output_file (str): The name of the file to optionally write the directory structure to.\n- target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n- max_lines (int): Maximum number of lines in output. If exceeded, deepest levels are truncated. Default 500.\nReturns:\nstr: A formatted string containing the directory structure, with each directory and file properly indented.\nExample output:\nmy_project/\n    venv/\n    module1/\n        script.py\n        README.md\n    module2/\n        utils.py\ncost: low",
    "input_schema": {
     "type": "object",
     "properties": {
      "start_path": {
       "type": "string"
      },
      "output_file": {
       "type": "string"
      },
      "target_extensions": {
       "type": "string"
      },
      "max_lines": {
       "type": "string"
      }
     },
     "required": []
    }
   },
   {
    "name": "patch_edit",
    "description": "Apply a git-style unified diff patch to a file.\nUse when you need to make a precise change to a file\nTips:\n- Small, focused changes work best\n- Exact content matching is important\n- Including surrounding context helps matching\n- Whitespace in the target lines matters\nParameters:\n- file_path (str): Path to the file to modify\n- patch_content (str): Unified diff format patch content\nReturns:\nstr: Description of changes made or error message\ncost: low",
    "input_schema": {
     "type": "object",
     "properties": {
      "file_path": {
       "type": "string"
      },
      "patch_content": {
       "type": "string"
      }
     },
     "required": [
      "file_path",
      "patch_content"
     ]
    }
   },
   {
    "type": "web_search_20250305",
    "name": "web_search",
    "max_uses": 10,
    "cache_control": {
     "type": "ephemeral"
    }
   }
  ],
  "requests": [],
  "results": [],
  "modules": {
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py": {
    "name": "dynamic_module_d64ce3a2b8cfb34ac3eb473953440645",
    "source": "import os, subprocess, traceback, time, threading\nfrom queue import Queue, Empty\nfrom threading import Thread, Lock, local\nfrom typing import Dict, Generator\nfrom datetime import datetime\nimport base64\nimport tempfile\n\nfrom bots.dev.decorators import log_errors, handle_errors\n\n@log_errors\n@handle_errors\ndef execute_powershell(command: str, output_length_limit: str='2500', timeout: str = '60') -> str:\n    \"\"\"\n    Executes PowerShell commands in a stateful environment\n\n    Use when you need to run PowerShell commands and capture their output. If\n    you have other tools available, you should use this as a fallback when the\n    other tools fail. Coerces to utf-8.\n\n    Potential use cases:\n    - git commands\n    - gh cli\n    - other cli (which you may need to install using this tool)\n\n    Parameters:\n    - command (str): PowerShell command to execute.\n    - output_length_limit (int, optional): Maximum number of lines in the output.\n      If set, output exceeding this limit will be truncated. Default 200.\n\n    Returns:\n        str: The complete output from the command execution\n    \"\"\"\n    manager = PowerShellManager.get_instance()\n    output = ''.join(manager.execute(command, int(output_length_limit), float(timeout)))\n    return output\n\n\nclass PowerShellSession:\n    \"\"\"\n    Manages a persistent PowerShell process for stateful command execution.\n    Maintains a single PowerShell process that persists between commands,\n    allowing for stateful operations like changing directories, setting\n    environment variables, or activating virtual environments.\n    \"\"\"\n\n    def __init__(self, timeout: float = 300):\n        self._process = None\n        self._command_counter = 0\n        self._output_queue = Queue()\n        self._error_queue = Queue()\n        self._reader_threads = []\n        self.startupinfo = None\n        if os.name == 'nt':\n            self.startupinfo = subprocess.STARTUPINFO()\n            self.startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n\n    def _start_reader_threads(self):\n        \"\"\"Start background threads to read stdout and stderr\"\"\"\n\n        def reader_thread(pipe, queue):\n            try:\n                for line in pipe:\n                    queue.put(line.rstrip('\\n\\r'))\n            finally:\n                queue.put(None)\n        self._reader_threads = [\n            Thread(target=reader_thread, args=(self._process.stdout, self._output_queue), daemon=True),\n            Thread(target=reader_thread, args=(self._process.stderr, self._error_queue), daemon=True)\n        ]\n        for thread in self._reader_threads:\n            thread.start()\n\n    def __enter__(self):\n        if not self._process:\n            self._process = subprocess.Popen(\n                ['powershell', '-NoProfile', '-NoLogo', '-NonInteractive', '-Command', '-'],\n                stdin=subprocess.PIPE,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                startupinfo=self.startupinfo,\n                encoding='utf-8',\n                errors='replace',\n                bufsize=1\n            )\n            self._start_reader_threads()\n            # Initialize PowerShell session with better encoding and error handling\n            init_commands = [\n                \"$VerbosePreference='SilentlyContinue'\",\n                \"$DebugPreference='SilentlyContinue'\",\n                \"$ProgressPreference='SilentlyContinue'\",\n                \"$WarningPreference='SilentlyContinue'\",\n                \"$ErrorActionPreference='Stop'\",\n                \"function prompt { '' }\",\n                \"$PSDefaultParameterValues['*:Encoding']='utf8'\",\n                '[Console]::OutputEncoding=[System.Text.Encoding]::UTF8',\n                '[Console]::InputEncoding=[System.Text.Encoding]::UTF8',\n                '$OutputEncoding=[System.Text.Encoding]::UTF8',\n                \"$env:PYTHONIOENCODING='utf-8'\",\n                # Add function to handle complex strings safely\n                \"\"\"\nfunction Invoke-SafeCommand {\n    param([string]$Command)\n    try {\n        Invoke-Expression $Command\n        return $true\n    } catch {\n        Write-Error $_\n        return $false\n    }\n}\n                \"\"\".strip()\n            ]\n            for cmd in init_commands:\n                self._process.stdin.write(cmd + '\\n')\n            self._process.stdin.flush()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self._process:\n            try:\n                self._process.terminate()\n                self._process.wait(timeout=5)\n            except:\n                self._process.kill()\n            finally:\n                self._process = None\n                self._output_queue = Queue()\n                self._error_queue = Queue()\n                self._reader_threads = []\n\n    def execute(self, code: str, timeout: float = 60) -> str:\n        \"\"\"\n        Execute PowerShell code and return its complete output.\n\n        Args:\n            code: The PowerShell code to execute\n            timeout: Maximum time in seconds to wait for command completion\n\n        Returns:\n            The complete output from the command(s)\n\n        Raises:\n            Exception if the process is not running or other execution errors\n            TimeoutError if command execution exceeds timeout\n        \"\"\"\n        if not self._process:\n            raise Exception('PowerShell process is not running')\n\n        try:\n            self._command_counter += 1\n            delimiter = f'<<<COMMAND_{self._command_counter}_COMPLETE>>>'\n            \n            # Use a more robust approach for complex code execution\n            wrapped_code = self._wrap_code_safely(code, delimiter)\n            \n            # Clear queues\n            while not self._output_queue.empty():\n                self._output_queue.get_nowait()\n            while not self._error_queue.empty():\n                self._error_queue.get_nowait()\n\n            self._process.stdin.write(wrapped_code + '\\n')\n            self._process.stdin.flush()\n\n            output_lines = []\n            error_output = []\n            start_time = time.time()\n            done = False\n\n            while not done:\n                if time.time() - start_time > timeout:\n                    raise TimeoutError(f'Command execution timed out after {timeout} seconds')\n\n                if self._process.poll() is not None:\n                    raise Exception('PowerShell process unexpectedly closed')\n\n                try:\n                    line = self._output_queue.get(timeout=0.1)\n                    if line is None:\n                        raise Exception('PowerShell process closed stdout')\n                    if line == delimiter:\n                        done = True\n                    else:\n                        output_lines.append(line)\n                except Empty:\n                    pass\n\n                try:\n                    while True:\n                        line = self._error_queue.get_nowait()\n                        if line is None:\n                            raise Exception('PowerShell process closed stderr')\n                        error_output.append(line)\n                except Empty:\n                    pass\n\n            all_output = [line for line in output_lines if line.strip()]\n            if error_output:\n                error_lines = [line for line in error_output if line.strip()]\n                if error_lines:\n                    all_output.extend(['', 'Errors:', *error_lines])\n\n            # Get current directory for display\n            try:\n                current_dir = self._get_current_directory()\n                dir_info = f\"[System: current directory <{current_dir}>]\"\n                if all_output:\n                    all_output.insert(0, dir_info)\n                else:\n                    all_output = [dir_info]\n            except:\n                pass  # If we can't get directory, continue without it\n            return '\\n'.join(all_output)\n\n        except Exception as e:\n            self.__exit__(type(e), e, e.__traceback__)\n            raise\n\n    def _get_current_directory(self) -> str:\n        \"\"\"Get the current directory from the PowerShell session.\"\"\"\n        if not self._process:\n            return \"unknown\"\n        try:\n            # Use a simple PowerShell command to get current directory\n            temp_counter = self._command_counter + 1000  # Use different counter to avoid conflicts\n            delimiter = f'<<<DIR_QUERY_{temp_counter}_COMPLETE>>>'\n            # Clear queues first\n            while not self._output_queue.empty():\n                try:\n                    self._output_queue.get_nowait()\n                except:\n                    break\n            # Send directory query\n            dir_command = f\"(Get-Location).Path; Write-Output '{delimiter}'\"\n            self._process.stdin.write(dir_command + '\\n')\n            self._process.stdin.flush()\n            # Read response with timeout\n            start_time = time.time()\n            result_lines = []\n            while time.time() - start_time < 2:  # 2 second timeout\n                try:\n                    line = self._output_queue.get(timeout=0.1)\n                    if line == delimiter:\n                        break\n                    elif line and line.strip():\n                        result_lines.append(line.strip())\n                except:\n                    continue\n            # Return the first non-empty line as the directory\n            for line in result_lines:\n                if line and not line.startswith('PS ') and (':\\\\' in line or line.startswith('/')):\n                    return line\n            return \"unknown\"\n        except:\n            return \"unknown\"\n    def _wrap_code_safely(self, code: str, delimiter: str) -> str:\n        \"\"\"\n        Safely wrap code for execution, handling complex strings and multiline code.\n        \"\"\"\n        # Check if this looks like a complex Python command that might have quote issues\n        if 'python -c' in code and ('\"' in code or \"'\" in code):\n            return self._handle_python_command_safely(code, delimiter)\n        else:\n            # Use the original wrapping approach for simple commands\n            return f\"\"\"\n            $ErrorActionPreference = 'Stop'\n            \n            # Execute in main scope\n            {code}\n            \n            # Collect output after execution\n            $output = @()\n            try {{\n                if ($?) {{ \n                    # Add any output from the last command\n                    $output += $LASTOUTPUT\n                }}\n            }} catch {{\n                Write-Error $_\n            }}\n            $output | ForEach-Object {{ $_ }}\n            Write-Output '{delimiter}'\n            \"\"\"\n\n    def _get_current_directory(self) -> str:\n        \"\"\"Get the current directory from the PowerShell session.\"\"\"\n        if not self._process:\n            return \"unknown\"\n        # Send a quick command to get current directory\n        self._process.stdin.write(\"Get-Location | Select-Object -ExpandProperty Path\\n\")\n        self._process.stdin.flush()\n        # Read the response (simplified for this specific case)\n        import time\n        time.sleep(0.1)  # Brief wait for response\n        try:\n            # Try to get output from queue\n            lines = []\n            start_time = time.time()\n            while time.time() - start_time < 1:  # 1 second timeout\n                try:\n                    line = self._output_queue.get(timeout=0.1)\n                    if line and not line.startswith(\"PS \") and line.strip():\n                        return line.strip()\n                except:\n                    continue\n        except:\n            pass\n        return \"unknown\"\n    def _handle_python_command_safely(self, code: str, delimiter: str) -> str:\n        \"\"\"\n        Handle Python commands with complex strings by using file-based execution.\n        \"\"\"\n        # Extract the Python code part\n        if 'python -c \"' in code:\n            # Find the Python code between quotes\n            start_idx = code.find('python -c \"') + len('python -c \"')\n            # Find the closing quote - this is tricky with nested quotes\n            quote_count = 0\n            end_idx = start_idx\n            for i, char in enumerate(code[start_idx:], start_idx):\n                if char == '\"' and (i == start_idx or code[i-1] != '\\\\'):\n                    quote_count += 1\n                    if quote_count == 1:  # Found the closing quote\n                        end_idx = i\n                        break\n            \n            if end_idx > start_idx:\n                python_code = code[start_idx:end_idx]\n                # Clean up the Python code - remove extra escaping\n                python_code = python_code.replace('\\\\\"', '\"').replace(\"\\\\'\", \"'\")\n                \n                # Create a temporary file approach\n                return f\"\"\"\n                $ErrorActionPreference = 'Stop'\n                \n                # Create temporary file for Python code\n                $tempFile = [System.IO.Path]::GetTempFileName() + '.py'\n                $pythonCode = @'\n{python_code}\n'@\n                \n                try {{\n                    # Write Python code to temp file with UTF-8 encoding\n                    [System.IO.File]::WriteAllText($tempFile, $pythonCode, [System.Text.Encoding]::UTF8)\n                    \n                    # Execute Python with the temp file\n                    python $tempFile\n                }} catch {{\n                    Write-Error $_\n                }} finally {{\n                    # Clean up temp file\n                    if (Test-Path $tempFile) {{\n                        Remove-Item $tempFile -Force\n                    }}\n                }}\n                \n                Write-Output '{delimiter}'\n                \"\"\"\n        \n        # Fallback to original approach\n        return f\"\"\"\n        $ErrorActionPreference = 'Stop'\n        {code}\n        Write-Output '{delimiter}'\n        \"\"\"\n\n\nclass PowerShellManager:\n    \"\"\"\n    Thread-safe PowerShell session manager that maintains separate sessions.\n    Each instance automatically gets its own unique ID and isolated PowerShell session.\n    Includes session recovery logic for lost or failed sessions.\n    \"\"\"\n    _instances: Dict[str, 'PowerShellManager'] = {}\n    _lock = Lock()\n\n    @classmethod\n    def get_instance(cls, bot_id: str = None) -> 'PowerShellManager':\n        \"\"\"\n        Get or create a PowerShell manager instance for the given bot_id.\n        If no bot_id is provided, uses the current thread name.\n        \n        Args:\n            bot_id: Optional identifier for the bot instance\n            \n        Returns:\n            The PowerShell manager instance for this bot/thread\n        \"\"\"\n        if bot_id is None:\n            bot_id = threading.current_thread().name\n\n        with cls._lock:\n            if bot_id not in cls._instances:\n                instance = cls.__new__(cls)\n                instance.bot_id = bot_id\n                instance._thread_local = local()\n                instance.created_at = datetime.now()\n                cls._instances[bot_id] = instance\n            return cls._instances[bot_id]\n\n    def __init__(self):\n        \"\"\"\n        Private initializer - use get_instance() instead.\n        \"\"\"\n        raise RuntimeError('Use PowerShellManager.get_instance() to create or get a PowerShell manager')\n\n    @property\n    def session(self) -> PowerShellSession:\n        \"\"\"\n        Get the PowerShell session for the current thread.\n        Creates a new session if none exists or if the current session is invalid.\n        \"\"\"\n        try:\n            if not hasattr(self._thread_local, 'session'):\n                print(f'Session not found for {self.bot_id}, starting new session')\n                self._start_new_session()\n            elif not self._is_session_valid():\n                print(f'Invalid session detected for {self.bot_id}, starting new session')\n                self.cleanup()\n                self._start_new_session()\n            return self._thread_local.session\n        except Exception as e:\n            print(f'Error accessing session: {str(e)}. Starting new session.')\n            self._start_new_session()\n            return self._thread_local.session\n\n    def _start_new_session(self):\n        \"\"\"\n        Starts a new PowerShell session for the current thread.\n        \"\"\"\n        self._thread_local.session = PowerShellSession()\n        self._thread_local.session.__enter__()\n\n    def _is_session_valid(self) -> bool:\n        \"\"\"\n        Checks if the current session is valid and responsive.\n        Returns:\n            bool: True if session is valid, False otherwise\n        \"\"\"\n        try:\n            if not hasattr(self._thread_local, 'session'):\n                return False\n            session = self._thread_local.session\n            if not session._process or session._process.poll() is not None:\n                return False\n            # Use a simple test that's less likely to cause issues\n            test_output = session.execute(\"Write-Output 'test'\", timeout=5)\n            return 'test' in test_output\n        except Exception as e:\n            print(f'Session validation failed: {str(e)}')\n            return False\n\n    def execute(self, code: str, output_length_limit: str = '60', timeout: float = 60) -> Generator[str, None, None]:\n        \"\"\"\n        Execute PowerShell code in the session with automatic recovery.\n\n        Args:\n            code: PowerShell code to execute\n            output_length_limit: Maximum number of lines in output\n            timeout: Maximum time in seconds to wait for command completion\n\n        Yields:\n            Command output as strings\n        \"\"\"\n        max_retries = 0\n        retry_count = 0\n\n        def _process_error(error):\n            error_message = f'Tool Failed: {str(error)}\\n'\n            error_message += f\"Traceback:\\n{''.join(traceback.format_tb(error.__traceback__))}\"\n            return error_message\n\n        while retry_count <= max_retries:\n            try:\n                processed_code = _process_commands(code)\n                output = self.session.execute(processed_code, timeout)\n                \n                if output_length_limit is not None and output:\n                    output_length_limit_int = int(output_length_limit)\n                    lines = output.splitlines()\n                    if len(lines) > output_length_limit_int:\n                        half_limit = output_length_limit_int // 2\n                        start_lines = lines[:half_limit]\n                        end_lines = lines[-half_limit:]\n                        lines_omitted = len(lines) - output_length_limit_int\n                        truncated_output = '\\n'.join(start_lines)\n                        truncated_output += f'\\n\\n... {lines_omitted} lines omitted ...\\n\\n'\n                        truncated_output += '\\n'.join(end_lines)\n                        \n                        output_file = os.path.join(os.getcwd(), f'ps_output_{self.bot_id}.txt')\n                        with open(output_file, 'w', encoding='utf-8', errors='replace') as f:\n                            f.write(output)\n                        truncated_output += f'\\nFull output saved to {output_file}'\n                        yield truncated_output\n                    else:\n                        yield output\n                else:\n                    yield output\n                break\n\n            except Exception as e:\n                retry_count += 1\n                if retry_count <= max_retries:\n                    print(f'Command failed, attempt {retry_count} of {max_retries}. Starting new session...')\n                    self.cleanup()\n                else:\n                    yield _process_error(e)\n\n    def cleanup(self):\n        \"\"\"\n        Clean up the PowerShell session for the current thread.\n        Should be called when work is done or when a session needs to be reset.\n        \"\"\"\n        if hasattr(self._thread_local, 'session'):\n            try:\n                self._thread_local.session.__exit__(None, None, None)\n            except Exception as e:\n                print(f'Error during session cleanup: {str(e)}')\n            finally:\n                delattr(self._thread_local, 'session')\n\n\ndef _get_active_sessions() -> list:\n    \"\"\"\n    Get information about all active PowerShell sessions.\n\n    Returns:\n        List of dictionaries containing session information\n    \"\"\"\n    sessions = []\n    for thread in threading.enumerate():\n        if hasattr(thread, '_thread_local'):\n            local_dict = thread._thread_local.__dict__\n            if 'ps_manager' in local_dict:\n                manager = local_dict['ps_manager']\n                sessions.append({\n                    'bot_id': manager.bot_id,\n                    'thread_name': thread.name,\n                    'created_at': manager.created_at.isoformat(),\n                    'active': hasattr(manager._thread_local, 'session')\n                })\n    return sessions\n\n\n@handle_errors\ndef _execute_powershell_stateless(code: str, output_length_limit: str = '120'):\n    \"\"\"\n    Executes PowerShell code in a stateless environment\n\n    Use when you need to run PowerShell commands and capture their output. If\n    you have other tools available, you should use this as a fallback when the\n    other tools fail. Coerces to utf-8 encoding.\n\n    Potential use cases:\n    - git commands\n    - gh cli\n    - other cli (which you may need to install using this tool)\n\n    Parameters:\n    - code (str): PowerShell code to execute.\n    - output_length_limit (int, optional): Maximum number of lines in the output.\n      If set, output exceeding this limit will be truncated. Default 60.\n\n    Returns command output or an error message.\n    \"\"\"\n\n    def _process_error(error):\n        error_message = f'Tool Failed: {str(error)}\\n'\n        error_message += f\"Traceback:\\n{''.join(traceback.format_tb(error.__traceback__))}\"\n        return error_message\n\n    output = ''\n    try:\n        processed_code = _process_commands(code)\n        setup_encoding = '''\n        $PSDefaultParameterValues['*:Encoding'] = 'utf8'\n        [Console]::OutputEncoding = [System.Text.Encoding]::UTF8\n        [Console]::InputEncoding = [System.Text.Encoding]::UTF8\n        $OutputEncoding = [System.Text.Encoding]::UTF8\n        $env:PYTHONIOENCODING = \"utf-8\"\n        '''\n        wrapped_code = f'{setup_encoding}; {processed_code}'\n\n        startupinfo = None\n        if os.name == 'nt':\n            startupinfo = subprocess.STARTUPINFO()\n            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n\n        process = subprocess.Popen(\n            ['powershell', '-NoProfile', '-NonInteractive', '-Command', wrapped_code],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            startupinfo=startupinfo,\n            encoding='utf-8',\n            errors='replace'\n        )\n        stdout, stderr = process.communicate(timeout=300)\n\n        output = stdout\n        if stderr:\n            output += stderr\n\n    except subprocess.TimeoutExpired as e:\n        process.kill()\n        output += 'Error: Command execution timed out after 300 seconds.'\n    except Exception as e:\n        output += _process_error(e)\n\n    if output_length_limit is not None and output:\n        output_length_limit = int(output_length_limit)\n        lines = output.splitlines()\n        if len(lines) > output_length_limit:\n            half_limit = output_length_limit // 2\n            start_lines = lines[:half_limit]\n            end_lines = lines[-half_limit:]\n            lines_omitted = len(lines) - output_length_limit\n            truncated_output = '\\n'.join(start_lines)\n            truncated_output += f'\\n\\n... {lines_omitted} lines omitted ...\\n\\n'\n            truncated_output += '\\n'.join(end_lines)\n            \n            output_file = os.path.join(os.getcwd(), 'ps_output.txt')\n            with open(output_file, 'w', encoding='utf-8', errors='replace') as f:\n                f.write(output)\n            truncated_output += f'\\nFull output saved to {output_file}'\n            return truncated_output\n\n    return output\n\n\ndef _process_commands(code: str) -> str:\n    \"\"\"\n    Process PowerShell commands separated by &&, ensuring each command only runs if the previous succeeded.\n    Uses PowerShell error handling to catch both command failures and non-existent commands.\n    \n    Enhanced to better handle complex multiline code blocks.\n\n    Args:\n        code (str): The original command string with && separators\n\n    Returns:\n        str: PowerShell code with proper error checking between commands\n    \"\"\"\n    # Don't process commands that look like multiline code blocks\n    if '\\n' in code.strip() and '&&' not in code:\n        return code\n        \n    # Split on && but be more careful about it\n    commands = []\n    current_cmd = \"\"\n    i = 0\n    in_quotes = False\n    quote_char = None\n    \n    while i < len(code):\n        char = code[i]\n        \n        if char in ['\"', \"'\"] and (i == 0 or code[i-1] != '\\\\'):\n            if not in_quotes:\n                in_quotes = True\n                quote_char = char\n            elif char == quote_char:\n                in_quotes = False\n                quote_char = None\n        \n        if not in_quotes and i < len(code) - 1 and code[i:i+2] == '&&':\n            commands.append(current_cmd.strip())\n            current_cmd = \"\"\n            i += 2\n            continue\n            \n        current_cmd += char\n        i += 1\n    \n    if current_cmd.strip():\n        commands.append(current_cmd.strip())\n    \n    if len(commands) <= 1:\n        return code\n\n    processed_commands = []\n    for cmd in commands:\n        wrapped_cmd = f'$ErrorActionPreference = \"Stop\"; try {{ {cmd}; $LastSuccess = $true }} catch {{ $LastSuccess = $false; $_ }}'\n        processed_commands.append(wrapped_cmd)\n\n    return '; '.join([processed_commands[0]] + [f'if ($LastSuccess) {{ {cmd} }}' for cmd in processed_commands[1:]])\n",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py",
    "code_hash": "57733f0b9840cdc8ee4b6fefb30c62fb",
    "globals": {
     "os": "<module 'os' (frozen)>",
     "subprocess": "<module 'subprocess' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\subprocess.py'>",
     "traceback": "<module 'traceback' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\traceback.py'>",
     "time": "<module 'time' (built-in)>",
     "threading": "<module 'threading' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\threading.py'>",
     "Queue": "<class 'queue.Queue'>",
     "Empty": "<class '_queue.Empty'>",
     "Thread": "<class 'threading.Thread'>",
     "Lock": "<built-in function allocate_lock>",
     "local": "<class '_thread._local'>",
     "Dict": "typing.Dict",
     "Generator": "typing.Generator",
     "datetime": "<class 'datetime.datetime'>",
     "base64": "<module 'base64' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\base64.py'>",
     "tempfile": "<module 'tempfile' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\tempfile.py'>",
     "log_errors": "<function log_errors at 0x000001D7A9D045E0>",
     "handle_errors": "<function handle_errors at 0x000001D7A9D04720>",
     "execute_powershell": "<function execute_powershell at 0x000001D7A9E86520>",
     "PowerShellSession": "<class 'dynamic_module_d64ce3a2b8cfb34ac3eb473953440645.PowerShellSession'>",
     "PowerShellManager": "<class 'dynamic_module_d64ce3a2b8cfb34ac3eb473953440645.PowerShellManager'>",
     "_get_active_sessions": "<function _get_active_sessions at 0x000001D7A9E86200>",
     "_execute_powershell_stateless": "<function _execute_powershell_stateless at 0x000001D7A9E87100>",
     "_process_commands": "<function _process_commands at 0x000001D7A9E871A0>"
    }
   },
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py": {
    "name": "dynamic_module_2feb8c52b8785b27616df3d91d664fd4",
    "source": "import ast\nimport os\nimport textwrap\nfrom bots.utils.helpers import _process_error, _clean, _py_ast_to_source\nimport hashlib\nimport re\nfrom typing import Dict, Tuple, List\nfrom enum import Enum\n\n\n### CLASSES ##\n\nclass TokenType(Enum):\n    \"\"\"Types of tokens for metadata-driven processing\"\"\"\n    STANDALONE_COMMENT = 'standalone_comment'\n    INLINE_COMMENT = 'inline_comment'\n    IMPORT_COMMENT = 'import_comment'\n    COMPOUND_COMMENT = 'compound_comment'\n    STRING_LITERAL = 'string_literal'\n    FSTRING_LITERAL = 'fstring_literal'\n    MULTILINE_STRING = 'multiline_string'\n\nclass ScopeTransformer(ast.NodeTransformer):\n    \"\"\"AST transformer that handles scope-based Python code modifications.\"\"\"\n\n    def __init__(self, path_elements, new_nodes, insert_after=None, file_lines=None, file_tokens=None, new_tokens=None):\n        self.original_path = path_elements\n        self.new_nodes = new_nodes\n        self.insert_after = insert_after\n        self.file_lines = file_lines\n        self.file_tokens = file_tokens or {}\n        self.new_tokens = new_tokens or {}\n        self.current_path = []\n        self.success = False\n        self.line_match_count = 0\n        self.all_tokens = {**self.file_tokens, **self.new_tokens}\n\n    def visit_ClassDef(self, node):\n        \"\"\"Visit a class definition node.\"\"\"\n        if not self.original_path or node.name != self.original_path[0]:\n            return self.generic_visit(node)\n        self.current_path.append(node.name)\n        remaining_path = self.original_path[1:]\n        if not remaining_path:\n            if self.insert_after:\n                node = self._handle_insertion(node)\n            else:\n                self.success = True\n                if len(self.new_nodes) == 1:\n                    return self.new_nodes[0]\n                else:\n                    node.body = self.new_nodes\n                    return node\n        else:\n            saved_path = self.original_path\n            self.original_path = remaining_path\n            node.body = [self.visit(child) for child in node.body]\n            self.original_path = saved_path\n        self.current_path.pop()\n        return node\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Visit a function definition node.\"\"\"\n        return self._handle_function_node(node)\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"Visit an async function definition node.\"\"\"\n        return self._handle_function_node(node)\n\n    def _handle_function_node(self, node):\n        \"\"\"Common handling for both regular and async functions.\"\"\"\n        if not self.original_path or node.name != self.original_path[0]:\n            return self.generic_visit(node)\n        self.current_path.append(node.name)\n        remaining_path = self.original_path[1:]\n        if not remaining_path:\n            if self.insert_after:\n                node = self._handle_insertion(node)\n            else:\n                self.success = True\n                if len(self.new_nodes) == 1:\n                    return self.new_nodes[0]\n                else:\n                    node.body = self.new_nodes\n                    return node\n        else:\n            saved_path = self.original_path\n            self.original_path = remaining_path\n            node.body = [self.visit(child) for child in node.body]\n            self.original_path = saved_path\n        self.current_path.pop()\n        return node\n\n    def _handle_one_line_function(self, node, line):\n        \"\"\"Special handling for one-line function definitions\"\"\"\n        if ': pass' in line or ':pass' in line:\n            base_indent = len(line) - len(line.lstrip())\n            def_line = line.rstrip()\n            if self.insert_after == 'pass':\n                def_line = def_line.replace(': pass', ':').replace(':pass', ':')\n                body_indent = base_indent + 4\n                body_lines = []\n                body_lines.append(' ' * body_indent + 'pass')\n                for new_node in self.new_nodes:\n                    node_lines = _py_ast_to_source(new_node).split('\\n')\n                    for nl in node_lines:\n                        if nl.strip():\n                            body_lines.append(' ' * body_indent + nl.lstrip())\n                new_source = def_line + '\\n' + '\\n'.join(body_lines)\n                try:\n                    new_node = ast.parse(new_source).body[0]\n                    self.success = True\n                    return new_node\n                except Exception:\n                    normalized = textwrap.dedent(new_source)\n                    try:\n                        new_node = ast.parse(normalized).body[0]\n                        self.success = True\n                        return new_node\n                    except Exception:\n                        pass\n        return node\n\n    def _handle_insertion(self, node):\n        \"\"\"Handle inserting nodes after a specific point.\"\"\"\n        if isinstance(self.insert_after, str) and '::' not in self.insert_after:\n            if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                return node\n            if node.lineno == node.end_lineno:\n                return self._handle_one_line_function(node, self.file_lines[node.lineno - 1])\n            func_lines = self.file_lines[node.lineno - 1:node.end_lineno]\n            if not func_lines:\n                return node\n            target = self.insert_after.strip()\n            target_line_idx = None\n            for i, line in enumerate(func_lines):\n                detokenized_line = _detokenize_source(line, self.all_tokens)\n                if detokenized_line.strip() == target:\n                    self.line_match_count += 1\n                    if self.line_match_count == 1:\n                        target_line_idx = i\n            if self.line_match_count == 0:\n                return node\n            elif self.line_match_count > 1:\n                raise ValueError(f'Ambiguous insert_after - found {self.line_match_count} matches for: {self.insert_after}')\n            target_absolute_line = node.lineno + target_line_idx\n            containing_node = self._find_containing_node(node, target_absolute_line)\n            if containing_node and containing_node is not node:\n                self._insert_into_node(containing_node, target_absolute_line)\n            else:\n                insert_index = len(node.body)\n                for idx, child in enumerate(node.body):\n                    if hasattr(child, 'lineno') and child.lineno > target_absolute_line:\n                        insert_index = idx\n                        break\n                if insert_index == len(node.body):\n                    node.body.extend(self.new_nodes)\n                else:\n                    for i, new_node in enumerate(self.new_nodes):\n                        node.body.insert(insert_index + i, new_node)\n            self.success = True\n            return node\n        else:\n            if self.insert_after:\n                target_path = self.insert_after.split('::')\n                current_path = self.current_path\n                if len(target_path) > 1 and len(current_path) == len(target_path) - 1:\n                    if all((c == t for c, t in zip(current_path, target_path[:len(current_path)]))):\n                        target_name = target_path[-1]\n                        insert_index = None\n                        for idx, child in enumerate(node.body):\n                            if hasattr(child, 'name') and child.name == target_name:\n                                insert_index = idx + 1\n                                break\n                        if insert_index is not None:\n                            for i, new_node in enumerate(self.new_nodes):\n                                node.body.insert(insert_index + i, new_node)\n                            self.success = True\n                            return node\n                if len(current_path) == len(target_path) and all((c == t for c, t in zip(current_path, target_path))):\n                    self.success = True\n                    if isinstance(node, ast.ClassDef):\n                        node.body.extend(self.new_nodes)\n                    else:\n                        for new_node in self.new_nodes:\n                            node.body.append(new_node)\n            return node\n\n    def _find_containing_node(self, node, target_line):\n        \"\"\"Find the deepest AST node with a body that contains the target line.\"\"\"\n        if not (hasattr(node, 'lineno') and hasattr(node, 'end_lineno')):\n            return None\n        if not node.lineno <= target_line <= node.end_lineno:\n            return None\n        deepest_with_body = node if hasattr(node, 'body') else None\n        if hasattr(node, 'body'):\n            for child in node.body:\n                deeper_node = self._find_containing_node(child, target_line)\n                if deeper_node:\n                    return deeper_node\n        if hasattr(node, 'orelse'):\n            for child in node.orelse:\n                deeper_node = self._find_containing_node(child, target_line)\n                if deeper_node:\n                    return deeper_node\n        if deepest_with_body:\n            return deepest_with_body\n        return None\n\n    def _insert_into_node(self, node, target_line):\n        \"\"\"Insert new nodes into the given node after the target line.\"\"\"\n        if not hasattr(node, 'body'):\n            return\n        insert_index = len(node.body)\n        for idx, child in enumerate(node.body):\n            if hasattr(child, 'lineno') and child.lineno > target_line:\n                insert_index = idx\n                break\n        for i, new_node in enumerate(self.new_nodes):\n            node.body.insert(insert_index + i, new_node)\n\n### ENTRY POINT ###\n\nfrom bots.dev.decorators import handle_errors\n@handle_errors\ndef python_edit(target_scope: str, code: str, *, insert_after: str=None) -> str:\n    \"\"\"\n    Edit Python code using pytest-style scope syntax.\n    Default behavior: Replace entire target scope.\n\n    Parameters:\n    ----------\n    target_scope : str\n        Location to edit in pytest-style scope syntax:\n        - \"file.py\" (whole file)\n        - \"file.py::MyClass\" (class)\n        - \"file.py::my_function\" (function)\n        - \"file.py::MyClass::method\" (method)\n        - \"file.py::Outer::Inner::method\" (nested)\n        - \"file.py::utils::helper_func\" (nested function)\n\n    code : str\n        Python code to insert/replace. Will be cleaned/dedented.\n        Import statements will be automatically extracted and handled.\n\n    insert_after : str, optional\n        Where to insert the code. Can be either:\n        - \"__FILE_START__\" (special token for file beginning)\n        - A scope (\"MyClass::method\")\n        - An exact line match (like a context line in git patches)\n        If not specified, replaces the node at target_scope.\n\n    Returns:\n    --------\n    str\n        Description of what was modified or error message\n    \"\"\"\n    try:\n        file_path, *path_elements = target_scope.split('::')\n        if not file_path.endswith('.py'):\n            return _process_error(ValueError(f'File path must end with .py: {file_path}'))\n        for element in path_elements:\n            if not element.isidentifier():\n                return _process_error(ValueError(f'Invalid identifier in path: {element}'))\n        abs_path = _make_file(file_path)\n        cleaned_code = _clean(code)\n        tokenized_code, new_code_tokens = _tokenize_source(cleaned_code)\n        try:\n            new_tree = ast.parse(tokenized_code)\n            import_nodes = []\n            code_nodes = []\n            for node in new_tree.body:\n                if isinstance(node, (ast.Import, ast.ImportFrom)):\n                    import_nodes.append(node)\n                else:\n                    code_nodes.append(node)\n        except Exception as e:\n            return _process_error(ValueError(f'Error parsing new code: {str(e)}'))\n        try:\n            with open(abs_path, 'r', encoding='utf-8') as file:\n                original_content = file.read()\n            \n            # Extract top-level comments before processing\n            top_level_comments = []\n            if original_content.strip():\n                original_content, top_level_comments = _extract_top_level_comments(original_content)\n            \n            tokenized_content, file_tokens = _tokenize_source(original_content) if original_content.strip() else ('', {})\n            file_lines = tokenized_content.split('\\n')\n            tree = ast.parse(tokenized_content) if original_content.strip() else ast.Module(body=[], type_ignores=[])\n        except Exception as e:\n            return _process_error(ValueError(f'Error reading/parsing file {abs_path}: {str(e)}'))\n        existing_imports = []\n        non_imports = []\n        for node in tree.body:\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                existing_imports.append(node)\n            else:\n                non_imports.append(node)\n        if insert_after == '__FILE_START__':\n            tree.body = import_nodes + code_nodes + existing_imports + non_imports\n            try:\n                updated_content = _py_ast_to_source(tree)\n                all_tokens = {**file_tokens, **new_code_tokens}\n                final_content = _detokenize_source(updated_content, all_tokens)\n                final_content = _preserve_blank_lines(final_content, original_content)\n                # Restore top-level comments\n                final_content = _restore_top_level_comments(final_content, top_level_comments)\n                with open(abs_path, 'w', encoding='utf-8') as file:\n                    file.write(final_content)\n                return f\"Code inserted at start of '{abs_path}'.\"\n            except Exception as e:\n                return _process_error(e)\n        if not path_elements:\n            tree.body = existing_imports + import_nodes + non_imports + code_nodes\n            try:\n                updated_content = _py_ast_to_source(tree)\n                all_tokens = {**file_tokens, **new_code_tokens}\n                final_content = _detokenize_source(updated_content, all_tokens)\n                final_content = _preserve_blank_lines(final_content, original_content)\n                # Restore top-level comments\n                final_content = _restore_top_level_comments(final_content, top_level_comments)\n                with open(abs_path, 'w', encoding='utf-8') as file:\n                    file.write(final_content)\n                return f\"Code added at file level in '{abs_path}'.\"\n            except Exception as e:\n                return _process_error(e)\n        tree.body = existing_imports + import_nodes + non_imports\n        transformer = ScopeTransformer(path_elements, code_nodes, insert_after, file_lines, file_tokens, new_code_tokens)\n        modified_tree = transformer.visit(tree)\n        if not transformer.success:\n            if insert_after:\n                return _process_error(ValueError(f'Insert point not found: {insert_after}'))\n            else:\n                return _process_error(ValueError(f'Target scope not found: {target_scope}'))\n        try:\n            updated_content = _py_ast_to_source(modified_tree)\n            all_tokens = {**file_tokens, **new_code_tokens}\n            final_content = _detokenize_source(updated_content, all_tokens)\n            final_content = _preserve_blank_lines(final_content, original_content)\n            # Restore top-level comments\n            final_content = _restore_top_level_comments(final_content, top_level_comments)\n            with open(abs_path, 'w', encoding='utf-8') as file:\n                file.write(final_content)\n            action = 'inserted after' if insert_after else 'replaced at'\n            scope_str = '::'.join(path_elements) if path_elements else 'file level'\n            return f\"Code {action} {scope_str} in '{abs_path}'.\"\n        except Exception as e:\n            return _process_error(e)\n    except Exception as e:\n        return _process_error(e)\n\n\n### HELPER FUNCTIONS ###\n\ndef _make_file(file_path: str) -> str:\n    \"\"\"\n    Creates a file and its parent directories if they don't exist.\n    Converts relative paths to absolute paths.\n\n    Parameters:\n    - file_path (str): The path to the file to be created\n\n    Returns:\n    - str: The absolute path to the file\n\n    Raises:\n    - ValueError: If there's an error creating the file or directories,\n                 or if the file_path is empty\n    \"\"\"\n    if not file_path:\n        raise ValueError('File path cannot be empty')\n    abs_path = os.path.abspath(file_path)\n    dir_path = os.path.dirname(abs_path)\n    if dir_path:\n        try:\n            os.makedirs(dir_path, exist_ok=True)\n        except Exception as e:\n            raise ValueError(f'Error creating directories {dir_path}: {str(e)}')\n    if not os.path.exists(abs_path):\n        try:\n            with open(abs_path, 'w', encoding='utf-8') as f:\n                f.write('')\n        except Exception as e:\n            raise ValueError(f'Error creating file {abs_path}: {str(e)}')\n    return abs_path\n\ndef _create_token(content: str, index: int, current_hash: str, token_type: TokenType=None, extra_metadata: dict=None) -> Tuple[str, Dict]:\n    \"\"\"Create a token with our specific pattern and metadata\"\"\"\n    token_name = f'TOKEN_{current_hash}_{index}'\n    metadata = extra_metadata or {}\n    if token_type:\n        metadata['type'] = token_type.value\n    token_data = {'content': content, 'metadata': metadata}\n    return (token_name, token_data)\n\ndef _get_file_hash(content: str) -> str:\n    \"\"\"Create a short hash of file content\"\"\"\n    return hashlib.sha256(content.encode()).hexdigest()[:8]\n\ndef _contains_token(s: str) -> bool:\n    return 'TOKEN_' in s\n\ndef _find_string_end(line, start_pos, quote_char):\n    \"\"\"Find the end position of a quoted string, handling escape sequences properly.\"\"\"\n    pos = start_pos + 1\n    while pos < len(line):\n        if line[pos] == '\\\\' and pos + 1 < len(line):\n            # Skip the backslash and the escaped character\n            pos += 2\n        elif line[pos] == quote_char:\n            return pos\n        else:\n            pos += 1\n    return -1  # No closing quote found\n\ndef _is_fstring_start(line, pos):\n    \"\"\"Check if position starts an f-string (f\", f', rf\", etc.)\"\"\"\n    if pos >= len(line) or line[pos] not in ['\"', \"'\"]:\n        return False, False  # Return tuple (is_fstring, is_raw)\n    \n    # Check for f-string prefixes\n    prefixes = {\n        'f': (True, False),\n        'F': (True, False),\n        'rf': (True, True),\n        'Rf': (True, True),\n        'rF': (True, True),\n        'RF': (True, True),\n        'fr': (True, True),\n        'Fr': (True, True),\n        'fR': (True, True),\n        'FR': (True, True)\n    }\n    \n    for prefix, (is_f, is_raw) in prefixes.items():\n        prefix_start = pos - len(prefix)\n        if (prefix_start >= 0 and \n            line[prefix_start:pos] == prefix and\n            (prefix_start == 0 or not line[prefix_start-1].isalnum())):\n            return is_f, is_raw\n    \n    return False, False\n\ndef _find_fstring_end(line, start_pos, quote_char, is_raw=False):\n    \"\"\"Find the end of an f-string, handling nested braces and expressions.\"\"\"\n    pos = start_pos + 1\n    brace_depth = 0\n    \n    while pos < len(line):\n        char = line[pos]\n        \n        if char == '{':\n            if pos + 1 < len(line) and line[pos + 1] == '{':\n                # Escaped brace {{\n                pos += 2\n                continue\n            else:\n                # Start of expression\n                brace_depth += 1\n                pos += 1\n                continue\n                \n        elif char == '}':\n            if pos + 1 < len(line) and line[pos + 1] == '}':\n                # Escaped brace }}\n                pos += 2\n                continue\n            else:\n                # End of expression\n                brace_depth -= 1\n                pos += 1\n                continue\n                \n        elif char == quote_char and brace_depth == 0:\n            # Found closing quote at top level\n            return pos\n            \n        elif char == '\\\\' and not is_raw and pos + 1 < len(line):\n            # Handle escape sequences (but not in raw strings)\n            pos += 2\n            continue\n            \n        elif char in ['\"', \"'\"] and brace_depth > 0:\n            # Handle nested quotes inside f-string expressions\n            nested_end = _find_string_end(line, pos, char)\n            if nested_end == -1:\n                return -1\n            pos = nested_end + 1\n            continue\n            \n        pos += 1\n    \n    return -1  # Unterminated f-string\n\ndef _determine_comment_type(code_stripped: str) -> Tuple[TokenType, dict]:\n    \"\"\"Determine the type of comment based on the preceding code.\"\"\"\n    \n    # Strip indentation for pattern matching\n    code_no_indent = code_stripped.lstrip()\n    \n    if code_no_indent.startswith(('import ', 'from ')):\n        return TokenType.IMPORT_COMMENT, {'import_statement': code_stripped}\n    \n    compound_patterns = [\n        'def ', 'class ', 'if ', 'elif ', 'for ', 'while ', 'with ', 'async def ', 'try:', 'except ', 'finally:', 'else:'\n    ]\n    \n    # Check if this is a compound statement (starts with compound keyword and ends with colon)\n    # Use the stripped version for pattern matching but keep original for colon check\n    is_compound = (\n        any(code_no_indent.startswith(pattern) for pattern in compound_patterns) \n        and code_stripped.endswith(':')\n    ) or code_stripped.strip() in ['else:', 'try:', 'finally:']\n    \n    if is_compound:\n        return TokenType.COMPOUND_COMMENT, {'statement': code_stripped}\n    \n    return TokenType.INLINE_COMMENT, {}\n\ndef _get_indentation_at_position(source: str, pos: int) -> str:\n    \"\"\"Get the indentation level at a given position in source\"\"\"\n    line_start = source.rfind('\\n', 0, pos) + 1\n    return source[line_start:pos].replace(source[line_start:pos].lstrip(), '')\n\ndef _indent_multiline_content(content: str, indent: str) -> str:\n    \"\"\"Indent multiline content while preserving internal formatting\"\"\"\n    if not content.strip():\n        return content\n    lines = content.split('\\n')\n    if len(lines) == 1:\n        return content\n    if all((line.startswith(' ') for line in lines[1:] if line.strip())):\n        return content\n    result = [lines[0]]\n    for line in lines[1:]:\n        if line.strip():\n            result.append(indent + line)\n        else:\n            result.append(line)\n    return '\\n'.join(result)\n\ndef _preserve_blank_lines(result: str, original_source: str) -> str:\n    \"\"\"\n    Restore blank lines around comments that had them in the original source.\n    \"\"\"\n    original_lines = original_source.split('\\n')\n    result_lines = result.split('\\n')\n    comments_with_blanks = {}\n    for i, line in enumerate(original_lines):\n        if line.strip().startswith('#') and 'marker' in line.lower():\n            blank_before = i > 0 and (not original_lines[i - 1].strip())\n            blank_after = i < len(original_lines) - 1 and (not original_lines[i + 1].strip())\n            if blank_before or blank_after:\n                comments_with_blanks[line.strip()] = {'blank_before': blank_before, 'blank_after': blank_after}\n    if not comments_with_blanks:\n        return result\n    new_result_lines = []\n    i = 0\n    while i < len(result_lines):\n        line = result_lines[i]\n        line_stripped = line.strip()\n        if line_stripped in comments_with_blanks:\n            info = comments_with_blanks[line_stripped]\n            if info['blank_before']:\n                if i > 0 and new_result_lines[-1].strip():\n                    new_result_lines.append('')\n            new_result_lines.append(line)\n            if info['blank_after']:\n                if i < len(result_lines) - 1 and result_lines[i + 1].strip():\n                    new_result_lines.append('')\n        else:\n            new_result_lines.append(line)\n        i += 1\n    return '\\n'.join(new_result_lines)\n\n\ndef _analyze_string_at_position_in_source(source, pos, paren_depth):\n    \"\"\"\n    Analyze a potential string starting at the given position in the full source.\n    Enhanced with better bounds checking and comment awareness.\n    \"\"\"\n    if pos >= len(source):\n        return None\n        \n    char = source[pos]\n    if char not in ['\"', \"'\"]:\n        return None\n    \n    # Check if this is an f-string\n    is_fstring, is_raw = _is_fstring_start(source, pos)\n    \n    if is_fstring:\n        # Find the actual start of the f-string prefix\n        string_start = pos\n        for prefix in ['rf', 'Rf', 'rF', 'RF', 'fr', 'Fr', 'fR', 'FR', 'f', 'F']:\n            prefix_start = pos - len(prefix)\n            if (prefix_start >= 0 and \n                source[prefix_start:pos] == prefix and\n                (prefix_start == 0 or not source[prefix_start-1].isalnum())):\n                string_start = prefix_start\n                break\n        \n        end_pos = _find_fstring_end(source, pos, char, is_raw)\n    else:\n        string_start = pos\n        end_pos = _find_string_end(source, pos, char)\n    \n    if end_pos == -1:\n        # Invalid/unterminated string\n        return None\n    \n    # Extract the complete string content\n    string_content = source[string_start:end_pos + 1]\n    \n    # Validate the string - additional safety checks\n    if len(string_content) < 2:\n        return None\n    \n    # Sanity check: if this is a single-quoted string, it shouldn't contain \n    # unescaped newlines (except for triple quotes)\n    if (string_content.startswith(\"'\") and not string_content.startswith(\"'''\") and \n        '\\n' in string_content):\n        # This is likely an over-capture - reject it\n        return None\n    \n    if (string_content.startswith('\"') and not string_content.startswith('\"\"\"') and \n        '\\n' in string_content and not is_fstring):\n        # This is likely an over-capture - reject it  \n        return None\n    \n    return {\n        'start': string_start,\n        'end': end_pos,\n        'content': string_content,\n        'is_fstring': is_fstring,\n        'is_raw': is_raw,\n        'paren_depth': paren_depth,\n        'quote_char': char\n    }\n\ndef _find_string_end(line, start_pos, quote_char):\n    \"\"\"\n    Find the end position of a quoted string, handling escape sequences properly.\n    Enhanced with better validation.\n    \"\"\"\n    if start_pos >= len(line):\n        return -1\n        \n    pos = start_pos + 1\n    while pos < len(line):\n        if line[pos] == '\\\\' and pos + 1 < len(line):\n            # Skip the backslash and the escaped character\n            pos += 2\n        elif line[pos] == quote_char:\n            return pos\n        elif line[pos] == '\\n' and quote_char in ['\"', \"'\"]:\n            # Single/double quoted strings shouldn't span lines (unless escaped)\n            # This indicates we've gone too far\n            return -1\n        else:\n            pos += 1\n    return -1  # No closing quote found\n\ndef _find_all_string_locations_in_source(source):\n    \"\"\"\n    Find all string literal locations in the entire source, being aware of comments.\n    Skip quotes that appear inside comments to avoid over-capturing.\n    \"\"\"\n    locations = []\n    paren_depth = 0\n    pos = 0\n    \n    while pos < len(source):\n        char = source[pos]\n        \n        # Track parentheses depth\n        if char == '(':\n            paren_depth += 1\n            pos += 1\n            continue\n        elif char == ')':\n            paren_depth -= 1\n            pos += 1\n            continue\n        \n        # Check for comments - if we hit a '#', skip to end of line\n        if char == '#':\n            # Skip to end of line to avoid processing quotes in comments\n            while pos < len(source) and source[pos] != '\\n':\n                pos += 1\n            # pos is now at '\\n' or end of source\n            continue\n        \n        # Look for string starts\n        if char in ['\"', \"'\"]:\n            # Check if this quote is inside a comment by looking back on the current line\n            line_start = source.rfind('\\n', 0, pos) + 1\n            line_before_quote = source[line_start:pos]\n            \n            # If there's a '#' in the line before this quote (and no string quotes to protect it),\n            # then this quote is inside a comment\n            if _is_quote_in_comment(line_before_quote):\n                pos += 1\n                continue\n            \n            # This is a real string quote, process it\n            string_info = _analyze_string_at_position_in_source(source, pos, paren_depth)\n            if string_info:\n                locations.append(string_info)\n                pos = string_info['end'] + 1  # Skip past this string\n            else:\n                pos += 1\n        else:\n            pos += 1\n    \n    return locations\n\ndef _is_quote_in_comment(line_before_quote):\n    \"\"\"\n    Determine if a quote is inside a comment by analyzing the line content before it.\n    \n    Returns True if the quote is inside a comment, False otherwise.\n    \"\"\"\n    # Simple approach: look for '#' that's not inside a string\n    in_string = False\n    string_char = None\n    i = 0\n    \n    while i < len(line_before_quote):\n        char = line_before_quote[i]\n        \n        if not in_string:\n            if char == '#':\n                # Found unprotected '#' - everything after is a comment\n                return True\n            elif char in ['\"', \"'\"]:\n                # Start of string\n                in_string = True\n                string_char = char\n        else:\n            # Inside a string\n            if char == '\\\\' and i + 1 < len(line_before_quote):\n                # Skip escaped character\n                i += 2\n                continue\n            elif char == string_char:\n                # End of string\n                in_string = False\n                string_char = None\n        \n        i += 1\n    \n    return False\n\n### TOKENIZATION ###\n\ndef _tokenize_source(source: str) -> Tuple[str, Dict[str, Dict]]:\n    \"\"\"\n    Tokenize source code, preserving exact formatting.\n    \n    Returns:\n    - tokenized_source: Source with tokens inserted\n    - token_map: Mapping of tokens to {'content': str, 'metadata': dict}\n    \"\"\"\n    token_map = {}\n    current_hash = _get_file_hash(source)\n    token_counter = 0\n    \n    # First pass: handle multi-line strings BEFORE any other processing\n    tokenized = _process_multiline_strings(source, token_map, token_counter, current_hash)\n    token_counter = len(token_map)\n    \n    # Second pass: process ALL string literals in the entire source at once\n    # CRITICAL: Do this BEFORE splitting into lines to preserve escape sequences\n    tokenized, token_counter = _process_all_string_literals(tokenized, token_map, token_counter, current_hash)\n    \n    # Third pass: process line by line for comments ONLY after string processing is complete\n    lines = tokenized.split('\\n')\n    processed_lines = []\n    \n    for line in lines:\n        processed_line, token_counter = _process_line_comments(\n            line, token_map, token_counter, current_hash\n        )\n        processed_lines.append(processed_line)\n    \n    return '\\n'.join(processed_lines), token_map\n\n\ndef _extract_top_level_comments(source: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Extract top-level standalone comments that appear before any code.\n    \n    Returns:\n        (source_without_top_comments, list_of_top_comment_lines)\n    \"\"\"\n    lines = source.split('\\n')\n    top_comments = []\n    first_code_line_idx = 0\n    \n    # Find all leading comments and blank lines\n    for i, line in enumerate(lines):\n        stripped = line.strip()\n        if not stripped:  # Blank line\n            continue\n        elif stripped.startswith('#'):  # Comment line\n            top_comments.append(line)\n        else:  # First non-comment, non-blank line\n            first_code_line_idx = i\n            break\n    else:\n        # File is only comments and blank lines\n        first_code_line_idx = len(lines)\n    \n    # If we found top-level comments, remove them from source\n    if top_comments:\n        remaining_lines = lines[first_code_line_idx:]\n        source_without_comments = '\\n'.join(remaining_lines)\n        return source_without_comments, top_comments\n    \n    return source, []\n\ndef _process_line_comments(line: str, token_map: dict, token_counter: int, current_hash: str) -> Tuple[str, int]:\n    \"\"\"Process comments on a single line, after string processing.\"\"\"\n    if not line.strip():\n        return line, token_counter\n    \n    indent = len(line) - len(line.lstrip())\n    indentation = ' ' * indent\n    content = line[indent:]\n    \n    # Handle standalone comments\n    if content.strip().startswith('#'):\n        token_name, token_data = _create_token(\n            content.strip(), token_counter, current_hash, TokenType.STANDALONE_COMMENT\n        )\n        token_map[token_name] = token_data\n        return f\"{indentation}{token_name}\", token_counter + 1\n    \n    # Handle inline comments - pass the FULL line, not just content\n    if '#' in content:\n        # Check if the # is already inside a token\n        hash_positions = [i for i, c in enumerate(content) if c == '#']\n        \n        for hash_pos in hash_positions:\n            # Check if this # is inside any existing token\n            is_inside_token = False\n            for token_name in token_map:\n                if token_name in content:\n                    token_start = content.find(token_name)\n                    token_end = token_start + len(token_name)\n                    if token_start <= hash_pos < token_end:\n                        is_inside_token = True\n                        break\n            \n            if not is_inside_token:\n                # This is a real comment, process it with the FULL line\n                processed_line, new_counter = _process_inline_comment(\n                    line, token_map, token_counter, current_hash\n                )\n                return processed_line, new_counter\n    \n    return line, token_counter\n\ndef _process_multiline_strings(source: str, token_map: dict, token_counter: int, current_hash: str) -> str:\n    \"\"\"Process all triple-quoted strings in the source, including f-strings.\"\"\"\n    tokenized = source\n    processed_positions = set()\n    \n    # Extended patterns to handle f-strings\n    patterns = [\n        ('\"\"\"', '\"\"\"'),\n        (\"'''\", \"'''\"),\n        ('f\"\"\"', '\"\"\"'),\n        (\"f'''\", \"'''\"),\n        ('F\"\"\"', '\"\"\"'),\n        (\"F'''\", \"'''\"),\n        ('rf\"\"\"', '\"\"\"'),\n        (\"rf'''\", \"'''\"),\n        ('fr\"\"\"', '\"\"\"'),\n        (\"fr'''\", \"'''\"),\n        # Add all case variations\n        ('Rf\"\"\"', '\"\"\"'), (\"Rf'''\", \"'''\"),\n        ('rF\"\"\"', '\"\"\"'), (\"rF'''\", \"'''\"),\n        ('RF\"\"\"', '\"\"\"'), (\"RF'''\", \"'''\"),\n        ('Fr\"\"\"', '\"\"\"'), (\"Fr'''\", \"'''\"),\n        ('fR\"\"\"', '\"\"\"'), (\"fR'''\", \"'''\"),\n        ('FR\"\"\"', '\"\"\"'), (\"FR'''\", \"'''\"),\n    ]\n    \n    for start_pattern, end_pattern in patterns:\n        pos = 0\n        while True:\n            start_pos = tokenized.find(start_pattern, pos)\n            if start_pos == -1:\n                break\n            \n            # Check if already processed\n            if start_pos in processed_positions:\n                pos = start_pos + 1\n                continue\n            \n            # Find the matching end\n            end_pos = tokenized.find(end_pattern, start_pos + len(start_pattern))\n            if end_pos == -1:\n                pos = start_pos + 1\n                continue\n            \n            # Extract the complete string\n            end_pos += len(end_pattern)\n            string_content = tokenized[start_pos:end_pos]\n            \n            # Skip if it contains tokens or is too large\n            if _contains_token(string_content) or len(string_content) > 1000:\n                pos = start_pos + 1\n                continue\n            \n            # Mark as processed\n            processed_positions.add(start_pos)\n            \n            # Determine if it's an f-string\n            is_fstring = any(string_content.startswith(p) for p in ['f', 'F', 'rf', 'Rf', 'rF', 'RF', 'fr', 'Fr', 'fR', 'FR'])\n            \n            # Create token\n            token_name, token_data = _create_token(\n                string_content, token_counter, current_hash, \n                TokenType.MULTILINE_STRING,\n                {'is_fstring': is_fstring} if is_fstring else None\n            )\n            token_map[token_name] = token_data\n            \n            # Replace in source\n            tokenized = tokenized[:start_pos] + token_name + tokenized[end_pos:]\n            pos = start_pos + len(token_name)\n            token_counter += 1\n    \n    return tokenized\n\ndef _process_all_string_literals(source, token_map, token_counter, current_hash):\n    \"\"\"Process all string literals in the entire source at once, preserving escape sequences.\"\"\"\n    \n    if _contains_token(source):\n        # If source already contains tokens, we need to be more careful\n        # Process line by line to avoid conflicts with existing tokens\n        lines = source.split('\\n')\n        processed_lines = []\n        \n        for line in lines:\n            processed_line, token_counter = _process_string_literals(line, token_map, token_counter, current_hash)\n            processed_lines.append(processed_line)\n        \n        return '\\n'.join(processed_lines), token_counter\n    \n    # Step 1: Identify ALL string positions in the entire source\n    string_locations = _find_all_string_locations_in_source(source)\n    \n    if not string_locations:\n        return source, token_counter\n    \n    # Step 2: Process strings from right to left to avoid position shifts\n    # Sort by start position in descending order\n    string_locations.sort(key=lambda x: x['start'], reverse=True)\n    \n    result = source\n    \n    # Step 3: Replace each string with its token\n    for string_info in string_locations:\n        start = string_info['start']\n        end = string_info['end']\n        string_content = string_info['content']\n        is_fstring = string_info['is_fstring']\n        is_raw = string_info['is_raw']\n        paren_depth = string_info['paren_depth']\n        quote_char = string_info['quote_char']\n        \n        # Create token with proper metadata\n        metadata = {'is_fstring': is_fstring, 'is_raw': is_raw, 'in_parens': paren_depth > 0}\n        if paren_depth > 0:\n            metadata['quote_char'] = quote_char\n        \n        token_name, token_data = _create_token(\n            string_content, token_counter, current_hash, TokenType.STRING_LITERAL, metadata\n        )\n        token_map[token_name] = token_data\n        \n        # Determine replacement text\n        if paren_depth > 0:\n            replacement = f'{quote_char}{token_name}{quote_char}'\n        else:\n            replacement = token_name\n        \n        # Replace in result (working right-to-left so positions stay valid)\n        result = result[:start] + replacement + result[end + 1:]\n        token_counter += 1\n    \n    return result, token_counter\n\n\ndef _find_all_string_locations_in_source(source):\n    \"\"\"Find all string literal locations in the entire source, preserving escape sequences.\"\"\"\n    locations = []\n    paren_depth = 0\n    pos = 0\n    \n    while pos < len(source):\n        char = source[pos]\n        \n        # Track parentheses depth\n        if char == '(':\n            paren_depth += 1\n            pos += 1\n            continue\n        elif char == ')':\n            paren_depth -= 1\n            pos += 1\n            continue\n        \n        # Look for string starts\n        if char in ['\"', \"'\"]:\n            string_info = _analyze_string_at_position_in_source(source, pos, paren_depth)\n            if string_info:\n                locations.append(string_info)\n                pos = string_info['end'] + 1  # Skip past this string\n            else:\n                pos += 1\n        else:\n            pos += 1\n    \n    return locations\n\n\ndef _analyze_string_at_position_in_source(source, pos, paren_depth):\n    \"\"\"Analyze a potential string starting at the given position in the full source.\"\"\"\n    char = source[pos]\n    \n    # Check if this is an f-string\n    is_fstring, is_raw = _is_fstring_start(source, pos)\n    \n    if is_fstring:\n        # Find the actual start of the f-string prefix\n        string_start = pos\n        for prefix in ['rf', 'Rf', 'rF', 'RF', 'fr', 'Fr', 'fR', 'FR', 'f', 'F']:\n            prefix_start = pos - len(prefix)\n            if (prefix_start >= 0 and \n                source[prefix_start:pos] == prefix and\n                (prefix_start == 0 or not source[prefix_start-1].isalnum())):\n                string_start = prefix_start\n                break\n        \n        end_pos = _find_fstring_end(source, pos, char, is_raw)\n    else:\n        string_start = pos\n        end_pos = _find_string_end(source, pos, char)\n    \n    if end_pos == -1:\n        # Invalid/unterminated string\n        return None\n    \n    # Extract the complete string content\n    string_content = source[string_start:end_pos + 1]\n    \n    # Validate the string\n    if len(string_content) < 2:\n        return None\n    \n    return {\n        'start': string_start,\n        'end': end_pos,\n        'content': string_content,\n        'is_fstring': is_fstring,\n        'is_raw': is_raw,\n        'paren_depth': paren_depth,\n        'quote_char': char\n    }\n\n\ndef _process_string_literals(processed_line, token_map, token_counter, current_hash):\n    \"\"\"Process all strings in a line at once - fallback for when source already has tokens.\"\"\"\n    \n    if _contains_token(processed_line):\n        return processed_line, token_counter\n\n    # Step 1: Identify ALL string positions in the line\n    string_locations = _find_all_string_locations(processed_line)\n    \n    if not string_locations:\n        return processed_line, token_counter\n    \n    # Step 2: Process strings from right to left to avoid position shifts\n    # Sort by start position in descending order\n    string_locations.sort(key=lambda x: x['start'], reverse=True)\n    \n    result = processed_line\n    \n    # Step 3: Replace each string with its token\n    for string_info in string_locations:\n        start = string_info['start']\n        end = string_info['end']\n        string_content = string_info['content']\n        is_fstring = string_info['is_fstring']\n        is_raw = string_info['is_raw']\n        paren_depth = string_info['paren_depth']\n        quote_char = string_info['quote_char']\n        \n        # Create token with proper metadata\n        metadata = {'is_fstring': is_fstring, 'is_raw': is_raw, 'in_parens': paren_depth > 0}\n        if paren_depth > 0:\n            metadata['quote_char'] = quote_char\n        \n        token_name, token_data = _create_token(\n            string_content, token_counter, current_hash, TokenType.STRING_LITERAL, metadata\n        )\n        token_map[token_name] = token_data\n        \n        # Determine replacement text\n        if paren_depth > 0:\n            replacement = f'{quote_char}{token_name}{quote_char}'\n        else:\n            replacement = token_name\n        \n        # Replace in result (working right-to-left so positions stay valid)\n        result = result[:start] + replacement + result[end + 1:]\n        token_counter += 1\n    \n    return result, token_counter\n\n\ndef _find_all_string_locations(line):\n    \"\"\"Find all string literal locations in a line without tokenizing.\"\"\"\n    locations = []\n    paren_depth = 0\n    pos = 0\n    \n    while pos < len(line):\n        char = line[pos]\n        \n        # Track parentheses depth\n        if char == '(':\n            paren_depth += 1\n            pos += 1\n            continue\n        elif char == ')':\n            paren_depth -= 1\n            pos += 1\n            continue\n        \n        # Look for string starts\n        if char in ['\"', \"'\"]:\n            string_info = _analyze_string_at_position(line, pos, paren_depth)\n            if string_info:\n                locations.append(string_info)\n                pos = string_info['end'] + 1  # Skip past this string\n            else:\n                pos += 1\n        else:\n            pos += 1\n    \n    return locations\n\n\ndef _analyze_string_at_position(line, pos, paren_depth):\n    \"\"\"Analyze a potential string starting at the given position.\"\"\"\n    char = line[pos]\n    \n    # Check if this is an f-string\n    is_fstring, is_raw = _is_fstring_start(line, pos)\n    \n    if is_fstring:\n        # Find the actual start of the f-string prefix\n        string_start = pos\n        for prefix in ['rf', 'Rf', 'rF', 'RF', 'fr', 'Fr', 'fR', 'FR', 'f', 'F']:\n            prefix_start = pos - len(prefix)\n            if (prefix_start >= 0 and \n                line[prefix_start:pos] == prefix and\n                (prefix_start == 0 or not line[prefix_start-1].isalnum())):\n                string_start = prefix_start\n                break\n        \n        end_pos = _find_fstring_end(line, pos, char, is_raw)\n    else:\n        string_start = pos\n        end_pos = _find_string_end(line, pos, char)\n    \n    if end_pos == -1:\n        # Invalid/unterminated string\n        return None\n    \n    # Extract the complete string content\n    string_content = line[string_start:end_pos + 1]\n    \n    # Validate the string\n    if len(string_content) < 2:\n        return None\n    \n    return {\n        'start': string_start,\n        'end': end_pos,\n        'content': string_content,\n        'is_fstring': is_fstring,\n        'is_raw': is_raw,\n        'paren_depth': paren_depth,\n        'quote_char': char\n    }\n\ndef _process_inline_comment(line: str, token_map: dict, token_counter: int, current_hash: str) -> Tuple[str, int]:\n    \"\"\"Process inline comments, determining their type based on the preceding code.\"\"\"\n    # First check if the # is inside a string token\n    hash_positions = [i for i, c in enumerate(line) if c == '#']\n    \n    for hash_pos in hash_positions:\n        # Check if this # is inside any token\n        is_inside_token = False\n        for token_name in token_map:\n            if token_name in line:\n                token_start = line.find(token_name)\n                token_end = token_start + len(token_name)\n                if token_start <= hash_pos < token_end:\n                    is_inside_token = True\n                    break\n        \n        if not is_inside_token:\n            # This is a real comment\n            comment_start = hash_pos\n            code = line[:comment_start]\n            code_end = len(code.rstrip())\n            spacing_and_comment = line[code_end:]\n            code_stripped = code.rstrip()\n            \n            # Determine comment type and metadata\n            token_type, extra_metadata = _determine_comment_type(code_stripped)            \n            token_name, token_data = _create_token(\n                spacing_and_comment, token_counter, current_hash, token_type, extra_metadata\n            )\n            token_map[token_name] = token_data\n            \n            # For compound comments AND import comments, put token on separate line\n            if token_type == TokenType.COMPOUND_COMMENT:\n                # Get the base indentation of the current line\n                base_indent = len(line) - len(line.lstrip())\n                # Add 4 spaces for the expected function/class body indentation\n                token_indent = ' ' * (base_indent + 4)\n                processed_line = f\"{code.rstrip()}\\n{token_indent}{token_name}\"\n                return processed_line, token_counter + 1\n            elif token_type == TokenType.IMPORT_COMMENT:\n                # For import comments, also move to separate line (like compound comments)\n                # This ensures the import statement remains valid for AST parsing\n                base_indent = len(line) - len(line.lstrip())\n                token_indent = ' ' * base_indent  # Same indentation as import\n                processed_line = f\"{code.rstrip()}\\n{token_indent}{token_name}\"\n                return processed_line, token_counter + 1\n            else:\n                # For regular inline comments, check if we need semicolon\n                # Don't add semicolon if the code ends with certain characters\n                code_trimmed = code.rstrip()\n                \n                # Characters where semicolon is inappropriate\n                no_semicolon_endings = [',', ':', '(', '[', '{', '\\\\']\n                \n                if any(code_trimmed.endswith(char) for char in no_semicolon_endings):\n                    # For commas and other special endings, move comment to next line\n                    if code_trimmed.endswith(','):\n                        # Get the base indentation of the current line\n                        base_indent = len(line) - len(line.lstrip())\n                        # Use same indentation for the comment\n                        token_indent = ' ' * base_indent\n                        processed_line = f\"{code_trimmed}\\n{token_indent}{token_name}\"\n                    else:\n                        # For other special characters, preserve spacing but replace comment with token\n                        spacing_before_comment = line[len(code_trimmed):comment_start]\n                        processed_line = f\"{code_trimmed}{spacing_before_comment}{token_name}\"\n                else:\n                    # Add semicolon for regular inline comments\n                    processed_line = f\"{code.rstrip()}; {token_name}\"\n                \n                return processed_line, token_counter + 1\n    \n    # No comments found\n    return line, token_counter\n\n### DETOKENIZATION ###\n\ndef _detokenize_source(tokenized_source: str, token_map: Dict[str, Dict]) -> str:\n    \"\"\"\n    Restore original source from tokenized version using reverse tokenization order.\n    This prevents issues with nested tokens and maintains proper reconstruction sequence.\n    \"\"\"\n    result = tokenized_source\n    \n    # Group tokens by type for ordered processing\n    comment_tokens = []\n    string_tokens = []\n    multiline_tokens = []\n    other_tokens = []\n    \n    for token_name, token_data in token_map.items():\n        metadata = token_data['metadata']\n        token_type = metadata.get('type')\n        \n        if token_type in [TokenType.STANDALONE_COMMENT.value, TokenType.INLINE_COMMENT.value, \n                         TokenType.IMPORT_COMMENT.value, TokenType.COMPOUND_COMMENT.value]:\n            comment_tokens.append((token_name, token_data))\n        elif token_type == TokenType.STRING_LITERAL.value:\n            string_tokens.append((token_name, token_data))\n        elif token_type == TokenType.MULTILINE_STRING.value:\n            multiline_tokens.append((token_name, token_data))\n        else:\n            other_tokens.append((token_name, token_data))\n    \n    # Sort each group by token name length (longest first) to avoid substring conflicts\n    comment_tokens.sort(key=lambda x: len(x[0]), reverse=True)\n    string_tokens.sort(key=lambda x: len(x[0]), reverse=True)\n    multiline_tokens.sort(key=lambda x: len(x[0]), reverse=True)\n    other_tokens.sort(key=lambda x: len(x[0]), reverse=True)\n    \n    # Process in REVERSE order of tokenization:\n    # 3. Comments (last tokenized, first detokenized)\n    # 2. String literals \n    # 1. Multi-line strings (first tokenized, last detokenized)\n    \n    # Step 1: Process comment tokens first\n    for token_name, token_data in comment_tokens:\n        result = _process_single_token(result, token_name, token_data)\n    \n    # Step 2: Process string literal tokens\n    for token_name, token_data in string_tokens:\n        result = _process_single_token(result, token_name, token_data)\n    \n    # Step 3: Process multiline string tokens last\n    for token_name, token_data in multiline_tokens:\n        result = _process_single_token(result, token_name, token_data)\n    \n    # Step 4: Process any remaining tokens\n    for token_name, token_data in other_tokens:\n        result = _process_single_token(result, token_name, token_data)\n    \n    return result\n\ndef _process_single_token(result: str, token_name: str, token_data: Dict) -> str:\n    \"\"\"Process a single token replacement with all its occurrences.\"\"\"\n    content = token_data['content']\n    metadata = token_data['metadata']\n    token_type = metadata.get('type')\n    \n    # Replace all occurrences of this token\n    while token_name in result:\n        # Handle string concatenation patterns first\n        if token_type == TokenType.STRING_LITERAL.value:\n            # Check for concatenation patterns - use the stored quote character\n            if metadata.get('in_parens', False):\n                quote_char = metadata.get('quote_char', '\"')\n                pattern = f'{quote_char}{token_name}{quote_char}'\n                if pattern in result:\n                    result = result.replace(pattern, content)\n                    continue\n            \n            # Fallback: check both quote types if quote_char not stored\n            quote_patterns = [f'\"{token_name}\"', f\"'{token_name}'\"]\n            found_pattern = False\n            \n            for pattern in quote_patterns:\n                if pattern in result:\n                    result = result.replace(pattern, content)\n                    found_pattern = True\n                    break\n            \n            if found_pattern:\n                continue\n            \n        start = result.find(token_name)\n        if start == -1:\n            break\n            \n        # Use the appropriate handler based on token type\n        if token_type == TokenType.COMPOUND_COMMENT.value:\n            result = _handle_compound_comment(result, token_name, content, start)\n        elif token_type == TokenType.IMPORT_COMMENT.value:\n            result = _handle_import_comment(result, token_name, content, start, metadata)\n        elif token_type == TokenType.INLINE_COMMENT.value:\n            result = _handle_inline_comment(result, token_name, content, start)\n        elif token_type == TokenType.STANDALONE_COMMENT.value:\n            result = _handle_standalone_comment(result, token_name, content, start)\n        elif token_type == TokenType.MULTILINE_STRING.value:\n            result = _handle_multiline_string(result, token_name, content, start)\n        elif token_type == TokenType.STRING_LITERAL.value:\n            result = _handle_string_literal(result, token_name, content, start)\n        else:\n            result = _handle_default_token(result, token_name, content, start)\n    \n    return result\n\ndef _handle_compound_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle compound statement comments (def, class, if, etc.) with smart reunification\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    line_stripped = line.strip()\n    \n    # Check if this token is alone on its line (moved during tokenization)\n    if line_stripped == token_name:\n        # Look for the compound statement on the previous line\n        search_start = line_start - 1\n        while search_start > 0:\n            prev_line_start = result.rfind('\\n', 0, search_start) + 1\n            prev_line = result[prev_line_start:search_start]\n            if prev_line.strip():\n                # Found the compound statement - reunite them\n                if prev_line.strip().endswith(':'):\n                    reunited = prev_line.rstrip() + content\n                    return result[:prev_line_start] + reunited + result[line_end:]\n                break\n            search_start = prev_line_start - 1\n    \n    # Fallback: just replace the token\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_import_comment(result: str, token_name: str, content: str, start: int, metadata: dict) -> str:\n    \"\"\"Handle import statement comments with metadata-driven reunification\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    line_stripped = line.strip()\n    \n    # Check if this token is alone on its line (moved during tokenization)\n    if line_stripped == token_name:\n        # Look for the import statement on the previous line\n        search_start = line_start - 1\n        while search_start > 0:\n            prev_line_start = result.rfind('\\n', 0, search_start) + 1\n            prev_line = result[prev_line_start:search_start]\n            import_statement = metadata.get('import_statement', '').strip()\n            \n            if prev_line.strip() == import_statement:\n                # Found the import statement - reunite them\n                reunited = prev_line.rstrip() + content\n                return result[:prev_line_start] + reunited + result[line_end:]\n            search_start = prev_line_start - 1\n    \n    # Fallback: just replace the token\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_inline_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle inline comments (non-import, non-compound) with smart reunification\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    \n    # Check if this is a semicolon-separated inline comment that needs reunification\n    if '; ' + token_name in line:\n        # Remove the semicolon and space, then add the comment content\n        new_line = line.replace('; ' + token_name, content)\n        return result[:line_start] + new_line + result[line_end:]\n    \n    # Check if this is a direct replacement (no semicolon case)\n    # This handles cases where the comment was after comma, colon, etc.\n    if token_name in line:\n        # Find the position of the token in the line\n        token_pos_in_line = line.find(token_name)\n        \n        # Check what comes before the token\n        before_token = line[:token_pos_in_line].rstrip()\n        \n        # Characters that indicate we should do direct replacement (no semicolon was added)\n        no_semicolon_endings = [',', ':', '(', '[', '{', '\\\\']\n        \n        if any(before_token.endswith(char) for char in no_semicolon_endings):\n            # Direct replacement - the comment was processed without semicolon\n            new_line = line.replace(token_name, content)\n            return result[:line_start] + new_line + result[line_end:]\n    \n    # Check if token is on its own line (moved during compound comment processing)\n    line_stripped = line.strip()\n    if line_stripped == token_name:\n        search_start = line_start - 1\n        while search_start > 0:\n            prev_line_start = result.rfind('\\n', 0, search_start) + 1\n            prev_line = result[prev_line_start:search_start]\n            if prev_line.strip():\n                reunited = prev_line.rstrip() + content\n                return result[:prev_line_start] + reunited + result[line_end:]\n            search_start = prev_line_start - 1\n    \n    # Default replacement (fallback case)\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_standalone_comment(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle standalone comment lines\"\"\"\n    line_start = result.rfind('\\n', 0, start) + 1\n    line_end = result.find('\\n', start)\n    if line_end == -1:\n        line_end = len(result)\n    line = result[line_start:line_end]\n    indent = line[:len(line) - len(line.lstrip())]\n    replacement = indent + content\n    return result[:line_start] + replacement + result[line_end:]\n\ndef _handle_multiline_string(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle multiline strings with proper indentation\"\"\"\n    if '\\n' in content:\n        indent = _get_indentation_at_position(result, start)\n        indented = _indent_multiline_content(content, indent)\n        return result[:start] + indented + result[start + len(token_name):]\n    else:\n        return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_string_literal(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle simple string literals\"\"\"\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _handle_default_token(result: str, token_name: str, content: str, start: int) -> str:\n    \"\"\"Handle tokens without specific type metadata\"\"\"\n    return result[:start] + content + result[start + len(token_name):]\n\ndef _restore_top_level_comments(final_content: str, top_comments: List[str]) -> str:\n    \"\"\"\n    Restore top-level comments to the beginning of the file.\n    \n    Args:\n        final_content: The processed file content\n        top_comments: List of original top-level comment lines\n    \n    Returns:\n        Content with top-level comments restored\n    \"\"\"\n    if not top_comments:\n        return final_content\n    \n    # Add comments at the start, followed by a blank line if there's content\n    comment_section = '\\n'.join(top_comments)\n    \n    if final_content.strip():\n        return comment_section + '\\n\\n' + final_content\n    else:\n        return comment_section + '\\n'\n",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py",
    "code_hash": "aa6c7ccadb1e7175aabc885cdb9f946e",
    "globals": {
     "ast": "<module 'ast' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\ast.py'>",
     "os": "<module 'os' (frozen)>",
     "textwrap": "<module 'textwrap' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\textwrap.py'>",
     "_process_error": "<function _process_error at 0x000001D7A92B0EA0>",
     "_clean": "<function _clean at 0x000001D7A92B0FE0>",
     "_py_ast_to_source": "<function _py_ast_to_source at 0x000001D7A92B1080>",
     "hashlib": "<module 'hashlib' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\hashlib.py'>",
     "re": "<module 're' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\re\\\\__init__.py'>",
     "Dict": "typing.Dict",
     "Tuple": "typing.Tuple",
     "List": "typing.List",
     "Enum": "<enum 'Enum'>",
     "TokenType": "<enum 'TokenType'>",
     "ScopeTransformer": "<class 'dynamic_module_2feb8c52b8785b27616df3d91d664fd4.ScopeTransformer'>",
     "handle_errors": "<function handle_errors at 0x000001D7A9D04720>",
     "python_edit": "<function python_edit at 0x000001D7A9E87740>",
     "_make_file": "<function _make_file at 0x000001D7A9E877E0>",
     "_create_token": "<function _create_token at 0x000001D7A9E87880>",
     "_get_file_hash": "<function _get_file_hash at 0x000001D7A9E87920>",
     "_contains_token": "<function _contains_token at 0x000001D7A9E879C0>",
     "_find_string_end": "<function _find_string_end at 0x000001D7A9E87F60>",
     "_is_fstring_start": "<function _is_fstring_start at 0x000001D7A9E87B00>",
     "_find_fstring_end": "<function _find_fstring_end at 0x000001D7A9E87BA0>",
     "_determine_comment_type": "<function _determine_comment_type at 0x000001D7A9E87C40>",
     "_get_indentation_at_position": "<function _get_indentation_at_position at 0x000001D7A9E87CE0>",
     "_indent_multiline_content": "<function _indent_multiline_content at 0x000001D7A9E87D80>",
     "_preserve_blank_lines": "<function _preserve_blank_lines at 0x000001D7A9E87E20>",
     "_analyze_string_at_position_in_source": "<function _analyze_string_at_position_in_source at 0x000001D7A9E87A60>",
     "_find_all_string_locations_in_source": "<function _find_all_string_locations_in_source at 0x000001D7AB538400>",
     "_is_quote_in_comment": "<function _is_quote_in_comment at 0x000001D7AB538040>",
     "_tokenize_source": "<function _tokenize_source at 0x000001D7AB5380E0>",
     "_extract_top_level_comments": "<function _extract_top_level_comments at 0x000001D7AB538180>",
     "_process_line_comments": "<function _process_line_comments at 0x000001D7AB538220>",
     "_process_multiline_strings": "<function _process_multiline_strings at 0x000001D7AB5382C0>",
     "_process_all_string_literals": "<function _process_all_string_literals at 0x000001D7AB538360>",
     "_process_string_literals": "<function _process_string_literals at 0x000001D7A9E87EC0>",
     "_find_all_string_locations": "<function _find_all_string_locations at 0x000001D7AB5384A0>",
     "_analyze_string_at_position": "<function _analyze_string_at_position at 0x000001D7AB538540>",
     "_process_inline_comment": "<function _process_inline_comment at 0x000001D7AB5385E0>",
     "_detokenize_source": "<function _detokenize_source at 0x000001D7AB538680>",
     "_process_single_token": "<function _process_single_token at 0x000001D7AB538720>",
     "_handle_compound_comment": "<function _handle_compound_comment at 0x000001D7AB5387C0>",
     "_handle_import_comment": "<function _handle_import_comment at 0x000001D7AB538860>",
     "_handle_inline_comment": "<function _handle_inline_comment at 0x000001D7AB538900>",
     "_handle_standalone_comment": "<function _handle_standalone_comment at 0x000001D7AB5389A0>",
     "_handle_multiline_string": "<function _handle_multiline_string at 0x000001D7AB538A40>",
     "_handle_string_literal": "<function _handle_string_literal at 0x000001D7AB538AE0>",
     "_handle_default_token": "<function _handle_default_token at 0x000001D7AB538B80>",
     "_restore_top_level_comments": "<function _restore_top_level_comments at 0x000001D7AB538C20>"
    }
   },
   "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py": {
    "name": "dynamic_module_9ed125295933273528d22ebd82cdc6ad",
    "source": "import os\nimport traceback\nimport textwrap\nimport difflib\nfrom bots.dev.decorators import handle_errors\n\n@handle_errors\ndef view(file_path: str, start_line: str = None, end_line: str = None,\n         around_str_match: str = None, dist_from_match: str = '10'):\n    \"\"\"\n    Display the contents of a file with line numbers.\n    Parameters:\n    - file_path (str): The path to the file to be viewed.\n    - start_line (int, optional): Starting line number (1-indexed). If None, starts from beginning.\n    - end_line (int, optional): Ending line number (1-indexed). If None, goes to end of file.\n    - around_str_match (str, optional): String to search for. If provided, shows lines around matches.\n    - dist_from_match (int, optional): Number of lines to show before and after each match. Defaults to 10.\n    Returns:\n    A string containing the file contents with line numbers,\n    filtered according to the specified parameters.\n    \"\"\"\n    encodings = ['utf-8', 'utf-16', 'utf-16le', 'ascii', 'cp1252', 'iso-8859-1']\n    start_line = int(start_line) if start_line is not None else None\n    end_line = int(end_line) if end_line is not None else None\n    dist_from_match = int(dist_from_match) if dist_from_match is not None else None\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as file:\n                lines = file.readlines()\n            # If searching for a string match\n            if around_str_match:\n                matching_line_numbers = []\n                for i, line in enumerate(lines):\n                    if around_str_match in line:\n                        matching_line_numbers.append(i)\n                if not matching_line_numbers:\n                    return f\"No matches found for '{around_str_match}'\"\n                # Collect line ranges around matches\n                line_ranges = set()\n                for match_line in matching_line_numbers:\n                    start = max(0, match_line - dist_from_match)\n                    end = min(len(lines), match_line + dist_from_match + 1)\n                    for line_num in range(start, end):\n                        line_ranges.add(line_num)\n                # Sort and create output\n                sorted_lines = sorted(line_ranges)\n                numbered_lines = []\n                prev_line = None  # To detect gaps\n                for line_num in sorted_lines:\n                    if prev_line is not None and line_num > prev_line + 1:\n                        numbered_lines.append(\"...\")  # Add separator for gaps\n                    numbered_lines.append(f'{line_num + 1}:{lines[line_num].rstrip()}')\n                    prev_line = line_num\n                return '\\n'.join(numbered_lines)\n            # If using start_line and end_line parameters\n            else:\n                start_idx = (start_line - 1) if start_line else 0\n                end_idx = end_line if end_line else len(lines)\n                # Validate line numbers\n                start_idx = max(0, start_idx)\n                end_idx = min(len(lines), end_idx)\n                if start_idx >= len(lines):\n                    return f\"Error: start_line ({start_line}) exceeds file length ({len(lines)} lines)\"\n                selected_lines = lines[start_idx:end_idx]\n                numbered_lines = [f'{i + start_idx + 1}:{line.rstrip()}' for i, line in enumerate(selected_lines)]\n                return '\\n'.join(numbered_lines)\n        except UnicodeDecodeError:\n            continue\n    return f\"Error: Unable to read file with any of the attempted encodings: {', '.join(encodings)}\"\n\n@handle_errors\ndef view_dir(start_path: str='.', output_file=None, target_extensions: str=\"['py', 'txt', 'md']\", max_lines: int=500):\n    \"\"\"\n    Creates a summary of the directory structure starting from the given path, writing only files\n    with specified extensions and showing venv directories without their contents.\n    Parameters:\n    - start_path (str): The root directory to start scanning from.\n    - output_file (str): The name of the file to optionally write the directory structure to.\n    - target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n    - max_lines (int): Maximum number of lines in output. If exceeded, deepest levels are truncated. Default 500.\n    Returns:\n    str: A formatted string containing the directory structure, with each directory and file properly indented.\n    Example output:\n    my_project/\n        venv/\n        module1/\n            script.py\n            README.md\n        module2/\n            utils.py\n    cost: low\n    \"\"\"\n    if max_lines:\n        max_lines = int(max_lines)\n    extensions_list = [ext.strip().strip('\\'\"') for ext in target_extensions.strip('[]').split(',')]\n    extensions_list = ['.' + ext if not ext.startswith('.') else ext for ext in extensions_list]\n    # First pass: collect all directory entries with their levels\n    dir_entries = []\n    max_level = 0\n    for root, dirs, files in os.walk(start_path):\n        level = root.replace(start_path, '').count(os.sep)\n        indent = '    ' * level\n        basename = os.path.basename(root)\n        is_venv = basename in ['venv', 'env', '.env'] or 'pyvenv.cfg' in files\n        if is_venv:\n            dir_entries.append((level, f'{indent}{basename}/'))\n            max_level = max(max_level, level)\n            dirs[:] = []\n            continue\n        has_relevant_files = False\n        for _, _, fs in os.walk(root):\n            if any((f.endswith(tuple(extensions_list)) for f in fs)):\n                has_relevant_files = True\n                break\n        if has_relevant_files:\n            dir_entries.append((level, f'{indent}{basename}/'))\n            max_level = max(max_level, level)\n            subindent = '    ' * (level + 1)\n            for file in files:\n                if file.endswith(tuple(extensions_list)):\n                    dir_entries.append((level + 1, f'{subindent}{file}'))\n                    max_level = max(max_level, level + 1)\n    # Second pass: apply length limiting by progressively removing deeper levels\n    current_level_limit = max_level\n    while current_level_limit >= 0:\n        # Filter entries up to current level limit\n        filtered_entries = [(level, line) for level, line in dir_entries if level <= current_level_limit]\n        # Check if we need to add truncation indicators\n        truncated_dirs = set()\n        for level, line in dir_entries:\n            if level > current_level_limit and level == current_level_limit + 1:\n                # Find parent directory\n                parent_level = level - 1\n                for parent_level_check, parent_line in filtered_entries:\n                    if parent_level_check == parent_level and parent_line.endswith('/'):\n                        truncated_dirs.add(parent_line)\n        # Add truncation indicators\n        final_entries = []\n        for level, line in filtered_entries:\n            final_entries.append(line)\n            if line in truncated_dirs:\n                truncation_indent = '    ' * (level + 1)\n                final_entries.append(f'{truncation_indent}...')\n        if len(final_entries) <= max_lines:\n            output_text = final_entries\n            break\n        current_level_limit -= 1\n    else:\n        output_text = [f\"Project too large (>{max_lines} lines). Showing root level only:\", f\"{os.path.basename(start_path) or start_path}/\", \"    ...\"]\n    if output_file is not None:\n        with open(output_file, 'w') as file:\n            file.write('\\n'.join(output_text))\n    return '\\n'.join(output_text)\n\n\n@handle_errors\ndef patch_edit(file_path: str, patch_content: str):\n    \"\"\"\n    Apply a git-style unified diff patch to a file.\n    Use when you need to make a precise change to a file\n    Tips:\n    - Small, focused changes work best\n    - Exact content matching is important\n    - Including surrounding context helps matching\n    - Whitespace in the target lines matters\n    Parameters:\n    - file_path (str): Path to the file to modify\n    - patch_content (str): Unified diff format patch content\n    Returns:\n    str: Description of changes made or error message\n    cost: low\n    \"\"\"\n    file_path = _normalize_path(file_path)\n    encodings = ['utf-8', 'utf-16', 'utf-16le', 'ascii', 'cp1252', 'iso-8859-1']\n    content = None\n    used_encoding = 'utf-8'\n    \n    # Create directory if needed\n    dir_path = os.path.dirname(file_path)\n    if dir_path:\n        os.makedirs(dir_path, exist_ok=True)\n    \n    # Read existing file or start with empty content\n    if not os.path.exists(file_path):\n        content = ''\n    else:\n        for encoding in encodings:\n            try:\n                with open(file_path, 'r', encoding=encoding) as file:\n                    content = file.read()\n                    used_encoding = encoding\n                    break\n            except UnicodeDecodeError:\n                continue\n    \n    if content is None and os.path.exists(file_path):\n        return f\"Error: Unable to read existing file with any of the attempted encodings: {', '.join(encodings)}\"\n    \n    if not patch_content.strip():\n        return 'Error: patch_content is empty.'\n    \n    original_lines = content.splitlines() if content else []\n    current_lines = original_lines.copy()\n    changes_made = []\n    line_offset = 0\n    \n    # Clean up patch content\n    patch_content = textwrap.dedent(patch_content)\n    patch_content = '\\n' + patch_content\n    hunks = patch_content.split('\\n@@')[1:]\n    \n    if not hunks:\n        return 'Error: No valid patch hunks found. (No instances of \"\\\\n@@\". Did you accidentally indent the headers?)'\n    \n    for hunk in hunks:\n        hunk = hunk.strip()\n        if not hunk:\n            continue\n        \n        try:\n            header_end = hunk.index('\\n')\n            header = hunk[:header_end].strip()\n            if not header.endswith('@@'):\n                header = header + ' @@'\n            \n            old_range, new_range = header.rstrip(' @').split(' +')\n            old_start = int(old_range.split(',')[0].lstrip('- ')) - 1  # Convert to 0-based\n            \n            hunk_lines = _normalize_header_lines(hunk[header_end:].splitlines()[1:])\n        except (ValueError, IndexError) as e:\n            return f'Error parsing hunk header: {str(e)}\\nHeader: {header}'\n        \n        # Parse hunk into components\n        context_before = []\n        context_after = []\n        removals = []\n        additions = []\n        \n        for line in hunk_lines:\n            if not line:\n                continue\n            if not (line.startswith('+') or line.startswith('-')):\n                if not removals and not additions:\n                    context_before.append(line)\n                else:\n                    context_after.append(line)\n            elif line.startswith('-'):\n                removals.append(line[1:])\n            elif line.startswith('+'):\n                additions.append(line[1:])\n        \n        if not removals and not additions:\n            return f'Error: No additions or removals found in hunk starting with {hunk_lines[0][:20] if hunk_lines else \"empty hunk\"}'\n        \n        # Handle new file creation - must come before hierarchy check\n        # Debug: Let's be very explicit about the conditions\n        file_is_empty = len(current_lines) == 0\n        patch_starts_at_zero = old_start == 0  \n        no_context = len(context_before) == 0\n        no_removals = len(removals) == 0\n        \n        if file_is_empty and patch_starts_at_zero and no_context and no_removals:\n            current_lines.extend(additions)\n            changes_made.append('Applied changes to new file')\n            continue\n        \n        # Apply new matching hierarchy\n        match_result = _find_match_with_hierarchy(\n            current_lines, old_start + line_offset, context_before, removals, additions\n        )\n        \n        if not match_result['found']:\n            return match_result['error']\n        \n        # Apply the changes\n        match_line = match_result['line']\n        indented_additions = match_result['additions']\n        pos = match_line + len(context_before)\n        \n        if removals:\n            current_lines[pos:pos + len(removals)] = indented_additions\n        else:\n            current_lines[pos:pos] = indented_additions\n        \n        line_offset += len(additions) - len(removals)\n        changes_made.append(match_result['message'])\n    \n    if changes_made:\n        new_content = '\\n'.join(current_lines)\n        if not new_content.endswith('\\n'):\n            new_content += '\\n'\n        \n        with open(file_path, 'w', encoding=used_encoding) as file:\n            file.write(new_content)\n        \n        return 'Successfully applied patches:\\n' + '\\n'.join(changes_made)\n    \n    return 'No changes were applied'\n\n\ndef _find_match_with_hierarchy(current_lines, expected_line, context_before, removals, additions):\n    \"\"\"\n    Implement the new 5-step matching hierarchy.\n    Returns dict with keys: found, line, additions, message, error\n    \"\"\"\n    \n    # Special case: if no context and no removals, this might be a pure insertion\n    # that should go at the expected line (common for new file scenarios that slip through)\n    if not context_before and not removals:\n        if expected_line <= len(current_lines):\n            return {\n                'found': True,\n                'line': expected_line,\n                'additions': additions,  \n                'message': f'Applied pure insertion at line {expected_line + 1}',\n                'error': None\n            }\n    \n    # Step 1: Check exact match at line numbers (line numbers + context + indentation)\n    if _check_exact_match_at_position(current_lines, expected_line, context_before, removals):\n        return {\n            'found': True,\n            'line': expected_line,\n            'additions': additions,\n            'message': f'Applied hunk with exact match at line {expected_line + 1}',\n            'error': None\n        }\n    \n    # Step 2: Check exact match ignoring line numbers (context + indentation match anywhere)\n    exact_match_line = _find_exact_match_anywhere(current_lines, context_before, removals)\n    if exact_match_line is not None:\n        return {\n            'found': True,\n            'line': exact_match_line,\n            'additions': additions,\n            'message': f'Applied hunk with exact match at line {exact_match_line + 1} (different from specified line {expected_line + 1})',\n            'error': None\n        }\n    \n    # Step 3: Check match at line numbers ignoring whitespace\n    if _check_whitespace_match_at_position(current_lines, expected_line, context_before, removals):\n        if not context_before:\n            return {\n                'found': False,\n                'line': None,\n                'additions': None,\n                'message': None,\n                'error': 'Error: Need context lines to determine correct indentation when whitespace differs'\n            }\n        \n        adjusted_additions = _adjust_additions_to_context(\n            current_lines, expected_line, context_before, additions\n        )\n        return {\n            'found': True,\n            'line': expected_line,\n            'additions': adjusted_additions,\n            'message': f'Applied hunk at line {expected_line + 1} with indentation adjustment',\n            'error': None\n        }\n    \n    # Step 4: Check match anywhere ignoring whitespace\n    whitespace_match_line = _find_whitespace_match_anywhere(current_lines, context_before, removals)\n    if whitespace_match_line is not None:\n        if not context_before:\n            return {\n                'found': False,\n                'line': None,\n                'additions': None,\n                'message': None,\n                'error': 'Error: Need context lines to determine correct indentation when whitespace differs'\n            }\n        \n        adjusted_additions = _adjust_additions_to_context(\n            current_lines, whitespace_match_line, context_before, additions\n        )\n        return {\n            'found': True,\n            'line': whitespace_match_line,\n            'additions': adjusted_additions,\n            'message': f'Applied hunk at line {whitespace_match_line + 1} (different from specified line {expected_line + 1}) with indentation adjustment',\n            'error': None\n        }\n    \n    # Step 5: Fuzzy matching - find best partial match\n    if context_before:\n        _, best_line, match_quality, _ = _find_block_in_content(current_lines, context_before, ignore_whitespace=True)\n        if match_quality > 0.05:\n            context = _get_context(current_lines, best_line - 1, 2)\n            return {\n                'found': False,\n                'line': None,\n                'additions': None,\n                'message': None,\n                'error': f'Error: Could not find match. Best potential match at lines {best_line} to {best_line + len(context_before) - 1}\\nContext:\\n' + '\\n'.join(context) + f'\\nMatch quality: {match_quality:.2f}'\n            }\n    \n    return {\n        'found': False,\n        'line': None,\n        'additions': None,\n        'message': None,\n        'error': 'Error: Could not find any suitable match for the patch context'\n    }\n\ndef _check_exact_match_at_position(current_lines, line_pos, context_before, removals):\n    \"\"\"Check if there's an exact match at the specified position.\"\"\"\n    if line_pos < 0 or line_pos + len(context_before) > len(current_lines):\n        return False\n    \n    # Check context lines\n    for i, ctx_line in enumerate(context_before):\n        if current_lines[line_pos + i] != ctx_line:\n            return False\n    \n    # Check removal lines if present\n    if removals:\n        removal_pos = line_pos + len(context_before)\n        if removal_pos + len(removals) > len(current_lines):\n            return False\n        for i, rem_line in enumerate(removals):\n            if current_lines[removal_pos + i] != rem_line:\n                return False\n    \n    return True\n\n\ndef _find_exact_match_anywhere(current_lines, context_before, removals):\n    \"\"\"Find exact match anywhere in the file, ignoring line numbers.\"\"\"\n    if not context_before and not removals:\n        return None\n    \n    search_lines = context_before + removals\n    \n    for i in range(len(current_lines) - len(search_lines) + 1):\n        match = True\n        for j, search_line in enumerate(search_lines):\n            if current_lines[i + j] != search_line:\n                match = False\n                break\n        if match:\n            return i\n    \n    return None\n\n\ndef _check_whitespace_match_at_position(current_lines, line_pos, context_before, removals):\n    \"\"\"Check if there's a whitespace-ignoring match at the specified position.\"\"\"\n    if line_pos < 0 or line_pos + len(context_before) > len(current_lines):\n        return False\n    \n    # Check context lines ignoring whitespace\n    for i, ctx_line in enumerate(context_before):\n        if current_lines[line_pos + i].strip() != ctx_line.strip():\n            return False\n    \n    # Check removal lines if present\n    if removals:\n        removal_pos = line_pos + len(context_before)\n        if removal_pos + len(removals) > len(current_lines):\n            return False\n        for i, rem_line in enumerate(removals):\n            if current_lines[removal_pos + i].strip() != rem_line.strip():\n                return False\n    \n    return True\n\n\ndef _find_whitespace_match_anywhere(current_lines, context_before, removals):\n    \"\"\"Find whitespace-ignoring match anywhere in the file.\"\"\"\n    if not context_before and not removals:\n        return None\n    \n    search_lines = context_before + removals\n    \n    for i in range(len(current_lines) - len(search_lines) + 1):\n        match = True\n        for j, search_line in enumerate(search_lines):\n            if current_lines[i + j].strip() != search_line.strip():\n                match = False\n                break\n        if match:\n            return i\n    \n    return None\n\ndef _adjust_additions_to_context(current_lines, match_line, context_before, additions):\n    \"\"\"\n    Adjust additions indentation based on context line indentation difference.\n    \n    The logic:\n    1. Find first non-empty context line in patch\n    2. Compare its indentation to corresponding line in actual file\n    3. Apply that indentation difference to all addition lines\n    \"\"\"\n    if not context_before:\n        return None  # Error should be handled by caller\n    \n    if not additions:\n        return additions\n    \n    # Find first non-empty context line\n    patch_context_line = None\n    context_index = None\n    for i, ctx_line in enumerate(context_before):\n        if ctx_line.strip():  # Skip empty lines\n            patch_context_line = ctx_line\n            context_index = i\n            break\n    \n    if patch_context_line is None:\n        # All context lines are empty, can't determine baseline\n        return additions\n    \n    # Get indentation of context line in patch vs actual file\n    patch_indent = _get_line_indentation(patch_context_line)\n    actual_file_line = current_lines[match_line + context_index]\n    actual_indent = _get_line_indentation(actual_file_line)\n    \n    # Calculate the indentation difference\n    indent_diff_spaces = len(actual_indent) - len(patch_indent)\n    \n    # Apply the difference to all addition lines\n    adjusted_additions = []\n    for addition in additions:\n        current_addition_indent = _get_line_indentation(addition)\n        new_indent_spaces = len(current_addition_indent) + indent_diff_spaces\n        \n        # Ensure we don't go negative\n        if new_indent_spaces < 0:\n            new_indent_spaces = 0\n            \n        new_indent = ' ' * new_indent_spaces\n        line_content = addition.lstrip()\n        adjusted_line = new_indent + line_content\n        adjusted_additions.append(adjusted_line)\n    \n    return adjusted_additions\n\ndef _check_exact_match_at_position(current_lines, line_pos, context_before, removals):\n    \"\"\"Check if there's an exact match at the specified position.\"\"\"\n    if line_pos < 0 or line_pos + len(context_before) > len(current_lines):\n        return False\n    \n    # Check context lines\n    for i, ctx_line in enumerate(context_before):\n        if current_lines[line_pos + i] != ctx_line:\n            return False\n    \n    # Check removal lines if present\n    if removals:\n        removal_pos = line_pos + len(context_before)\n        if removal_pos + len(removals) > len(current_lines):\n            return False\n        for i, rem_line in enumerate(removals):\n            if current_lines[removal_pos + i] != rem_line:\n                return False\n    \n    return True\n\n\ndef _find_exact_match_anywhere(current_lines, context_before, removals):\n    \"\"\"Find exact match anywhere in the file, ignoring line numbers.\"\"\"\n    if not context_before and not removals:\n        return None\n    \n    search_lines = context_before + removals\n    \n    for i in range(len(current_lines) - len(search_lines) + 1):\n        match = True\n        for j, search_line in enumerate(search_lines):\n            if current_lines[i + j] != search_line:\n                match = False\n                break\n        if match:\n            return i\n    \n    return None\n\n\ndef _check_whitespace_match_at_position(current_lines, line_pos, context_before, removals):\n    \"\"\"Check if there's a whitespace-ignoring match at the specified position.\"\"\"\n    if line_pos < 0 or line_pos + len(context_before) > len(current_lines):\n        return False\n    \n    # Check context lines ignoring whitespace\n    for i, ctx_line in enumerate(context_before):\n        if current_lines[line_pos + i].strip() != ctx_line.strip():\n            return False\n    \n    # Check removal lines if present\n    if removals:\n        removal_pos = line_pos + len(context_before)\n        if removal_pos + len(removals) > len(current_lines):\n            return False\n        for i, rem_line in enumerate(removals):\n            if current_lines[removal_pos + i].strip() != rem_line.strip():\n                return False\n    \n    return True\n\n\ndef _find_whitespace_match_anywhere(current_lines, context_before, removals):\n    \"\"\"Find whitespace-ignoring match anywhere in the file.\"\"\"\n    if not context_before and not removals:\n        return None\n    \n    search_lines = context_before + removals\n    \n    for i in range(len(current_lines) - len(search_lines) + 1):\n        match = True\n        for j, search_line in enumerate(search_lines):\n            if current_lines[i + j].strip() != search_line.strip():\n                match = False\n                break\n        if match:\n            return i\n    \n    return None\n\n\ndef _adjust_additions_to_context(current_lines, match_line, context_before, additions):\n    \"\"\"\n    Adjust additions indentation based on context line indentation difference.\n    \n    The logic:\n    1. Find first non-empty context line in patch\n    2. Compare its indentation to corresponding line in actual file\n    3. Apply that indentation difference to all addition lines\n    \"\"\"\n    if not context_before:\n        return None  # Error should be handled by caller\n    \n    if not additions:\n        return additions\n    \n    # Find first non-empty context line\n    patch_context_line = None\n    context_index = None\n    for i, ctx_line in enumerate(context_before):\n        if ctx_line.strip():  # Skip empty lines\n            patch_context_line = ctx_line\n            context_index = i\n            break\n    \n    if patch_context_line is None:\n        # All context lines are empty, can't determine baseline\n        return additions\n    \n    # Get indentation of context line in patch vs actual file\n    patch_indent_len = len(_get_line_indentation(patch_context_line))\n    actual_file_line = current_lines[match_line + context_index]\n    actual_indent_len = len(_get_line_indentation(actual_file_line))\n    \n    # Calculate the indentation difference\n    indent_diff_spaces = actual_indent_len - patch_indent_len\n    \n    # Apply the difference to all addition lines\n    adjusted_additions = []\n    for addition in additions:\n        if not addition.strip():  # Empty line\n            adjusted_additions.append(addition)\n            continue\n            \n        current_addition_indent_len = len(_get_line_indentation(addition))\n        new_indent_spaces = current_addition_indent_len + indent_diff_spaces\n        \n        # Ensure we don't go negative\n        if new_indent_spaces < 0:\n            new_indent_spaces = 0\n            \n        new_indent = ' ' * new_indent_spaces\n        line_content = addition.lstrip()\n        adjusted_line = new_indent + line_content\n        adjusted_additions.append(adjusted_line)\n    \n    return adjusted_additions\n\ndef _get_context(lines, center_idx, context_size):\n    \"\"\"Get context lines around an index with line numbers.\"\"\"\n    start = max(0, center_idx - context_size)\n    end = min(len(lines), center_idx + context_size + 1)\n    return [f'{i + 1}:{line}' for i, line in enumerate(lines[start:end], start)]\n\ndef _find_block_in_content(content_lines: list, block_lines: list, ignore_whitespace: bool=False) -> tuple[bool, int, float, bool]:\n    \"\"\"Helper function to find a block of lines anywhere in the content.\n    Returns (found, line_number, match_quality, was_whitespace_match)\"\"\"\n    if not block_lines:\n        return (False, 0, 0.0, False)\n    for i in range(len(content_lines) - len(block_lines) + 1):\n        current_block = content_lines[i:i + len(block_lines)]\n        if current_block == block_lines:\n            return (True, i + 1, 1.0, False)\n    if ignore_whitespace:\n        for i in range(len(content_lines) - len(block_lines) + 1):\n            current_block = [l.strip() for l in content_lines[i:i + len(block_lines)]]\n            block_to_match = [l.strip() for l in block_lines]\n            if current_block == block_lines:\n                return (True, i + 1, 0.9, True)\n    best_match = 0.0\n    best_line = 0\n    for i in range(len(content_lines) - len(block_lines) + 1):\n        current_block = content_lines[i:i + len(block_lines)]\n        matcher = difflib.SequenceMatcher(None, '\\n'.join(current_block), '\\n'.join(block_lines))\n        ratio = matcher.ratio()\n        if ratio > best_match:\n            best_match = ratio\n            best_line = i + 1\n    return (False, best_line, best_match, False)\ndef _get_line_indentation(line: str) -> str:\n    \"\"\"Extract the indentation from a line.\"\"\"\n    return line[:len(line) - len(line.lstrip())]\n\n\ndef _normalize_path(file_path: str) -> str:\n    \"\"\"\n    Normalize file path to use consistent separators and handle both / and \\\\.\n    Args:\n        file_path (str): The file path to normalize\n    Returns:\n        str: Normalized path using os.path.sep\n    \"\"\"\n    return os.path.normpath(file_path.replace('\\\\', '/').replace('//', '/'))\ndef _normalize_header_lines(lines):\n    \"\"\"\n    Normalize only the hunk headers (@@ lines) in a patch.\n    Leaves all other lines unchanged.\n    Args:\n        lines (list[str]): List of patch lines\n    Returns:\n        list[str]: Lines with normalized hunk headers\n    \"\"\"\n    normalized = []\n    for line in lines:\n        if not line:\n            normalized.append(line)\n            continue\n        if line.startswith('@@'):\n            parts = line.split('@@')\n            if len(parts) >= 2:\n                ranges = parts[1].strip()\n                line = f'@@ {ranges} @@'\n        normalized.append(line)\n    return normalized\ndef _adjust_indentation(lines: list, target_indent: str) -> list:\n    \"\"\"\n    Adjust indentation of a block of lines to match target indent while preserving relative indentation.\n    First applies target indentation to all lines, then preserves additional relative indentation.\n    Args:\n        lines (list[str]): Lines to adjust\n        target_indent (str): Target base indentation\n    Returns:\n        list[str]: Lines with adjusted indentation\n    \"\"\"\n    if not lines:\n        return lines\n    base_indent = None\n    for line in lines:\n        if line.strip():\n            base_indent = _get_line_indentation(line)\n            break\n    if base_indent is None:\n        return lines\n    adjusted_lines = []\n    for line in lines:\n        if not line.strip():\n            adjusted_lines.append('')\n            continue\n        current_indent = _get_line_indentation(line)\n        relative_indent = len(current_indent) - len(base_indent)\n        new_indent = target_indent + ' ' * max(0, relative_indent)\n        adjusted_lines.append(new_indent + line.lstrip())\n    return adjusted_lines\n",
    "file_path": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
    "code_hash": "ff3ffc81103dad9030b8b2cd1c06b4d4",
    "globals": {
     "os": "<module 'os' (frozen)>",
     "traceback": "<module 'traceback' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\traceback.py'>",
     "textwrap": "<module 'textwrap' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\textwrap.py'>",
     "difflib": "<module 'difflib' from 'C:\\\\Users\\\\benbu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312-arm64\\\\Lib\\\\difflib.py'>",
     "handle_errors": "<function handle_errors at 0x000001D7A9D04720>",
     "view": "<function view at 0x000001D7AB538F40>",
     "view_dir": "<function view_dir at 0x000001D7AB539080>",
     "patch_edit": "<function patch_edit at 0x000001D7AB5391C0>",
     "_find_match_with_hierarchy": "<function _find_match_with_hierarchy at 0x000001D7AB539260>",
     "_check_exact_match_at_position": "<function _check_exact_match_at_position at 0x000001D7AB539620>",
     "_find_exact_match_anywhere": "<function _find_exact_match_anywhere at 0x000001D7AB539300>",
     "_check_whitespace_match_at_position": "<function _check_whitespace_match_at_position at 0x000001D7AB5393A0>",
     "_find_whitespace_match_anywhere": "<function _find_whitespace_match_anywhere at 0x000001D7AB539440>",
     "_adjust_additions_to_context": "<function _adjust_additions_to_context at 0x000001D7AB5394E0>",
     "_get_context": "<function _get_context at 0x000001D7AB539580>",
     "_find_block_in_content": "<function _find_block_in_content at 0x000001D7AB5396C0>",
     "_get_line_indentation": "<function _get_line_indentation at 0x000001D7AB539760>",
     "_normalize_path": "<function _normalize_path at 0x000001D7AB539800>",
     "_normalize_header_lines": "<function _normalize_header_lines at 0x000001D7AB5398A0>",
     "_adjust_indentation": "<function _adjust_indentation at 0x000001D7AB539940>"
    }
   }
  },
  "function_paths": {
   "execute_powershell": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\terminal_tools.py",
   "python_edit": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\python_edit.py",
   "view": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
   "view_dir": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py",
   "patch_edit": "C:\\Users\\benbu\\Code\\llm-utilities-git\\bots\\bots\\tools\\code_tools.py"
  }
 },
 "autosave": true,
 "allow_web_search": true,
 "bot_class": "AnthropicBot"
}