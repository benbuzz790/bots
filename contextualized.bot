{
 "name": "Claude",
 "model_engine": "claude-sonnet-4-5-20250929",
 "max_tokens": 16000,
 "temperature": 0.3,
 "role": "assistant",
 "role_description": "a friendly AI assistant",
 "conversation": {
  "content": "",
  "pending_results": [],
  "role": "empty",
  "tool_calls": [],
  "tool_results": [],
  "node_class": "AnthropicNode",
  "replies": [
   {
    "content": "Hi Claude. Would you please read ROADMAP.md, CLAUDE.md, README.md, and get an overview of functional_prompts.py, base.py, and cli.py? (use python_view with 150 or so line limit.)",
    "pending_results": [],
    "role": "user",
    "tool_calls": [],
    "tool_results": [],
    "node_class": "AnthropicNode",
    "replies": [
     {
      "content": "Hello! I'm here to help you with your code! Let me read through those files to get an overview of the project.",
      "pending_results": [],
      "role": "assistant",
      "tool_calls": [
       {
        "type": "tool_use",
        "id": "toolu_01Rf97quiDsSxDA3SS1TyZnX",
        "name": "view",
        "input": {
         "file_path": "ROADMAP.md"
        }
       },
       {
        "type": "tool_use",
        "id": "toolu_01CBUxN4h1KQfuDcAdXjPUzb",
        "name": "view",
        "input": {
         "file_path": "CLAUDE.md"
        }
       },
       {
        "type": "tool_use",
        "id": "toolu_01PzbZifJT6PxqvDfBaoQ3jL",
        "name": "view",
        "input": {
         "file_path": "README.md"
        }
       },
       {
        "type": "tool_use",
        "id": "toolu_012XZ1wpCQhzsirfHxPHXtb7",
        "name": "python_view",
        "input": {
         "target_scope": "functional_prompts.py",
         "max_lines": "150"
        }
       },
       {
        "type": "tool_use",
        "id": "toolu_01GkKNmg3eEj2D93gi1ChpFm",
        "name": "python_view",
        "input": {
         "target_scope": "base.py",
         "max_lines": "150"
        }
       },
       {
        "type": "tool_use",
        "id": "toolu_01XdGxmn5KaZH9qc2j5XEXQV",
        "name": "python_view",
        "input": {
         "target_scope": "cli.py",
         "max_lines": "150"
        }
       }
      ],
      "tool_results": [],
      "node_class": "AnthropicNode",
      "replies": [
       {
        "content": "ok",
        "pending_results": [],
        "role": "user",
        "tool_calls": [],
        "tool_results": [
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01Rf97quiDsSxDA3SS1TyZnX",
          "content": "1:# Project Ideas and Future Development Notes\n2:**Date**: 2025-01-XX\n3:**Project**: bots - Agentic Programming Framework\n4:\n5:## Overview\n6:This document tracks development ideas, feature requests, and architectural improvements for the bots project. Each item includes context, rationale, and implementation considerations.\n7:\n8:\n9:---\n10:\n11:## Development Philosophy & Strategic Direction\n12:\n13:### The Core Principle: Standards Over Frameworks\n14:\n15:The bots project has reached a critical inflection point. After successfully implementing multi-provider support and achieving a robust 97%+ test pass rate, we're now positioned to make a fundamental choice about our future direction. The landscape of AI development tools has matured significantly, and a clear pattern has emerged: **universal standards are winning over proprietary frameworks**.\n16:\n17:This isn't about chasing trends\u2014\u009dit's about recognizing that the industry has spoken. Model Context Protocol (MCP) is projected to reach 90% organizational adoption by the end of 2025. Microsoft, OpenAI, and Google have all committed to it. OpenTelemetry has become the de facto standard for observability. These aren't just popular tools; they're becoming the *lingua franca* of AI development.\n18:\n19:### Why Standards Matter\n20:\n21:When we initially considered integrating with LangChain, the complexity was overwhelming. Layers of abstraction, steep learning curves, and constant API changes made it feel like we were building on shifting sand. But MCP? It's a protocol\u2014\u009dsimple, clear, and purpose-built for one thing: connecting AI models to tools and data. It's the USB-C of AI, and just like USB-C, its value comes from universal adoption, not feature richness.\n22:\n23:This realization shapes our entire strategy: **prioritize interoperability over features, standards over frameworks, simplicity over sophistication**.\n24:\n25:### The Three Pillars of Our Strategy\n26:\n27:#### 1. **Embrace Universal Standards**\n28:\n29:We're not building in isolation. The bots project should be a *citizen* of the broader AI ecosystem, not an island. This means:\n30:\n31:- **MCP Integration**: Our tools should be accessible to anyone using Claude Desktop, Cursor, or any MCP-compatible client. Conversely, our bots should be able to use the hundreds of MCP servers already published. This isn't just about compatibility\u2014\u009dit's about network effects. Every MCP server we can connect to multiplies our capabilities without writing a single line of integration code.\n32:\n33:- **OpenTelemetry for Observability**: Print statements scattered through code are a symptom of development without proper instrumentation. OpenTelemetry isn't just about removing print statements\u2014\u009dit's about building production-ready software that can be monitored, debugged, and optimized in real-world deployments. When something goes wrong at 3 AM, structured traces and metrics are the difference between hours of debugging and minutes.\n34:\n35:- **LiteLLM for Provider Expansion**: Rather than manually implementing each new LLM provider, we leverage LiteLLM's unified interface to support 100+ providers instantly. This isn't laziness\u2014\u009dit's recognizing that provider APIs are commoditizing, and our value lies elsewhere.\n36:\n37:#### 2. **Maintain Our Core Differentiators**\n38:\n39:Standards handle connectivity and interoperability, but they don't replace our unique capabilities:\n40:\n41:- **Conversation Trees**: The ability to branch, explore multiple paths, and navigate conversation history as a tree structure remains unique. Most frameworks treat conversations as linear chains. We treat them as explorable spaces.\n42:\n43:- **Functional Prompts**: Composable patterns like `chain`, `branch`, `prompt_while`, and `par_branch_while` provide structured reasoning that goes beyond simple prompt-response cycles. These aren't just convenience functions\u2014\u009dthey're a programming model for AI interactions.\n44:\n45:- **Tool Excellence**: Our tools like `python_edit` (scope-aware code editing) and `branch_self` (recursive self-branching) represent genuine innovation. By exposing them via MCP, we share this innovation with the broader community while maintaining our implementation advantages.\n46:\n47:- **Self-Context Management**: The upcoming expansion of self-tools (delete_stale_context, fork_from_node, etc.) gives bots unprecedented control over their own conversation history. This is agentic AI in the truest sense\u2014\u009dagents that can manage their own context.\n48:\n49:#### 3. **Build for Production, Not Just Prototypes**\n50:\n51:The shift from \"works on my machine\" to \"works in production\" requires fundamental changes:\n52:\n53:- **Observability First**: OpenTelemetry integration isn't optional\u2014\u009dit's foundational. Every API call, every tool execution, every decision point should be traceable. This enables debugging, optimization, cost tracking, and compliance.\n54:\n55:- **Quality Guardrails**: Branch protection, required CI/CD checks, and automated code review (CodeRabbit) aren't bureaucracy\u2014\u009dthey're insurance against regressions. The coveted green checkmark isn't a badge of honor; it's a minimum bar for production readiness.\n56:\n57:- **Test Organization**: Moving from \"we have tests\" to \"we have a test strategy\" means organizing tests by speed (unit/integration/e2e), using proper fixtures, and fixing parallelism issues. Fast feedback loops enable rapid iteration without sacrificing quality.\n58:\n59:- **Configuration Over Code**: The shift to configuration-driven architecture (base.py using config files, plugin systems, etc.) makes the system adaptable without code changes. This is essential for deployment in diverse environments.\n60:\n61:### The Practical Implications\n62:\n63:This philosophy translates into concrete priorities:\n64:\n65:**Phase 1 focuses on standards and foundations** because these are force multipliers. MCP integration doesn't just add features\u2014\u009dit adds an entire ecosystem. OpenTelemetry doesn't just remove print statements\u2014\u009dit makes the system observable and debuggable. Branch protection doesn't just prevent bad commits\u2014\u009dit ensures every change meets quality standards.\n66:\n67:**Phase 2 builds on these foundations** with features that leverage the standards. Once we have MCP, expanding providers via LiteLLM becomes trivial. Once we have OpenTelemetry, making the CLI prettier with real-time progress indicators becomes straightforward. Once we have proper configuration, adding plugin support becomes natural.\n68:\n69:**Phase 3 and 4 are about refinement and reach** rather than fundamental capabilities. The GUI, advanced self-tools, and cosmetic improvements are valuable, but they're not foundational. They're the house we build on the foundation, not the foundation itself.\n70:\n71:### Why This Matters Now\n72:\n73:The AI development landscape is consolidating around standards faster than anyone expected. MCP was introduced in November 2024 and is already approaching universal adoption. OpenTelemetry's GenAI semantic conventions were formalized in early 2025. The window for \"build everything ourselves\" has closed. The window for \"integrate with standards and differentiate on capabilities\" is wide open.\n74:\n75:We're not abandoning our vision\u2014\u009dwe're recognizing that our vision is best served by standing on the shoulders of standards rather than building everything from scratch. The bots project's value isn't in reimplementing tool connectivity or observability. It's in conversation trees, functional prompts, and agentic capabilities that no standard can provide.\n76:\n77:### The Path Forward\n78:\n79:This isn't a pivot\u2014 it's a maturation. We're moving from \"build an AI framework\" to \"build the best agentic AI system on top of industry standards.\" The difference is critical. One path leads to constant maintenance of infrastructure. The other leads to innovation on capabilities.\n80:\n81:Every item in this document, from MCP integration to GUI development, flows from this philosophy: **embrace standards for infrastructure, innovate on capabilities, and build for production**. This is how we ensure the bots project remains relevant, maintainable, and valuable as the AI landscape continues its rapid evolution.\n82:\n83:The future of AI development isn't about building walled gardens\u2014\u009dit's about building excellent tools that work seamlessly in a standardized ecosystem. That's the future we're building toward.\n84:\n85:---\n86:\n87:---\n88:\n89:## 1. Expand Provider Base\n90:\n91:**Current State**:\n92:- Supports Anthropic Claude, OpenAI GPT, and Google Gemini\n93:- v3.0.0 achieved multi-provider support with 97%+ test pass rate\n94:\n95:**Goal**: Add support for additional LLM providers\n96:\n97:**Potential Providers**:\n98:- Cohere\n99:- Mistral AI\n100:- Local models (Ollama, LM Studio)\n101:- Azure OpenAI (separate from standard OpenAI)\n102:- AWS Bedrock\n103:- Hugging Face Inference API\n104:\n105:**Priority**: Medium\n106:\n107:---\n108:\n109:## 2. Make CLI Prettier\n110:\n111:**Goal**: Enhance visual presentation and user experience\n112:\n113:**Potential Improvements**:\n114:- Color coding for different message types\n115:- Syntax highlighting for code in tool results\n116:- Progress indicators for long-running operations (see item 12)\n117:- Rich text formatting (using `rich` or `textual`)\n118:- Better error message formatting\n119:\n120:**Priority**: Medium\n121:\n122:---\n123:\n124:## 3. JavaScript GUI / Frontend Backend for CLI\n125:\n126:**Goal**: Create web-based GUI that interfaces with CLI backend\n127:\n128:**Architecture Options**:\n129:- Thin Client: FastAPI backend + React frontend + WebSocket\n130:- Electron App: Package CLI with Electron\n131:- Jupyter-style Notebook: Web-based notebook interface\n132:\n133:**Features**: Visual tree navigation, drag-and-drop, side-by-side comparison, visual FP builder\n134:\n135:**Related**: See large_refactor_tasks.md item 2 - \"GUI over the cli\"\n136:\n137:**Priority**: Low-Medium\n138:\n139:---\n140:\n141:## 4. Rename auto_stash to 'mustache' (must stash)\n142:\n143:**Goal**: Rename for better branding/memorability\n144:\n145:**Changes**: CLI command, config keys, documentation, help text\n146:\n147:**Priority**: Low\n148:\n149:---\n150:\n151:## 5. Ensure CLI Haiku Bots Match Provider\n152:\n153:**Goal**: Initialize utility bots from same provider as main CLI bot\n154:\n155:**Provider Model Tiers**:\n156:- Anthropic: Sonnet (flagship) / Haiku (fast)\n157:- OpenAI: GPT-4 Turbo (flagship) / GPT-4o-mini (fast)\n158:- Google: Gemini Pro (flagship) / Gemini Flash (fast)\n159:\n160:**Priority**: Medium\n161:\n162:---\n163:\n164:## 6. Configure CLI More Thoroughly (Plugins?)\n165:\n166:**Goal**: Comprehensive configuration system with plugin support\n167:\n168:**Categories**: Display settings, behavior settings, provider settings, plugin system\n169:\n170:**Related**: See large_refactor_tasks.md item 4 - \"Update base.py to use config file\"\n171:\n172:**Priority**: Medium-High\n173:\n174:---\n175:\n176:## 7. Review large_refactor_tasks.md\n177:\n178:**Status**: \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u0153 COMPLETED\n179:\n180:**Key Items Found**:\n181:1. Conversation Tree Visualization Tools\n182:2. GUI over the cli\n183:3. Enhanced Callback System for functional prompts\n184:4. Update base.py to use config file\n185:5. Centralized file-writing wrapper with BOM removal\n186:6. Tool requirements decorator\n187:7. Repo-wide logging strategy\n188:8. bot.requirements for tool dependencies\n189:9. AST warnings cleanup\n190:10. python_edit feedback improvements\n191:11. Autosave behavior improvements\n192:12. Terminal tool output format\n193:13. Tool configurations in CI/CD\n194:14. Fix test parallelism\n195:15. Uniform tempfile handling\n196:\n197:**Priority**: High\n198:\n199:---\n200:\n201:## 8. Refactor build_messages to Match tool_handler Pattern\n202:\n203:**Goal**: Standardize message building to follow tool_handler's build_schema pattern\n204:\n205:**Benefits**: Consistency, easier provider addition, better maintenance\n206:\n207:**Priority**: Medium-High\n208:\n209:---\n210:\n211:## 9. Organize Tests Better - Best Practices\n212:\n213:**Goal**: Implement testing best practices\n214:\n215:**Structure**: tests/unit/, tests/integration/, tests/e2e/, tests/fixtures/\n216:\n217:**Patterns**: pytest fixtures, parameterization, markers, AAA pattern\n218:\n219:**Related**: Fix test parallelism, uniform tempfile handling\n220:\n221:**Priority**: High\n222:\n223:---\n224:\n225:## 10. Expand self_tools Beyond branch_self\n226:\n227:**Goal**: Add more tools for bots to manage their own conversation context\n228:\n229:**Proposed Tools**:\n230:- delete_stale_context: Remove outdated message pairs\n231:- summarize_context: Replace verbose history with summary\n232:- fork_from_node: Create branch from earlier point\n233:- merge_branches: Combine insights from multiple branches\n234:- list_conversation_nodes: Get overview of structure\n235:- mark_checkpoint: Create named checkpoint\n236:- goto_checkpoint: Navigate to checkpoint\n237:\n238:**Implementation Phases**:\n239:1. Core context management (delete_stale_context, list_conversation_nodes, fork_from_node)\n240:2. Advanced operations (summarize_context, merge_branches)\n241:3. Navigation helpers (mark_checkpoint, goto_checkpoint)\n242:\n243:**Priority**: Medium-High\n244:\n245:---\n246:\n247:## 11. Remove Print Statements from anthropic_bots.py\n248:\n249:**Goal**: Remove all print statements and replace with logging or callbacks\n250:\n251:**Locations**: Mailbox send, tool execution, API calls, state transitions\n252:\n253:**Implementation**: Replace with logging module or callback system (item 12)\n254:\n255:**Related**: Repo-wide logging strategy\n256:\n257:**Priority**: Medium\n258:\n259:---\n260:\n261:## 12. Add Callbacks to Major Bot Operation Steps\n262:\n263:**Goal**: Implement callback system with progress indicators\n264:\n265:**Major Steps**:\n266:1. Message Building\n267:2. API Call\n268:3. Response Processing\n269:4. Tool Execution\n270:\n271:**Progress Indicator**: Four dots (. .. ... ....)\n272:\n273:**Callback Interface**:\n274:- on_step_start(step_name, metadata)\n275:- on_step_complete(step_name, result)\n276:- on_step_error(step_name, error)\n277:- on_tool_start(tool_name, args)\n278:- on_tool_complete(tool_name, result)\n279:\n280:**Use Cases**: Progress indication, logging, monitoring, debugging, testing\n281:\n282:**Related**: Enhanced Callback System for functional prompts\n283:\n284:**Priority**: Medium-High\n285:\n286:---\n287:\n288:\n289:## 13. MCP Integration (Model Context Protocol)\n290:\n291:**Current State**:\n292:- No MCP support\n293:- Tools are bot-specific, not accessible to other MCP clients\n294:\n295:**Goal**: Integrate MCP as both client and server\n296:\n297:### MCP as Client (Priority 1)\n298:**Purpose**: Connect to existing MCP servers to access their tools\n299:\n300:**Implementation**:\n301:```python\n302:# Bot gains access to all MCP server tools\n303:bot = AnthropicBot()\n304:bot.add_mcp_server(\"filesystem\", \"npx @modelcontextprotocol/server-filesystem /path\")\n305:bot.add_mcp_server(\"github\", \"npx @modelcontextprotocol/server-github\")\n306:bot.add_mcp_server(\"postgres\", \"npx @modelcontextprotocol/server-postgres\")\n307:# Now bot can use filesystem, GitHub, and PostgreSQL tools via MCP\n308:```\n309:\n310:**Benefits**:\n311:- Instant access to growing MCP ecosystem\n312:- Standardized tool connectivity (like USB-C for AI)\n313:- Compatible with Claude Desktop, Cursor, and other MCP clients\n314:- No need to build custom integrations for each tool\n315:\n316:**MCP Ecosystem (October 2025)**:\n317:- 90% of organizations projected to use MCP by end of 2025\n318:- Major adoption: Microsoft, OpenAI, Google DeepMind\n319:- Early adopters: Block, Apollo, Zed, Replit, Codeium, Sourcegraph\n320:- Microsoft has MCP in Azure AI Foundry\n321:- Market growing from $1.2B (2022) to $4.5B (2025)\n322:\n323:**Implementation Steps**:\n324:1. Install MCP Python SDK: `pip install mcp`\n325:2. Create MCPToolWrapper class to convert MCP tools to bot-compatible functions\n326:3. Add `bot.add_mcp_server(name, command)` method\n327:4. Handle async MCP communication\n328:5. Test with popular MCP servers (filesystem, github, etc.)\n329:\n330:### MCP as Server (Priority 2)\n331:**Purpose**: Expose your bot's tools as MCP server for other applications\n332:\n333:**Implementation**:\n334:```python\n335:# Your code_tools become available to Claude Desktop, Cursor, etc.\n336:from bots.mcp import MCPServer\n337:\n338:mcp_server = MCPServer()\n339:mcp_server.expose_tools(code_tools)\n340:mcp_server.expose_tools([python_edit, python_view, branch_self])\n341:mcp_server.start()\n342:```\n343:\n344:**Benefits**:\n345:- Your excellent tools (python_edit, branch_self, etc.) become available to entire MCP ecosystem\n346:- Other developers can use them in Claude Desktop, Cursor, Windsurf, etc.\n347:- Positions your project as valuable MCP tool provider\n348:- Increases project visibility and adoption\n349:\n350:**Tools to Expose**:\n351:- python_edit - Scope-aware Python editing\n352:- python_view - Scope-aware Python viewing\n353:- branch_self - Parallel conversation exploration\n354:- execute_python - Python code execution\n355:- execute_powershell - PowerShell execution\n356:- All self_tools (when implemented)\n357:\n358:**Related Standards**:\n359:- Open Agentic Schema Framework (OASF) - Launched early 2025 for agent interoperability\n360:- Agent Connect Protocol (ACP) - Part of AGNTCY initiative for agent-to-agent communication\n361:\n362:**Priority**: High (Phase 1 for Client, Phase 2 for Server)\n363:\n364:**Why MCP Over LangChain**:\n365:- Simpler protocol vs. complex framework\n366:- Industry standard emerging NOW\n367:- LangChain itself is adapting to MCP (released MCP adapters)\n368:- \"USB-C for AI\" - universal connectivity standard\n369:- Less complexity than direct LangChain integration\n370:\n371:---\n372:\n373:## 14. OpenTelemetry Integration for Observability\n374:\n375:**Current State**:\n376:- Print statements scattered throughout code (item 11)\n377:- No structured logging or monitoring\n378:- Difficult to debug issues in production\n379:- No visibility into performance bottlenecks\n380:- No cost/token tracking\n381:\n382:**Goal**: Implement comprehensive observability using OpenTelemetry standard\n383:\n384:### What is Observability?\n385:\n386:**Observability** = Understanding what's happening inside your system by examining its outputs\n387:\n388:**Three Pillars**:\n389:1. **Logs** - What happened? (events, errors, state changes)\n390:2. **Metrics** - How much/many? (API calls, response times, token usage)\n391:3. **Traces** - Journey of a request (full flow with timing)\n392:\n393:### Why OpenTelemetry?\n394:\n395:**OpenTelemetry** is the industry standard for observability (like MCP for tools):\n396:- CNCF-backed open standard\n397:- Vendor-agnostic (works with any monitoring tool)\n398:- Automatic instrumentation available\n399:- Growing consensus: \"Should be the standard for LLM observability\"\n400:- Semantic conventions for GenAI systems (2025)\n401:\n402:### Implementation Design\n403:\n404:#### Basic Setup\n405:```python\n406:from opentelemetry import trace\n407:from opentelemetry.sdk.trace import TracerProvider\n408:from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n409:\n410:# Setup (one time)\n411:tracer_provider = TracerProvider()\n412:tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n413:trace.set_tracer_provider(tracer_provider)\n414:\n415:tracer = trace.get_tracer(__name__)\n416:```\n417:\n418:#### Bot Integration\n419:```python\n420:class AnthropicBot:\n421:    def respond(self, prompt):\n422:        with tracer.start_span(\"bot.respond\") as span:\n423:            span.set_attribute(\"prompt_length\", len(prompt))\n424:            span.set_attribute(\"provider\", \"anthropic\")\n425:            span.set_attribute(\"model\", self.model)\n426:\n427:            # Step 1: Build messages\n428:            with tracer.start_span(\"build_messages\"):\n429:                messages = self._build_messages()\n430:                span.set_attribute(\"message_count\", len(messages))\n431:\n432:            # Step 2: API call\n433:            with tracer.start_span(\"api_call\") as api_span:\n434:                api_span.set_attribute(\"input_tokens\", count_tokens(messages))\n435:                response = self._call_api(messages)\n436:                api_span.set_attribute(\"output_tokens\", response.usage.output_tokens)\n437:                api_span.set_attribute(\"cost_usd\", calculate_cost(response.usage))\n438:\n439:            # Step 3: Process response\n440:            with tracer.start_span(\"process_response\"):\n441:                result = self._process_response(response)\n442:\n443:            # Step 4: Execute tools if needed\n444:            if result.tool_calls:\n445:                with tracer.start_span(\"execute_tools\") as tool_span:\n446:                    tool_span.set_attribute(\"tool_count\", len(result.tool_calls))\n447:                    for tool_call in result.tool_calls:\n448:                        with tracer.start_span(f\"tool.{tool_call.name}\") as t_span:\n449:                            t_span.set_attribute(\"tool_name\", tool_call.name)\n450:                            t_span.set_attribute(\"tool_args\", str(tool_call.args))\n451:                            result = self._execute_tool(tool_call)\n452:                            t_span.set_attribute(\"tool_result_length\", len(result))\n453:\n454:            return result\n455:```\n456:\n457:#### What You Get\n458:```\n459:Trace: bot.respond (4.2s)\n460:  \u00c3\u00a2\u00e2\u20ac\u009d\u00c5\u201c\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac build_messages (0.1s)\n461:  \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 message_count: 5\n462:  \u00c3\u00a2\u00e2\u20ac\u009d\u00c5\u201c\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac api_call (2.8s)\n463:  \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 input_tokens: 1234\n464:  \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 output_tokens: 567\n465:  \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 cost_usd: 0.015\n466:  \u00c3\u00a2\u00e2\u20ac\u009d\u00c5\u201c\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac process_response (0.05s)\n467:  \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u009d\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac execute_tools (1.25s)\n468:      \u2014\u00c2\u00a2 tool_count: 2\n469:      \u00c3\u00a2\u00e2\u20ac\u009d\u00c5\u201c\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac tool.python_view (0.8s)\n470:      \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 tool_name: python_view\n471:      \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u0161   \u2014\u00c2\u00a2 tool_result_length: 2456\n472:      \u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u20ac\u009d\u00c3\u00a2\u00e2\u20ac\u009d\u00e2\u201a\u00ac tool.python_edit (0.45s)\n473:          \u2014\u00c2\u00a2 tool_name: python_edit\n474:          \u2014\u00c2\u00a2 tool_result_length: 89\n475:```\n476:\n477:### Integration with Existing Items\n478:\n479:#### Replaces Item 11 (Remove Print Statements)\n480:**Before:**\n481:```python\n482:print(\"Sending message to mailbox...\")\n483:print(f\"Tool execution: {tool_name}\")\n484:```\n485:\n486:**After:**\n487:```python\n488:logger.info(\"mailbox.send\", extra={\"message_id\": msg_id})\n489:span.add_event(\"tool.execution\", {\"tool_name\": tool_name})\n490:```\n491:\n492:#### Enables Item 12 (Progress Callbacks)\n493:```python\n494:class ObservabilityCallbacks(BotCallbacks):\n495:    def on_step_start(self, step_name, metadata):\n496:        # Log to OpenTelemetry\n497:        span = trace.get_current_span()\n498:        span.add_event(f\"step.{step_name}.start\", metadata or {})\n499:\n500:        # Show progress to user (four dots)\n501:        print(\".\", end=\"\", flush=True)\n502:\n503:    def on_step_complete(self, step_name, result):\n504:        span = trace.get_current_span()\n505:        span.add_event(f\"step.{step_name}.complete\")\n506:```\n507:\n508:### Metrics to Track\n509:\n510:**Performance Metrics**:\n511:- `bot.response_time` - Total time for bot.respond()\n512:- `bot.api_call_duration` - Time spent in API calls\n513:- `bot.tool_execution_duration` - Time spent executing tools\n514:- `bot.message_building_duration` - Time building messages\n515:\n516:**Usage Metrics**:\n517:- `bot.api_calls_total` - Count of API calls (by provider, model)\n518:- `bot.tool_calls_total` - Count of tool executions (by tool name)\n519:- `bot.conversations_total` - Count of conversations\n520:- `bot.tokens_used` - Token usage (input/output, by provider)\n521:\n522:**Cost Metrics**:\n523:- `bot.cost_usd` - Cost in USD (by provider, model)\n524:- `bot.cost_per_conversation` - Average cost per conversation\n525:\n526:**Error Metrics**:\n527:- `bot.errors_total` - Count of errors (by type, provider)\n528:- `bot.tool_failures_total` - Count of tool failures (by tool)\n529:\n530:### Benefits\n531:\n532:**For Development**:\n533:- Debug issues quickly (see exact flow and timing)\n534:- Identify performance bottlenecks\n535:- Understand tool usage patterns\n536:- Track down errors with full context\n537:\n538:**For Production**:\n539:- Monitor system health\n540:- Track costs and token usage\n541:- Set up alerts for anomalies\n542:- Audit trails for compliance\n543:- Performance optimization\n544:\n545:**For Users**:\n546:- Better progress indicators (know what's happening)\n547:- Faster issue resolution\n548:- More reliable system\n549:\n550:### Implementation Steps\n551:\n552:**Phase 1: Basic Tracing**\n553:1. Install OpenTelemetry: `pip install opentelemetry-api opentelemetry-sdk`\n554:2. Add tracing to bot.respond() method\n555:3. Add tracing to tool execution\n556:4. Test with console exporter\n557:\n558:**Phase 2: Structured Logging**\n559:1. Replace all print statements with logging\n560:2. Add OpenTelemetry logging integration\n561:3. Configure log levels (DEBUG, INFO, WARNING, ERROR)\n562:4. Add structured context to logs\n563:\n564:**Phase 3: Metrics**\n565:1. Add metrics collection\n566:2. Track performance, usage, cost metrics\n567:3. Set up dashboards (Grafana, Datadog, etc.)\n568:\n569:**Phase 4: Production Observability**\n570:1. Configure exporters (Jaeger, Zipkin, or cloud provider)\n571:2. Set up alerting\n572:3. Create runbooks for common issues\n573:4. Add cost tracking and budgets\n574:\n575:### Observability Backends (Choose One)\n576:\n577:**Open Source**:\n578:- Jaeger - Distributed tracing\n579:- Prometheus + Grafana - Metrics and dashboards\n580:- Loki - Log aggregation\n581:\n582:**Commercial (with free tiers)**:\n583:- Datadog - Full observability platform\n584:- New Relic - APM and monitoring\n585:- Honeycomb - Observability for complex systems\n586:- Lightstep - Distributed tracing\n587:\n588:**All work with OpenTelemetry** - switch anytime without code changes!\n589:\n590:### Related Standards\n591:\n592:- **OpenTelemetry GenAI Semantic Conventions** - Standard attributes for AI/LLM systems\n593:- **OWASP LLM Security Verification Standard** - Security observability requirements\n594:\n595:**Priority**: High (Phase 1)\n596:\n597:**Effort**: Medium - Incremental implementation possible\n598:\n599:**Impact**: High - Essential for production, debugging, and optimization\n600:\n601:---\n602:\n603:## 15. LiteLLM Integration for Provider Expansion\n604:\n605:**Current State**:\n606:- Manual implementation for each provider (Anthropic, OpenAI, Gemini)\n607:- Each provider requires custom code for API calls, message formatting, tool handling\n608:- Adding new providers is time-consuming\n609:\n610:**Goal**: Use LiteLLM as unified API layer for 100+ LLM providers\n611:\n612:### What is LiteLLM?\n613:\n614:**LiteLLM** provides a unified interface for 100+ LLM providers:\n615:- OpenAI, Anthropic, Google, Cohere, Mistral, Replicate\n616:- Azure OpenAI, AWS Bedrock, Vertex AI\n617:- Local models (Ollama, LM Studio, vLLM)\n618:- Open source models (Llama, Mistral, etc.)\n619:\n620:**One API format for all providers:**\n621:```python\n622:from litellm import completion\n623:\n624:# Works with ANY provider\n625:response = completion(\n626:    model=\"gpt-4\",  # or \"claude-3-5-sonnet\", \"gemini-pro\", etc.\n627:    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n628:)\n629:```\n630:\n631:### Benefits\n632:\n633:**Instant Provider Support**:\n634:- 100+ providers immediately available\n635:- No need to implement each provider manually\n636:- Automatic handling of provider-specific quirks\n637:\n638:**Unified Interface**:\n639:- Same code works across all providers\n640:- Easy to switch providers\n641:- Easy to compare providers\n642:\n643:**Active Maintenance**:\n644:- LiteLLM team handles provider API changes\n645:- New providers added regularly\n646:- Bug fixes and updates\n647:\n648:**Cost Tracking**:\n649:- Built-in cost calculation for all providers\n650:- Token usage tracking\n651:- Budget management\n652:\n653:### Implementation Approach\n654:\n655:#### Option 1: LiteLLM as Base Layer\n656:```python\n657:from litellm import completion\n658:import bots\n659:\n660:class LiteLLMBot(bots.BaseBot):\n661:    def __init__(self, model=\"gpt-4\", **kwargs):\n662:        super().__init__(**kwargs)\n663:        self.model = model\n664:\n665:    def _call_api(self, messages):\n666:        response = completion(\n667:            model=self.model,\n668:            messages=messages,\n669:            tools=self._get_tool_schemas(),\n670:            **self.api_params\n671:        )\n672:        return response\n673:\n674:# Now supports 100+ providers!\n675:bot = LiteLLMBot(model=\"claude-3-5-sonnet-20241022\")\n676:bot = LiteLLMBot(model=\"gpt-4-turbo\")\n677:bot = LiteLLMBot(model=\"gemini-1.5-pro\")\n678:bot = LiteLLMBot(model=\"ollama/llama3\")  # Local model!\n679:```\n680:\n681:#### Option 2: Hybrid Approach\n682:Keep existing provider implementations for fine-grained control, add LiteLLM for quick provider support:\n683:```python\n684:# Use native implementation for main providers\n685:bot = AnthropicBot()  # Full control, optimized\n686:\n687:# Use LiteLLM for others\n688:bot = LiteLLMBot(model=\"cohere/command-r-plus\")  # Quick support\n689:```\n690:\n691:### Integration with Other Items\n692:\n693:**Complements Item 5** (CLI Haiku Bots):\n694:```python\n695:PROVIDER_MODELS = {\n696:    'anthropic': {\n697:        'flagship': 'claude-3-5-sonnet-20241022',\n698:        'fast': 'claude-3-5-haiku-20241022'\n699:    },\n700:    'openai': {\n701:        'flagship': 'gpt-4-turbo',\n702:        'fast': 'gpt-4o-mini'\n703:    },\n704:    'google': {\n705:        'flagship': 'gemini-1.5-pro',\n706:        'fast': 'gemini-1.5-flash'\n707:    },\n708:    'cohere': {\n709:        'flagship': 'command-r-plus',\n710:        'fast': 'command-r'\n711:    },\n712:    'mistral': {\n713:        'flagship': 'mistral-large-latest',\n714:        'fast': 'mistral-small-latest'\n715:    }\n716:    # Easy to add more!\n717:}\n718:```\n719:\n720:**Enables Item 1** (Expand Provider Base):\n721:Instead of manually implementing each provider, use LiteLLM to get instant support for:\n722:- Cohere\n723:- Mistral AI\n724:- Local models (Ollama, LM Studio)\n725:- Azure OpenAI\n726:- AWS Bedrock\n727:- Hugging Face\n728:\n729:### Considerations\n730:\n731:**Pros**:\n732:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Instant 100+ provider support\n733:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Unified interface\n734:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Active maintenance\n735:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Cost tracking built-in\n736:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Easy to test different providers\n737:\n738:**Cons**:\n739:- \u00c3\u00a2\u00c5\u00a1\u00c2\u00a0\u00c3\u00af\u00c2\u00b8\u00c2\u008f Abstraction layer (slight overhead)\n740:- \u00c3\u00a2\u00c5\u00a1\u00c2\u00a0\u00c3\u00af\u00c2\u00b8\u00c2\u008f Less control over provider-specific features\n741:- \u00c3\u00a2\u00c5\u00a1\u00c2\u00a0\u00c3\u00af\u00c2\u00b8\u00c2\u008f Dependency on external library\n742:\n743:**Recommendation**:\n744:- Use LiteLLM for quick provider expansion\n745:- Keep native implementations for primary providers (Anthropic, OpenAI, Gemini)\n746:- Best of both worlds: control + breadth\n747:\n748:### Implementation Steps\n749:\n750:1. Install LiteLLM: `pip install litellm`\n751:2. Create LiteLLMBot class extending BaseBot\n752:3. Implement _call_api() using litellm.completion()\n753:4. Test with multiple providers\n754:5. Add provider configuration\n755:6. Update documentation\n756:7. Add tests for LiteLLM integration\n757:\n758:**Priority**: Medium (Phase 2)\n759:\n760:**Effort**: Low - LiteLLM handles most complexity\n761:\n762:**Impact**: High - Instant support for 100+ providers\n763:\n764:---\n765:\n766:## Additional Items from large_refactor_tasks.md\n767:\n768:### 16. Centralized File-Writing Wrapper\n769:**Goal**: Wrapper with BOM removal logic (for all tools using toolify decorator?)\n770:**Priority**: Medium\n771:\n772:### 17. Tool Requirements Decorator\n773:**Goal**: Decorator that applies tool requirements to functions\n774:**Priority**: Low-Medium\n775:\n776:### 18. Bot Requirements System\n777:**Goal**: bot.requirements parameter for tool dependencies\n778:**Priority**: Medium\n779:\n780:### 19. AST String vs Constant Warnings\n781:**Goal**: Clean up AST warnings\n782:**Priority**: Low\n783:\n784:### 20. python_edit Feedback\n785:**Goal**: Better feedback when input is incorrect\n786:**Priority**: Complete!\n787:\n788:### 21. Autosave Behavior\n789:**Goal**: More intuitive autosave (save over loaded filename)\n790:**Priority**: Low-Medium\n791:\n792:### 22. Terminal Tool Output Format\n793:**Goal**: Implicit [dir]> format like in terminal_tools.py\n794:**Priority**: Complete!\n795:\n796:### 23. Tool Configurations in CI/CD\n797:**Goal**: Tool factories respecting CI/CD constraints\n798:**Priority**: Medium\n799:\n800:### 24. Test Parallelism\n801:**Goal**: Fix test parallelism issues (mainly in terminal_tools)\n802:**Priority**: High\n803:**Status**: \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u0153 COMPLETED (test-efficiency branch)\n804:\n805:```\n806:Fixed PowerShell test files to use unique temp directories per test. Removed serial execution marker. Updated pr-checks.yml to use -n 12 for parallel execution. Tests now run in parallel without file conflicts.\n807:```\n808:\n809:### 25. Uniform Tempfile Handling\n810:**Goal**: Consistent tempfile handling in tests\n811:**Priority**: High\n812:**Reason**: Tests pollute repository with extraneous files\n813:**Status**: \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u0153 COMPLETED (test-efficiency branch)\n814:\n815:```\n816:Fixed test_patch_edit.py, test_class_replace.py, and test_python_edit_edge_cases.py to use temp directories. All test artifacts now properly isolated and cleaned up.\n817:```\n818:\n819:### Docker Testing Infrastructure (Archived)\n820:**Status**: \u2717 ARCHIVED (cleanup-docker branch)\n821:\n822:Docker-based test infrastructure was developed but abandoned due to Docker Desktop issues. All Docker testing files have been moved to `archived_docker_testing/` directory and preserved on the `docker-testing` branch for potential future use. The infrastructure included Dockerfile, test orchestration scripts, comprehensive documentation, and pytest integration with --use-docker flags. See `archived_docker_testing/README.md` for details.\n823:\n824:---\n825:\n826:## 26. GitHub Branch Protection and CI/CD Guardrails\n827:\n828:**Current State**:\n829:- Direct pushes to main branch are allowed\n830:- No enforcement of CI/CD checks before merge\n831:- Manual code review process\n832:\n833:**Goal**: Implement branch protection rules and automated code review\n834:\n835:**Priority**: Very High\n836:\n837:### Branch Protection Rules\n838:\n839:**Requirements for main branch**:\n840:1. **Require pull request before merging**\n841:   - No direct pushes to main\n842:   - All changes must go through PR workflow\n843:\n844:2. **Require status checks to pass**\n845:   - All CI/CD tests must pass (green checkmark \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u0153)\n846:   - Cannot merge until all checks are green\n847:\n848:3. **Require code review**\n849:   - At least one approval required\n850:   - CodeRabbit AI review + human review\n851:\n852:4. **Additional protections**:\n853:   - Require branches to be up to date before merging\n854:   - Require linear history (optional)\n855:   - Include administrators in restrictions\n856:\n857:### Implementation Steps\n858:\n859:#### Step 1: Configure Branch Protection\n860:```\n861:GitHub Repository Settings:\n862:1. Go to Settings \u00c3\u00a2\u00e2\u20ac\u00a0\u00e2\u20ac\u2122 Branches\n863:2. Add branch protection rule for \"main\"\n864:3. Enable:\n865:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Require a pull request before merging\n866:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Require approvals (1 minimum)\n867:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Dismiss stale pull request approvals when new commits are pushed\n868:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Require status checks to pass before merging\n869:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Require branches to be up to date before merging\n870:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Require conversation resolution before merging\n871:   \u00c3\u00a2\u00cb\u0153\u00e2\u20ac\u02dc Do not allow bypassing the above settings (include administrators)\n872:```\n873:\n874:#### Step 2: Required Status Checks\n875:Configure these checks to be required:\n876:- `test` - Run test suite\n877:- `lint` - Code quality checks\n878:- `type-check` - Type checking (if applicable)\n879:- `coverage` - Code coverage threshold\n880:- `security` - Security scanning\n881:- `coderabbit` - AI code review\n882:\n883:#### Step 3: Set Up CodeRabbit\n884:\n885:**What is CodeRabbit?**\n886:- AI-powered code review assistant\n887:- Automated PR reviews with contextual feedback\n888:- Suggests improvements, catches bugs, identifies security issues\n889:- Integrates directly with GitHub PRs\n890:\n891:**Setup**:\n892:1. Install CodeRabbit GitHub App\n893:   - Go to https://github.com/apps/coderabbitai\n894:   - Click \"Install\"\n895:   - Select repository: benbuzz790/bots\n896:   - Grant permissions\n897:\n898:2. Configure CodeRabbit\n899:   - Create `.coderabbit.yaml` in repository root:\n900:   ```yaml\n901:   # .coderabbit.yaml\n902:   language: \"en-US\"\n903:\n904:   reviews:\n905:     profile: \"chill\"  # or \"assertive\" for stricter reviews\n906:     request_changes_workflow: false  # Don't block PRs, just suggest\n907:     high_level_summary: true\n908:     poem: false  # Disable fun poems (optional)\n909:     review_status: true  # Post review status\n910:\n911:   chat:\n912:     auto_reply: true\n913:\n914:   # Customize review focus\n915:   tone_instructions: |\n916:     Focus on:\n917:     - Code quality and maintainability\n918:     - Potential bugs and edge cases\n919:     - Security vulnerabilities\n920:     - Performance issues\n921:     - Test coverage\n922:     - Documentation completeness\n923:\n924:     Follow project principles:\n925:     - YAGNI (You Aren't Gonna Need It)\n926:     - KISS (Keep It Simple, Stupid)\n927:     - Defensive programming (NASA methodology)\n928:     - Small, incremental changes\n929:\n930:   # File patterns to review\n931:   path_filters:\n932:     - \"!**/*.md\"  # Skip markdown files\n933:     - \"!**/test_*.py\"  # Optional: lighter review for tests\n934:\n935:   # Auto-review settings\n936:   auto_review:\n937:     enabled: true\n938:     drafts: false  # Don't review draft PRs\n939:     base_branches:\n940:       - main\n941:   ```\n942:\n943:3. Test CodeRabbit\n944:   - Create a test PR\n945:   - CodeRabbit should automatically comment with review\n946:   - Verify it's providing useful feedback\n947:\n948:#### Step 4: Update CI/CD Workflow\n949:\n950:Update `.github/workflows/test.yml` to be a required check:\n951:\n952:```yaml\n953:name: CI/CD Pipeline\n954:\n955:on:\n956:  pull_request:\n957:    branches: [ main ]\n958:  push:\n959:    branches: [ main ]\n960:\n961:jobs:\n962:  test:\n963:    name: Run Tests\n964:    runs-on: ubuntu-latest\n965:\n966:    steps:\n967:      - uses: actions/checkout@v3\n968:\n969:      - name: Set up Python\n970:        uses: actions/setup-python@v4\n971:        with:\n972:          python-version: '3.12'\n973:\n974:      - name: Install dependencies\n975:        run: |\n976:          pip install -e .[dev]\n977:\n978:      - name: Run tests\n979:        run: |\n980:          pytest tests/ -v --cov=bots --cov-report=term-missing\n981:\n982:      - name: Check coverage threshold\n983:        run: |\n984:          coverage report --fail-under=80\n985:\n986:  lint:\n987:    name: Code Quality\n988:    runs-on: ubuntu-latest\n989:\n990:    steps:\n991:      - uses: actions/checkout@v3\n992:\n993:      - name: Set up Python\n994:        uses: actions/setup-python@v4\n995:        with:\n996:          python-version: '3.12'\n997:\n998:      - name: Install dependencies\n999:        run: |\n1000:          pip install black flake8 mypy\n1001:\n1002:      - name: Run Black\n1003:        run: black --check .\n1004:\n1005:      - name: Run Flake8\n1006:        run: flake8 bots/ --max-line-length=100\n1007:\n1008:      - name: Run MyPy (if using type hints)\n1009:        run: mypy bots/ --ignore-missing-imports\n1010:        continue-on-error: true  # Optional: don't fail on type errors initially\n1011:\n1012:  security:\n1013:    name: Security Scan\n1014:    runs-on: ubuntu-latest\n1015:\n1016:    steps:\n1017:      - uses: actions/checkout@v3\n1018:\n1019:      - name: Run Bandit security scan\n1020:        run: |\n1021:          pip install bandit\n1022:          bandit -r bots/ -f json -o bandit-report.json\n1023:        continue-on-error: true\n1024:\n1025:      - name: Upload security report\n1026:        uses: actions/upload-artifact@v3\n1027:        with:\n1028:          name: security-report\n1029:          path: bandit-report.json\n1030:```\n1031:\n1032:#### Step 5: PR Template\n1033:\n1034:Create `.github/pull_request_template.md`:\n1035:\n1036:```markdown\n1037:## Description\n1038:<!-- Describe your changes in detail -->\n1039:\n1040:## Type of Change\n1041:- [ ] Bug fix (non-breaking change which fixes an issue)\n1042:- [ ] New feature (non-breaking change which adds functionality)\n1043:- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n1044:- [ ] Documentation update\n1045:- [ ] Refactoring (no functional changes)\n1046:- [ ] Test updates\n1047:\n1048:## Related Issues\n1049:<!-- Link to related issues: Fixes #123, Relates to #456 -->\n1050:\n1051:## Testing\n1052:<!-- Describe the tests you ran and how to reproduce -->\n1053:- [ ] All existing tests pass\n1054:- [ ] Added new tests for changes\n1055:- [ ] Manual testing completed\n1056:\n1057:## Checklist\n1058:- [ ] Code follows project style guidelines (YAGNI, KISS, defensive programming)\n1059:- [ ] Self-review completed\n1060:- [ ] Comments added for complex logic\n1061:- [ ] Documentation updated (if applicable)\n1062:- [ ] No new warnings generated\n1063:- [ ] Tests added/updated and passing\n1064:- [ ] Changes are small and focused (not too big)\n1065:\n1066:## Screenshots (if applicable)\n1067:<!-- Add screenshots for UI changes -->\n1068:\n1069:## Additional Notes\n1070:<!-- Any additional information for reviewers -->\n1071:```\n1072:\n1073:### Workflow After Implementation\n1074:\n1075:**Developer Workflow**:\n1076:1. Create feature branch: `git checkout -b feature/my-feature`\n1077:2. Make changes (small, incremental)\n1078:3. Commit and push: `git push origin feature/my-feature`\n1079:4. Create Pull Request on GitHub\n1080:5. Wait for CI/CD checks to run (automated)\n1081:6. CodeRabbit reviews automatically\n1082:7. Address CodeRabbit feedback\n1083:8. Request human review\n1084:9. Once approved + all checks green \u00c3\u00a2\u00e2\u20ac\u00a0\u00e2\u20ac\u2122 Merge\n1085:\n1086:**What Blocks a Merge**:\n1087:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 Any CI/CD test failures\n1088:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 Code coverage below threshold\n1089:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 Linting errors\n1090:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 No approval from reviewer\n1091:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 Unresolved conversations\n1092:- \u00c3\u00a2\u00c2\u009d\u00c5\u2019 Branch not up to date with main\n1093:\n1094:**What Allows a Merge**:\n1095:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 All tests passing (green checkmark)\n1096:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Code coverage meets threshold\n1097:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Linting passes\n1098:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 At least one approval\n1099:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 All conversations resolved\n1100:- \u00c3\u00a2\u00c5\u201c\u00e2\u20ac\u00a6 Branch up to date\n1101:\n1102:### Benefits\n1103:\n1104:**Quality Assurance**:\n1105:- No broken code reaches main\n1106:- Consistent code quality\n1107:- Automated checks catch issues early\n1108:\n1109:**Code Review**:\n1110:- CodeRabbit provides instant AI feedback\n1111:- Human reviewers focus on architecture/design\n1112:- Faster review cycles\n1113:\n1114:**Safety**:\n1115:- Protected main branch\n1116:- No accidental direct pushes\n1117:- Rollback is easier (clean history)\n1118:\n1119:**Collaboration**:\n1120:- Clear PR process\n1121:- Discussion on changes\n1122:- Knowledge sharing through reviews\n1123:\n1124:### Alternative/Additional Tools\n1125:\n1126:**Other AI Code Review Tools**:\n1127:- **GitHub Copilot for PRs** - Similar to CodeRabbit\n1128:- **Sourcery** - Python-specific refactoring suggestions\n1129:- **DeepCode (Snyk)** - Security-focused\n1130:\n1131:**Traditional Tools**:\n1132:- **SonarCloud** - Code quality and security\n1133:- **Codecov** - Coverage reporting with PR comments\n1134:- **Dependabot** - Dependency updates\n1135:\n1136:**Recommendation**: Start with CodeRabbit (easy setup, comprehensive) + existing CI/CD\n1137:\n1138:### Cost Considerations\n1139:\n1140:**CodeRabbit Pricing** (as of 2025):\n1141:- Free for open source projects\n1142:- Pro: $15/user/month for private repos\n1143:- Enterprise: Custom pricing\n1144:\n1145:**GitHub Actions**:\n1146:- 2,000 minutes/month free for private repos\n1147:- Unlimited for public repos\n1148:\n1149:### Implementation Priority\n1150:\n1151:**Phase 1** (Immediate):\n1152:1. Set up branch protection rules\n1153:2. Configure required status checks\n1154:3. Update CI/CD workflow\n1155:\n1156:**Phase 2** (Short-term):\n1157:1. Install and configure CodeRabbit\n1158:2. Create PR template\n1159:3. Document workflow for contributors\n1160:\n1161:**Phase 3** (Ongoing):\n1162:1. Refine CodeRabbit configuration based on feedback\n1163:2. Add additional checks as needed\n1164:3. Monitor and improve coverage thresholds\n1165:\n1166:**Priority**: High (Phase 1) - Essential for code quality and preventing regressions\n1167:\n1168:**Effort**: Low-Medium - Mostly configuration, minimal code changes\n1169:\n1170:**Impact**: High - Significantly improves code quality and prevents bugs from reaching main\n1171:\n1172:---\n1173:\n1174:## Implementation Roadmap\n1175:\n1176:### Phase 1: Repo Reliability (High Priority)\n1177:1. **GitHub Branch Protection & CodeRabbit** (item 26) - Guardrails for main branch - DONE\n1178:3. **OpenTelemetry Integration** (item 14) - Observability and monitoring\n1179:4. Remove print statements (item 11) - Replaced by OpenTelemetry\n1180:5. Add callback system (item 12) - Integrated with OpenTelemetry\n1181:6. Organize tests better (item 9)\n1182:7. Fix test parallelism (item 24)\n1183:8. Uniform tempfile handling (item 25)\n1184:9. Refactor build_messages pattern (item 8)\n1185:10. Ensure CLI haiku bots match provider (item 5)\n1186:\n1187:### Phase 2: Core Features & Expansion (Medium Priority)\n1188:1. **MCP Integration - Client** (item 13) - Industry standard for tool connectivity\n1189:2. **MCP Integration - Server** (item 13) - Expose tools to ecosystem\n1190:3. **LiteLLM Integration** (item 15) - 100+ provider support\n1191:4. Configure CLI more thoroughly (item 6)\n1192:5. Update base.py to use config file\n1193:7. Centralized file-writing wrapper (item 16)\n1194:8. Bot requirements system (item 18)\n1195:9. Expand self_tools Phase 1 (item 10)\n1196:10. python_edit feedback improvements (item 20)\n1197:\n1198:### Phase 3: Enhancement (Medium-Low Priority)\n1199:1. Make CLI prettier (item 2) - Integrate with OpenTelemetry for progress\n1200:2. Expand self_tools Phase 2-3 (item 10)\n1201:3. Rename auto_stash to mustache (item 4)\n1202:4. Tool requirements decorator (item 17)\n1203:5. Autosave behavior improvements (item 21)\n1204:6. Terminal tool output format (item 22)\n1205:7. AST warnings cleanup (item 19)\n1206:\n1207:### Phase 4: Major Features (Low Priority, High Effort)\n1208:1. JavaScript GUI / Frontend backend (item 3)\n1209:2. Conversation tree visualization tools\n1210:3. Tool configurations in CI/CD (item 23)\n1211:4. Integration with emerging standards (OASF, ACP)\n1212:\n1213:---\n1214:\n1215:## Next Actions\n1216:\n1217:1. **Immediate**: Prioritize Phase 1 items, create work orders\n1218:2. **Short-term**: Begin Phase 1 implementation\n1219:3. **Medium-term**: Complete Phase 1, begin Phase 2\n1220:4. **Long-term**: Evaluate GUI feasibility\n1221:\n1222:---\n1223:\n1224:**Last Updated**: 3-Oct-2025\n1225:**Maintainer**: Ben Rinauto\n1226:**Status**: Active Planning"
         },
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01CBUxN4h1KQfuDcAdXjPUzb",
          "content": "1:# CI/CD Pipeline Commands Reference for Claude\n2:This document provides essential commands for accessing information from GitHub Actions CI/CD pipeline runs.\n3:## Essential Commands\n4:\n5:### 0. Running linters\n6:Run all linters to match CI checks:\n7:```powershell\n8:black --check --diff .\n9:isort --check-only --diff .\n10:flake8 . --count --statistics --show-source\n11:```\n12:\n13:To auto-fix formatting issues:\n14:```powershell\n15:black .\n16:isort .\n17:python -m bots.dev.remove_boms\n18:```\n19:\n20:### 1. View PR Check Status\n21:Get a quick overview of all checks for a PR:\n22:```powershell\n23:gh pr checks <PR_NUMBER>\n24:```\n25:### 2. View Detailed Run Logs\n26:Get the full logs from a specific workflow run:\n27:```powershell\n28:gh run view <RUN_ID> --log\n29:```\n30:**Tip:** Get the RUN_ID from the URL in gh pr checks output.\n31:\n32:---\n33:### 3. View Only Failed Logs\n34:Get just the failed job logs:\n35:```powershell\n36:gh run view <RUN_ID> --log-failed\n37:```\n38:\n39:---\n40:### 4. Search Logs for Specific Patterns\n41:Filter logs to find specific errors or patterns:\n42:```powershell\n43:gh run view <RUN_ID> --log | Select-String -Pattern \"error|FAILED|would reformat\" -Context 0,2\n44:```\n45:**Example - Find formatting issues:**\n46:```powershell\n47:gh run view 18234577748 --log | Select-String -Pattern \"would reformat\" -Context 1,3\n48:```\n49:**Example - Find test failures:**\n50:```powershell\n51:gh run view 18234577748 --log-failed | Select-String -Pattern \"FAILED|AssertionError\" -Context 0,2\n52:```\n53:\n54:---\n55:### 5. View PR Comments (including CodeRabbit)\n56:View all comments on a PR:\n57:```powershell\n58:gh pr view <PR_NUMBER> --comments\n59:```\n60:\n61:---\n62:## Common Patterns\n63:### Check if PR is ready to merge\n64:```powershell\n65:gh pr checks <PR_NUMBER> | Select-String -Pattern \"pass|fail\"\n66:```\n67:### Get summary of test failures\n68:```powershell\n69:gh run view <RUN_ID> --log | Select-String -Pattern \"FAILED tests/\" -Context 0,1\n70:```\n71:### Check Black formatting issues\n72:```powershell\n73:gh run view <RUN_ID> --log | Select-String -Pattern \"would reformat\" -Context 2,5\n74:```\n75:### Check flake8 linting issues\n76:```powershell\n77:gh run view <RUN_ID> --log | Select-String -Pattern \"E[0-9]{3}|F[0-9]{3}\" -Context 0,1\n78:```\n79:\n80:---\n81:\n82:### 6. Extract CodeRabbit AI Prompts\n83:Extract actionable AI prompts from CodeRabbit review comments:\n84:```powershell\n85:python -m bots.dev.pr_comment_parser <REPO> <PR_NUMBER>\n86:```\n87:**Example:**\n88:```powershell\n89:python -m bots.dev.pr_comment_parser owner/repo 123\n90:```\n91:**Save to file:**\n92:```powershell\n93:python -m bots.dev.pr_comment_parser owner/repo 123 output.txt\n94:```\n95:This tool extracts the \"\ud83e\udd16 Prompt for AI Agents\" sections from CodeRabbit comments, filtering out outdated comments and including both regular and inline review comments.\n96:\n97:---"
         },
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01PzbZifJT6PxqvDfBaoQ3jL",
          "content": "1:# bots: convenient agentic programming\n2:\n3:## Overview\n4:\n5:**bots** (b\u0252ts), ***n.pl.*** : Tool-using agents, encapsulated with model parameters, conversation history, and tool source code in a single sharable file.\n6:\n7:The bots library provides a structured interface for working with such agents, aiming to make agents more convenient, accessible, and sharable for developers and researchers.\n8:\n9:## Core Elements\n10:\n11:1. **The 'bot' Abstraction**\n12:   The bot abstraction provides a unified interface for working with different LLM services (like OpenAI and Anthropic),\n13:   handling all the complexity of tool management, conversation history, and state preservation. When you save a bot,\n14:   it preserves not just the conversation history and model parameters, but also all added tools and their context,\n15:   allowing complete portability and sharing between developers.\n16:\n17:   Core functions:\n18:   - `respond(prompt)`: Send a message to the bot and get its response.\n19:   - `add_tools(tools)`: Add Python functions, modules, or files as tools the bot can use.\n20:   - `save(filename)`: Save the complete bot state including conversation history and tools\n21:   - `load(filename)`: Restore a previously saved bot with all its context\n22:   - `chat()`: Start an interactive chat in a terminal with the bot\n23:\n24:2. **Automatic Function to Tool Conversion**\n25:   - Define a python function which returns a string. Call bot.add_tools(my_function). The bot can now use the function as a tool.\n26:   - Tools are saved with the bot and run locally.\n27:\n28:3. **Tree-based Conversations**\n29:   - Implements a linked tree structure for conversation histories\n30:   - Allows branching conversations and exploring multiple prompt paths\n31:   - Allows sophisticated workflows.\n32:\n33:   Example of manual conversation branching:\n34:   ```python\n35:   # Start a conversation\n36:   response = bot.respond(\"View the current working directory\")\n37:\n38:   # Save current conversation point\n39:   context_complete = bot.conversation\n40:\n41:   # Branch 1: Update a file\n42:   bot.conversation = context_complete\n43:   branch1_response = bot.respond(\"Refactor cli.py\")\n44:\n45:   # Branch 2: Update a different file\n46:   bot.conversation = context_complete\n47:   branch2_response = bot.respond(\"Refactor test_cli.py\")\n48:   ```\n49:\n50:4. **Functional Prompting**\n51:   - Core operations: chain(), branch(), prompt_while()\n52:   - Composable patterns for complex tasks\n53:   - Iteration control (prompt_while, chain_while)\n54:   - Support for parallel exploration with par_branch() and par_dispatch()\n55:\n56:   Example of using prompt_while:\n57:   ```python\n58:   # Continue analyzing until no more tools are used\n59:   responses, nodes = prompt_while(\n60:       bot,\n61:       \"Analyze this codebase and fix any issues you find\",\n62:       continue_prompt=\"ok\",\n63:       stop_condition=conditions.tool_not_used\n64:   )\n65:   ```\n66:\n67:## Key Features\n68:\n69:1. **Pre-built Tools**\n70:   - Built-in tools for:\n71:     - Powershell execution\n72:     - Python execution\n73:   - Tool state preservation: When tools are added to a bot and the bot is saved, all tool context\n74:     (including source code and dependencies) is preserved, allowing the bot to be shared with others\n75:     who can use the same tools without additional setup (beyond installing this and any other required\n76:     libraries for the tools).\n77:\n78:2. **Functional Prompts**\n79:   - A functional prompt is function which sends a bot a prompt or sequence of prompts in a particular way.\n80:   - A 'chain' prompt, for example, sends an array of prompts one after the other:\n81:     ```python\n82:     import functional_prompts as fp\n83:     import bots\n84:     bot = bots.load('my_file.bot')\n85:     fp.chain(bot, [\"View the directory\", \"Read cli.py\", \"Make me a mermaid diagram of the program flow\"])\n86:     ```\n87:   - A 'branch' prompt is similar to a chain, but creates a new conversation branch for each prompt in the list.\n88:   - Parallel branching is easy with the par_branch fp. General parallelization is possible with the par_dispatch fp.\n89:   - Why \"functional prompt\" and not \"workflow?\" To emphasize that these are *composable*.\n90:   - See functional_prompts.py for all available functional prompts and functional_prompt_primer.md for more information.\n91:\n92:3. **CLI Interface**\n93:   The CLI interface provides an advanced interface for working with bots interactively. It allows you to\n94:   navigate through conversation history like a tree, moving up to previous points in the conversation and\n95:   exploring different branches. The CLI also features an autonomous mode where the bot will continue\n96:   working on a task until it completes (indicated by no further tool usage).\n97:\n98:   ```bash\n99:   python -m bots.dev.cli\n100:\n101:   or\n102:\n103:   python -m bots.dev.cli [filepath] #Loads bot at filepath\n104:   ```\n105:\n106:   Key capabilities:\n107:   - Navigate conversation history with /up, /down, /left, /right, /label, and /goto commands\n108:   - Enable autonomous operation with /auto command (sends 'ok' after each bot response until the bot doesn't use a tool)\n109:   - Control tool output visibility with /verbose and /quiet\n110:   - Save and load bots with /save and /load\n111:   - Run a functional prompt with /fp\n112:        - This starts a wizard to choose and set up a functional prompt\n113:        - Try parallel branching with 'par_branch_while' to make multiple files at the same time\n114:        - Try 'broadcast_to_leaves' afterward to debug each file at the same time\n115:\n116:4. **Lazy Decorator**\n117:   The Lazy Decorator enables *runtime code generation* using LLMs. When applied to a function or class,\n118:   it defers implementation until the first time that code is actually called. At runtime, the decorator\n119:   sends relevant context to the LLM and uses its response to create the implementation. The context\n120:   levels control how much information about your codebase is sent to help generate better implementations.\n121:\n122:   ```python\n123:   from bots import lazy\n124:   # Using the smart decorator for functions\n125:   @lazy(\"Sort using a divide-and-conquer approach with O(n log n) complexity\")\n126:   def mergesort(arr: list[int]) -> list[int]:\n127:       pass  # Will be implemented by LLM on first call\n128:\n129:   # Using the smart decorator for classes\n130:   @lazy(\"Implement a key-value store with LRU eviction policy\")\n131:   class Cache:\n132:       pass  # Will be implemented by LLM on first instantiation\n133:   ```\n134:   - Customizable context levels for better implementations:\n135:     - None: No additional context\n136:     - low: Minimal context (class or module)\n137:     - medium: Entire current file\n138:     - high: Current file plus interfaces of other files\n139:     - very high: All Python files in directory\n140:   - Saved bots can be used\n141:\n142:   ```python\n143:   # Using the decorator with all options\n144:   @lazy(\"Implement a key-value store with LRU eviction policy\", bot=bots.load('mybot.bot'), context='medium')\n145:   class Cache:\n146:       pass  # Will be implemented by LLM on first instantiation\n147:   ```\n148:\n149:## Installation\n150:\n151:```bash\n152:pip install git+https://github.com/benbuzz790/bots.git\n153:```\n154:\n155:## Quick Start\n156:\n157:1. Set up your API key as an environment variable:\n158:   - [OpenAI](https://platform.openai.com/docs/quickstart) or\n159:   - [Anthropic](https://docs.anthropic.com/en/docs/initial-setup#set-your-api-key)\n160:\n161:2. Basic Usage:\n162:```python\n163:from bots import AnthropicBot\n164:import bots.tools.code_tools as code_tools\n165:\n166:# Initialize and equip bot\n167:bot = AnthropicBot()\n168:bot.add_tools(code_tools)\n169:\n170:# Single response\n171:response = bot.respond(\"Please write a basic Flask app in app.py\")\n172:\n173:# Interactive mode\n174:bot.chat()\n175:```\n176:\n177:3. Save/Load Bot States:\n178:```python\n179:import bots\n180:from bots.flows import functional_prompts as fp\n181:# Create a bot with repository context\n182:bot = AnthropicBot()\n183:bot.add_tools(code_tools)\n184:fp.prompt_while(bot, \"Read and understand our repo structure\")\n185:bot.save(\"repo.bot\")\n186:\n187:# Later use\n188:import bots\n189:review_bot = bots.load(\"repo.bot\", autosave=False)\n190:review_bot.respond(\"Review PR #123\")\n191:```\n192:\n193:4. Functional Patterns:\n194:```python\n195:import bots.flows.functional_prompts as fp\n196:\n197:# Chain of thought\n198:responses, nodes = fp.chain(bot, [\n199:    \"Look at the directory\",\n200:    \"Read x, y, z\",\n201:    \"Refactor these three files to better separate concerns\"\n202:])\n203:\n204:# Parallel exploration\n205:analyses, nodes = fp.branch(bot, [\n206:    \"Security analysis\",\n207:    \"Performance analysis\",\n208:    \"Usability analysis\"\n209:])\n210:\n211:# Iterative refinement\n212:fp.prompt_while(\n213:    bot,\n214:    \"Run tests and create issues\",\n215:    continue_prompt=\"ok\",\n216:    stop_condition=fp.conditions.tool_not_used\n217:)\n218:```\n219:\n220:## Tool Development\n221:\n222:Tools must follow these specific requirements for reliability and compatibility:\n223:\n224:1. **Function Requirements**\n225:   - If using a module or file:\n226:      - Tools must be top-level functions (not nested in classes or other functions)\n227:      - Tools must not start with an underscore (all \"_functions()\" are stored with the bot, but not made available as tools)\n228:   - Should prefer string inputs\n229:   - Must return a string\n230:   - Must catch all errors and return error messages as strings\n231:\n232:2. **Documentation Requirements**\n233:   - Clear docstring describing what the tool does\n234:   - Explicit \"Use when...\" section explaining when to use the tool\n235:   - All parameters documented with types and descriptions\n236:   - Return format clearly specified\n237:\n238:Example of a well-structured tool:\n239:\n240:```python\n241:def lint_code(file_path: str) -> str:\n242:    \"\"\"Clean code with built in linter.\n243:\n244:    Use when you need to clean up your files\n245:\n246:    Parameters:\n247:    - file_path (str): Path to the Python file to analyze\n248:\n249:    Returns:\n250:    str: Analysis results or error message\n251:\n252:    cost: medium\n253:    \"\"\"\n254:    try:\n255:        # Perform analysis\n256:        result = _lint(file_path)\n257:        return result\n258:    except Exception as e:\n259:        return f'Error analyzing {file_path}: {str(e)}'\n260:```\n261:\n262:3. **Best Practices**\n263:   - Keep tools focused and single-purpose\n264:   - Handle all edge cases gracefully\n265:   - Return helpful error messages\n266:\n267:## Contributing\n268:\n269:We welcome contributions!\n270:\n271:## License\n272:\n273:This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.\n274:\n275:## Disclaimer\n276:\n277:This library is under active development. While it's being used successfully in various projects, please test thoroughly before using in production environments."
         },
         {
          "type": "tool_result",
          "tool_use_id": "toolu_012XZ1wpCQhzsirfHxPHXtb7",
          "content": "Tool Failed: File not found: C:\\Users\\benbu\\Code\\repo_working\\bots\\functional_prompts.py\nTraceback:\n"
         },
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01GkKNmg3eEj2D93gi1ChpFm",
          "content": "Tool Failed: File not found: C:\\Users\\benbu\\Code\\repo_working\\bots\\base.py\nTraceback:\n"
         },
         {
          "type": "tool_result",
          "tool_use_id": "toolu_01XdGxmn5KaZH9qc2j5XEXQV",
          "content": "Tool Failed: File not found: C:\\Users\\benbu\\Code\\repo_working\\bots\\cli.py\nTraceback:\n"
         }
        ],
        "node_class": "AnthropicNode",
        "replies": [
         {
          "content": "I see that some of the files weren't found with those exact names. Let me check the directory structure to find the correct paths:",
          "pending_results": [],
          "role": "assistant",
          "tool_calls": [
           {
            "type": "tool_use",
            "id": "toolu_01NTRnnBMj5nqbqLXcjMSqh9",
            "name": "view_dir",
            "input": {
             "start_path": ".",
             "target_extensions": "[\"py\", \"md\"]",
             "max_lines": "100"
            }
           }
          ],
          "tool_results": [],
          "node_class": "AnthropicNode",
          "replies": [
           {
            "content": "ok",
            "pending_results": [],
            "role": "user",
            "tool_calls": [],
            "tool_results": [
             {
              "type": "tool_result",
              "tool_use_id": "toolu_01NTRnnBMj5nqbqLXcjMSqh9",
              "content": "./\n    bot_gui.py\n    CHANGELOG.md\n    CLAUDE.md\n    CLI_PRIMER.md\n    conftest.py\n    CONTRIBUTING.md\n    functional_prompt_primer.md\n    installation_quickstart.md\n    README.md\n    ROADMAP.md\n    setup.py\n    .github/\n        ...\n    .pytest_cache/\n        ...\n    archived_docker_testing/\n        ...\n    bots/\n        ...\n    docs/\n        ...\n    examples/\n        ...\n    tests/\n        ...\n    trash/\n        ...\n    tutorials/\n        ...\n    _work_orders/\n        ...\n    __fix_bots/\n        ..."
             }
            ],
            "node_class": "AnthropicNode",
            "replies": [
             {
              "content": "~",
              "pending_results": [],
              "role": "assistant",
              "tool_calls": [
               {
                "type": "tool_use",
                "id": "toolu_01ASRvjMBK4jUQU99H4QKuAm",
                "name": "view_dir",
                "input": {
                 "start_path": "bots",
                 "target_extensions": "[\"py\"]",
                 "max_lines": "100"
                }
               }
              ],
              "tool_results": [],
              "node_class": "AnthropicNode",
              "replies": [
               {
                "content": "ok",
                "pending_results": [],
                "role": "user",
                "tool_calls": [],
                "tool_results": [
                 {
                  "type": "tool_result",
                  "tool_use_id": "toolu_01ASRvjMBK4jUQU99H4QKuAm",
                  "content": "\n        bots/\n            __init__.py\n            dev/\n                cli.py\n                decorators.py\n                pr_comment_parser.py\n                remove_boms.py\n                start.py\n                __init__.py\n            flows/\n                functional_prompts.py\n                recombinators.py\n                __init__.py\n                project_tree/\n                    project_tree.py\n                    project_tree_prompts.py\n                    project_tree_tools.py\n                    spec_builder.py\n                    __init__.py\n            foundation/\n                anthropic_bots.py\n                base.py\n                gemini_bots.py\n                openai_bots.py\n                __init__.py\n            testing/\n                mock_bot.py\n                __init__.py\n            tools/\n                code_tools.py\n                meta_tools.py\n                python_edit.py\n                python_editing_tools.py\n                python_execution_tool.py\n                self_tools.py\n                terminal_tools.py\n                web_tool.py\n                __init__.py\n            utils/\n                compile_bot_errors.py\n                helpers.py\n                logging.py\n                unicode_utils.py\n                __init__.py"
                 }
                ],
                "node_class": "AnthropicNode",
                "replies": [
                 {
                  "content": "Now let me view the correct files:",
                  "pending_results": [],
                  "role": "assistant",
                  "tool_calls": [
                   {
                    "type": "tool_use",
                    "id": "toolu_01HR6RhVtTD9UU1GJwVYwWBV",
                    "name": "python_view",
                    "input": {
                     "target_scope": "bots/flows/functional_prompts.py",
                     "max_lines": "150"
                    }
                   },
                   {
                    "type": "tool_use",
                    "id": "toolu_01SQMa21Q5i3dS5NFaz5AXCF",
                    "name": "python_view",
                    "input": {
                     "target_scope": "bots/foundation/base.py",
                     "max_lines": "150"
                    }
                   },
                   {
                    "type": "tool_use",
                    "id": "toolu_01BEHYRB52R7isngbmnK1Pt1",
                    "name": "python_view",
                    "input": {
                     "target_scope": "bots/dev/cli.py",
                     "max_lines": "150"
                    }
                   }
                  ],
                  "tool_results": [],
                  "node_class": "AnthropicNode",
                  "replies": [
                   {
                    "content": "ok",
                    "pending_results": [],
                    "role": "user",
                    "tool_calls": [],
                    "tool_results": [
                     {
                      "type": "tool_result",
                      "tool_use_id": "toolu_01HR6RhVtTD9UU1GJwVYwWBV",
                      "content": "\n        \"\"\"Functional patterns for structured bot interactions and complex reasoning.\n        \n        This module provides a collection of higher-level functions for orchestrating\n        interactions in common patterns like chains, branches, and trees. These patterns\n        enable sophisticated reasoning approaches while maintaining clear structure and\n        reproducibility.\n        \n        Key Features:\n        - Sequential Processing:\n            - chain(): Execute prompts in sequence\n            - chain_while(): Chain with iteration until condition met\n            - prompt_while(): Single prompt with iteration\n        \n        - Parallel Exploration:\n            - branch(): Explore multiple paths independently\n            - branch_while(): Branch with iteration\n            - par_branch(): Parallel version of branch()\n            - par_branch_while(): Parallel version of branch_while()\n        \n        - Advanced Reasoning:\n            - tree_of_thought(): Branch, explore, then synthesize\n            - prompt_for(): Dynamic prompts from data\n            - par_dispatch(): Compare multiple bots\n        \n        Common Parameters:\n        - bot (Bot): The bot instance to use\n        - prompts (List[Prompt]): List of prompts to process\n        - stop_condition (Condition): Function determining when to stop iteration\n        \n        Common Return Values:\n        - Response: String containing bot's response\n        - ResponseNode: ConversationNode containing response and context\n        \n        Example:\n            >>> responses, nodes = chain(bot, [\n            ...     \"First, analyze the problem...\",\n            ...     \"Now, propose solutions...\",\n            ...     \"Finally, implement the best solution...\"\n            ... ])\n        \"\"\"\n        \n        import os\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n        from typing import Any, Callable, List, Optional, Tuple, Union\n        \n        from bots.foundation.base import Bot, ConversationNode\n        \n        # Type Aliases\n        Prompt = str  # A string containing a prompt to be sent to the bot\n        Response = str  # A string containing a bot's response\n        PromptNode = ConversationNode  # A conversation node containing a prompt\n        ResponseNode = ConversationNode  # A conversation node containing a response\n        Condition = Callable[[Bot], bool]  # Function that evaluates bot state\n        DynamicPrompt = Callable[[Any], Prompt]  # Function that generates prompts\n        \n        RecombinatorFunction = Callable[\n            [List[Response], List[ResponseNode]], Tuple[Response, ResponseNode]\n        ]  # A function that combines multiple responses into a single response\n        \n        FunctionalPrompt = Callable[\n            [Bot, Any], Union[Tuple[Response, ResponseNode], Tuple[List[Response], List[ResponseNode]]]\n        ]  # A function that acts on a Bot and returns a response or set of responses in the form of a tuple.\n        \n        \n        class conditions:\n            \"\"\"Predefined condition functions for controlling bot iteration.\n        \n            This class provides a collection of static methods that can be used as\n            stop conditions in functions like prompt_while() and chain_while(). Each\n            condition evaluates some aspect of the bot's state to determine whether\n            iteration should continue.\n        \n            Attributes:\n                No instance attributes - all methods are static\n        \n            Common Usage:\n                >>> # Continue until bot stops using tools\n                >>> prompt_while(bot, prompt, stop_condition=conditions.tool_not_used)\n                >>>\n                >>> # Continue until bot says \"DONE\"\n                >>> chain_while(bot, prompts, stop_condition=conditions.said_DONE)\n        \n            Note:\n                Each condition function takes a Bot instance and returns a boolean.\n                True typically indicates iteration should stop.\n            \"\"\"\n        \n            @staticmethod\n            def said_READY(bot: Bot) -> bool:\n                ...\n        \n            @staticmethod\n            def five_iterations(bot: Bot):\n                ...\n            @staticmethod\n            def no_new_tools_used(bot: Bot) -> bool:\n                ...\n        \n            @staticmethod\n            def error_in_response(bot: Bot) -> bool:\n                ...\n        \n            def tool_used(bot: Bot) -> bool:\n                ...\n        \n            @staticmethod\n            def tool_not_used(bot: Bot) -> bool:\n                ...\n        \n            @staticmethod\n            def said_DONE(bot: Bot) -> bool:\n                ...\n        \n        \n        def single_prompt(bot: Bot, prompt: Prompt) -> Tuple[Response, ResponseNode]:\n            \"\"\"Execute a single prompt and return both response and conversation state.\n        \n            Use when you need:\n            - The simplest possible bot interaction\n            - To capture both response and conversation state\n            - A building block for custom interaction patterns\n            - To understand the fundamental bot interaction model\n        \n            This function serves as:\n            1. A demonstration of the basic bot interaction pattern\n            2. A template for more complex functional patterns\n            3. A utility for simple, one-shot interactions\n            4. A way to understand conversation node handling\n        \n            Args:\n                bot (Bot): The bot to interact with. The bot's current conversation\n                    state is preserved and updated with this interaction\n                prompt (Prompt): The prompt to send to the bot. Should be a clear,\n                    self-contained instruction or question\n        \n            Returns:\n                Tuple[Response, ResponseNode]: A tuple containing:\n                    - response (str): The bot's response text\n                    - node (ConversationNode): Conversation node containing:\n                        - The response\n                        - The prompt that generated it\n                        - Links to parent/child nodes\n                        - Associated metadata\n        \n            Examples:\n                >>> # Simple question and answer\n                >>> response, node = basic(bot,\n                ...     \"What is the time complexity of quicksort?\"\n                ... )\n                >>> print(response)"
                     },
                     {
                      "type": "tool_result",
                      "tool_use_id": "toolu_01SQMa21Q5i3dS5NFaz5AXCF",
                      "content": "\n        import ast\n        import copy\n        import hashlib\n        import importlib\n        import inspect\n        import json\n        import os\n        import re\n        import sys\n        import textwrap\n        import types\n        from abc import ABC, abstractmethod\n        from dataclasses import dataclass\n        from enum import Enum\n        from types import ModuleType\n        from typing import Any, Callable, Dict, List, Optional, Tuple, Type\n        \n        from bots.utils.helpers import _py_ast_to_source, formatted_datetime\n        \n        \"\"\"Core foundation classes for the bots framework.\n        This module provides the fundamental abstractions and base classes that power the bots framework:\n        - Bot: Abstract base class for all LLM implementations\n        - ToolHandler: Manages function/module tools with context preservation\n        - ConversationNode: Tree-based conversation storage\n        - Mailbox: Abstract interface for LLM service communication\n        - Engines: Supported LLM model configurations\n        The classes in this module are designed to:\n        - Provide a consistent interface across different LLM implementations\n        - Enable sophisticated context and tool management\n        - Support complete bot portability and state preservation\n        - Handle conversation branching and context management efficiently\n        Example:\n            `python\n            from bots import AnthropicBot\n            import my_tools\n            # Create a bot with tools\n            bot = AnthropicBot()\n            bot.add_tools(my_tools)\n            # Basic interaction\n            response = bot.respond(\"Hello!\")\n            # Save bot state\n            bot.save(\"my_bot.bot\")\n            `\n        \"\"\"\n        \n        \n        def load(filepath: str) -> \"Bot\":\n            \"\"\"Load a saved bot from a file.\n        \n            Use when you need to restore a previously saved bot with its complete state,\n            including conversation history, tools, and configuration.\n        \n            Parameters:\n                filepath (str): Path to the .bot file containing the saved bot state\n        \n            Returns:\n                Bot: A reconstructed Bot instance with the saved state\n        \n            Example:\n                ```python\n                bot = bots.load(\"my_saved_bot.bot\")\n                bot.respond(\"Continue our previous conversation\")\n                ```\n            \"\"\"\n            return Bot.load(filepath)\n        \n        \n        class Engines(str, Enum):\n            \"\"\"Enum class representing different AI model engines.\"\"\"\n        \n            GPT4 = \"gpt-4\"\n            GPT41 = \"gpt-4.1\"\n            GPT4_0613 = \"gpt-4-0613\"\n            GPT4_32K = \"gpt-4-32k\"\n            GPT4_32K_0613 = \"gpt-4-32k-0613\"\n            GPT35TURBO = \"gpt-3.5-turbo\"\n            GPT35TURBO_16K = \"gpt-3.5-turbo-16k\"\n            GPT35TURBO_0125 = \"gpt-3.5-turbo-0125\"\n            GPT35TURBO_INSTRUCT = \"gpt-3.5-turbo-instruct\"\n            CLAUDE3_HAIKU = \"claude-3-haiku-20240307\"\n            CLAUDE35_HAIKU = \"claude-3-5-haiku-latest\"\n            CLAUDE37_SONNET_20250219 = \"claude-3-7-sonnet-latest\"\n            CLAUDE4_OPUS = \"claude-opus-4-20250514\"\n            CLAUDE41_OPUS = \"claude-opus-4-1\"\n            CLAUDE4_SONNET = \"claude-sonnet-4-20250514\"\n            CLAUDE45_SONNET = \"claude-sonnet-4-5-20250929\"\n            GEMINI25_FLASH = \"gemini-2.5-flash\"\n        \n            @staticmethod\n            def get(name: str) -> Optional[\"Engines\"]:\n                ...\n        \n            @staticmethod\n            def get_bot_class(model_engine: \"Engines\") -> Type[\"Bot\"]:\n                ...\n        \n            @staticmethod\n            def get_conversation_node_class(class_name: str) -> Type[\"ConversationNode\"]:\n                ...\n        \n        \n        class ConversationNode:\n            \"\"\"Tree-based storage for conversation history and tool interactions.\n        \n            ConversationNode implements a linked tree structure that enables sophisticated\n            conversation management, including branching conversations and tool usage tracking.\n            Each node represents a message in the conversation and can have multiple replies,\n            forming a tree structure.\n        \n            Attributes:\n                role (str): The role of the message sender ('user', 'assistant', etc.)\n                content (str): The message content\n                parent (ConversationNode): Reference to the parent node\n                replies (List[ConversationNode]): List of reply nodes\n                tool_calls (List[Dict]): Tool invocations made in this message\n                tool_results (List[Dict]): Results from tool executions\n                pending_results (List[Dict]): Tool results waiting to be processed\n        \n            Example:\n                ```python\n                # Create a conversation tree\n                root = ConversationNode(role='user', content='Hello')\n                response = root._add_reply(role='assistant', content='Hi there!')\n                ```\n            \"\"\"\n        \n            def __init__(\n                ...\n        \n            @property\n            def tool_results(self):\n                ...\n        \n            @tool_results.setter\n            def tool_results(self, value):\n                ...\n        \n            @staticmethod\n            def _create_empty(cls: Optional[Type[\"ConversationNode\"]] = None) -> \"ConversationNode\":\n                ...\n        \n            def _is_empty(self) -> bool:\n                ...\n        \n            def _add_reply(self, **kwargs) -> \"ConversationNode\":\n                ...\n        \n            def _sync_tool_context(self) -> None:\n                ..."
                     },
                     {
                      "type": "tool_result",
                      "tool_use_id": "toolu_01BEHYRB52R7isngbmnK1Pt1",
                      "content": "\n        import argparse\n        import inspect\n        import json\n        import os\n        import platform\n        import re\n        import sys\n        import textwrap\n        from pathlib import Path\n        from typing import Any, Callable, Dict, List, Optional\n        \n        # Try to import readline, with fallback for Windows\n        try:\n            import readline\n        \n            HAS_READLINE = True\n        except ImportError:\n            HAS_READLINE = False\n        import bots.flows.functional_prompts as fp\n        import bots.flows.recombinators as recombinators\n        from bots.foundation.anthropic_bots import AnthropicBot\n        from bots.foundation.base import Bot, ConversationNode\n        \n        \"\"\"\n        CLI for bot interactions with improved architecture and dynamic parameter collection.\n        Architecture:\n        - Handler classes for logical command grouping\n        - Command registry for easy extensibility\n        - Dynamic parameter collection for functional prompts\n        - Robust error handling with conversation backup\n        - Configuration support for CLI settings\n        \"\"\"\n        if platform.system() == \"Windows\":\n            import msvcrt\n        else:\n            import select\n            import termios\n            import tty\n        COLOR_USER = \"\\033[36m\"  # Cyan\n        COLOR_ASSISTANT = \"\\033[36m\"  # Cyan\n        COLOR_SYSTEM = \"\\033[33m\"  # Yellow\n        COLOR_ERROR = \"\\033[31m\"  # Red\n        COLOR_RESET = \"\\033[0m\"  # Reset\n        COLOR_BOLD = \"\\033[1m\"  # Bold\n        COLOR_DIM = \"\\033[2m\"  # Dim\n        COLOR_TOOL_REQUEST = \"\\033[34m\"  # Blue\n        COLOR_TOOL_RESULT = \"\\033[32m\"  # Green\n        \n        \n        def create_auto_stash() -> str:\n            \"\"\"Create an automatic git stash with AI-generated message based on current diff.\"\"\"\n            import subprocess\n        \n            from bots.foundation.base import Engines\n        \n            try:\n                # Check for staged changes first\n                staged_diff_result = subprocess.run([\"git\", \"diff\", \"--cached\"], capture_output=True, text=True, timeout=10)\n                # Check for unstaged changes\n                unstaged_diff_result = subprocess.run([\"git\", \"diff\"], capture_output=True, text=True, timeout=10)\n                if staged_diff_result.returncode != 0 or unstaged_diff_result.returncode != 0:\n                    return f\"Error getting git diff: {staged_diff_result.stderr or unstaged_diff_result.stderr}\"\n                staged_diff = staged_diff_result.stdout.strip()\n                unstaged_diff = unstaged_diff_result.stdout.strip()\n                # Combine both diffs for analysis, but prefer staged if available\n                diff_content = staged_diff if staged_diff else unstaged_diff\n                if not diff_content:\n                    return \"No changes to stash\"\n                # Generate stash message using Haiku\n                stash_message = \"WIP: auto-stash before user message\"  # fallback\n                try:\n                    # Create a Haiku bot instance - use the module-level import\n                    haiku_bot = AnthropicBot(model_engine=Engines.CLAUDE3_HAIKU, max_tokens=100)\n                    # Create a prompt for generating the stash message\n                    prompt = (\n                        f\"Based on this git diff, generate a concise commit-style message \"\n                        f\"(under 50 chars) describing the changes. Start with 'WIP: ' if not already present:\\n\\n\"\n                        f\"{diff_content}\\n\\nRespond with just the message, nothing else.\"\n                    )\n                    ai_message = haiku_bot.respond(prompt)\n                    if ai_message and ai_message.strip():\n                        ai_message = ai_message.strip()\n                        if not ai_message.startswith(\"WIP:\"):\n                            ai_message = f\"WIP: {ai_message}\"\n                        if len(ai_message) <= 72:  # Git recommended limit\n                            stash_message = ai_message\n                except Exception:\n                    # Use fallback message if AI generation fails\n                    pass\n                # Create the stash - this will stash both staged and unstaged changes\n                stash_result = subprocess.run(\n                    [\"git\", \"stash\", \"push\", \"-m\", stash_message],\n                    capture_output=True,\n                    text=True,\n                    timeout=10,\n                )\n                if stash_result.returncode != 0:\n                    return f\"Error creating stash: {stash_result.stderr}\"\n                return f\"Auto-stash created: {stash_message}\"\n            except subprocess.TimeoutExpired:\n                return \"Git command timed out\"\n            except Exception as e:\n                return f\"Error in auto-stash: {str(e)}\"\n        \n        \n        class DynamicParameterCollector:\n            \"\"\"Dynamically collect parameters for functional prompts based on their signatures.\"\"\"\n        \n            def __init__(self, function_filter: Optional[Callable[[str, Callable], bool]] = None):\n                ...\n        \n            def _format_default_value(self, default: Any) -> str:\n                ...\n        \n            def collect_parameters(self, func: Callable) -> Optional[Dict[str, Any]]:\n                ...\n        \n            def _collect_prompts(self, param_name: str, default: Any) -> Optional[List[str]]:\n                ...\n        \n            def _collect_single_prompt(self, param_name: str, default: Any) -> Optional[str]:\n                ...\n        \n            def _collect_condition(self, param_name: str, default: Any) -> Optional[Callable]:\n                ...\n        \n            def _collect_continue_prompt(self, param_name: str, default: Any) -> Optional[str]:\n                ...\n        \n            def _collect_boolean(self, param_name: str, default: Any) -> Optional[bool]:\n                ...\n        \n            def _collect_skip_labels(self, param_name: str, default: Any) -> List[str]:\n                ...\n        \n            def _collect_recombinator(self, param_name: str, default: Any) -> Optional[Callable]:\n                ...\n        \n            def _collect_items(self, param_name: str, default: Any) -> Optional[List[Any]]:\n                ...\n        \n            def _collect_dynamic_prompt(self, param_name: str, default: Any) -> Optional[Callable]:\n                ...\n        \n            def _collect_functional_prompt(self, param_name: str, default: Any) -> Optional[Callable]:\n                ...\n        \n            def _collect_generic_parameter(self, param_name: str, default: Any) -> Optional[Any]:\n                ..."
                     }
                    ],
                    "node_class": "AnthropicNode",
                    "replies": [
                     {
                      "content": "Great! I've now reviewed all the requested files. Here's a summary of what I've learned:\n\n## Project Overview\n\n**bots** is an agentic programming framework that provides:\n\n1. **Core Abstraction (base.py)**:\n   - `Bot` class - unified interface for different LLM providers (Anthropic, OpenAI, Gemini)\n   - `ConversationNode` - tree-based conversation history allowing branching\n   - `ToolHandler` - manages function/module tools with context preservation\n   - Complete bot state saving/loading including tools and conversation history\n\n2. **Functional Prompts (functional_prompts.py)**:\n   - Composable patterns for complex bot interactions\n   - Sequential: `chain()`, `chain_while()`, `prompt_while()`\n   - Parallel: `branch()`, `par_branch()`, `par_branch_while()`\n   - Advanced: `tree_of_thought()`, `prompt_for()`, `par_dispatch()`\n   - Predefined `conditions` class for iteration control\n\n3. **CLI Interface (cli.py)**:\n   - Interactive terminal interface for bot conversations\n   - Tree navigation commands (`/up`, `/down`, `/left`, `/right`, `/goto`)\n   - Autonomous mode (`/auto`) - continues until bot stops using tools\n   - Functional prompt wizard (`/fp`)\n   - Auto-stash feature with AI-generated commit messages\n   - Dynamic parameter collection for functional prompts\n\n## Key Insights from ROADMAP.md\n\nThe project is focused on:\n- **Phase 1**: Standards integration (MCP, OpenTelemetry), branch protection, test organization\n- **Philosophy**: \"Standards over frameworks\" - integrate with industry standards rather than building everything from scratch\n- **MCP Integration**: High priority to connect to MCP ecosystem (both as client and server)\n- **OpenTelemetry**: Replace print statements with proper observability\n- **Production-ready**: Focus on quality guardrails, CI/CD, and proper testing\n\nIs there something specific you'd like me to help with in this project?",
                      "pending_results": [],
                      "role": "assistant",
                      "tool_calls": [],
                      "tool_results": [],
                      "node_class": "AnthropicNode"
                     }
                    ]
                   }
                  ]
                 }
                ]
               }
              ]
             }
            ]
           }
          ]
         }
        ]
       }
      ]
     }
    ]
   }
  ]
 },
 "system_message": "You're a coding agent. When greeting users, always start with \"Hello! I'm here to help you\".\n            Please follow these rules:\n                    1. Keep edits and even writing new files to small chunks. You have a low max_token limit\n                       and will hit tool errors if you try making too big of a change.\n                    2. Avoid using cd. Your terminal is stateful and will remember if you use cd.\n                       Instead, use full relative paths.\n",
 "tool_handler": {
  "class": "bots.foundation.anthropic_bots.AnthropicToolHandler",
  "tools": [
   {
    "name": "view",
    "description": "Display the contents of a file with line numbers.\nParameters:\n- file_path (str): The path to the file to be viewed.\n- start_line (int, optional): Starting line number (1-indexed). If None, starts from beginning.\n- end_line (int, optional): Ending line number (1-indexed). If None, goes to end of file.\n- around_str_match (str, optional): String to search for. If provided, shows lines around matches.\n- dist_from_match (int, optional): Number of lines to show before and after each match. Defaults to 10.\nReturns:\nA string containing the file contents with line numbers,\nfiltered according to the specified parameters.",
    "input_schema": {
     "type": "object",
     "properties": {
      "file_path": {
       "type": "string"
      },
      "start_line": {
       "type": "string"
      },
      "end_line": {
       "type": "string"
      },
      "around_str_match": {
       "type": "string"
      },
      "dist_from_match": {
       "type": "string"
      }
     },
     "required": [
      "file_path"
     ]
    }
   },
   {
    "name": "view_dir",
    "description": "Creates a summary of the directory structure starting from the given path, writing only files\nwith specified extensions and showing venv directories without their contents.\nParameters:\n- start_path (str): The root directory to start scanning from.\n- output_file (str): The name of the file to optionally write the directory structure to.\n- target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n- max_lines (int): Maximum number of lines in output. If exceeded, deepest levels are truncated. Default 500.\nReturns:\nstr: A formatted string containing the directory structure, with each directory and file properly indented.\nExample output:\nmy_project/\n    venv/\n    module1/\n        script.py\n        README.md\n    module2/\n        utils.py\ncost: low",
    "input_schema": {
     "type": "object",
     "properties": {
      "start_path": {
       "type": "string"
      },
      "output_file": {
       "type": "string"
      },
      "target_extensions": {
       "type": "string"
      },
      "max_lines": {
       "type": "string"
      }
     },
     "required": []
    }
   },
   {
    "name": "execute_powershell",
    "description": "Executes PowerShell commands in a stateful environment with automatic BOM removal\n\nUse when you need to run PowerShell commands and capture their output. If\nyou have other tools available, you should use this as a fallback when the\nother tools fail. Coerces to utf-8 (no BOM).\n\nPotential use cases:\n- git commands\n- gh cli\n- other cli (which you may need to install using this tool)\n\nParameters:\n- command (str): PowerShell command to execute.\n- output_length_limit (int, optional): Maximum number of lines in the output.\n  If set, output exceeding this limit will be truncated. Default 200.\n\nReturns:\n    str: The complete output from the command execution",
    "input_schema": {
     "type": "object",
     "properties": {
      "command": {
       "type": "string"
      },
      "output_length_limit": {
       "type": "string"
      },
      "timeout": {
       "type": "string"
      }
     },
     "required": [
      "command"
     ]
    }
   },
   {
    "name": "execute_python",
    "description": "Executes python code in a stateless environment with cross-platform\ntimeout handling.\nParameters:\n- code (str): Syntactically correct python code\n- timeout (int): Maximum execution time in seconds (default: 300)\nReturns stdout or an error message.\ncost: varies",
    "input_schema": {
     "type": "object",
     "properties": {
      "code": {
       "type": "string"
      },
      "timeout": {
       "type": "string"
      }
     },
     "required": [
      "code"
     ]
    }
   },
   {
    "name": "branch_self",
    "description": "Create multiple conversation branches to explore different approaches or tackle separate tasks.\n\nSimplified approach that creates isolated conversation branches:\n1. Save current bot state to temporary file\n2. Execute each branch with independent bot instances\n3. Stitch results back into main conversation tree\nArgs:\n    self_prompts (str): List of prompts as a string array, like ['task 1', 'task 2', 'task 3']\n                       Each prompt becomes a separate conversation branch\n    allow_work (str): 'True' to let each branch use tools and continue working until done\n                     'False' (default) for single-response branches\n    parallel (str): 'True' to let branches work in parallel, 'False' for sequential (default).\n    recombine (str): One of ('none', 'concatenate', 'llm_merge', 'llm_vote', 'llm_judge'), default 'concatenate'.\n                     Combines the final messages from each branch using that method.\n\nReturns:\n    str: Success message with branch count, or error details if something went wrong",
    "input_schema": {
     "type": "object",
     "properties": {
      "self_prompts": {
       "type": "string"
      },
      "allow_work": {
       "type": "string"
      },
      "parallel": {
       "type": "string"
      },
      "recombine": {
       "type": "string"
      }
     },
     "required": [
      "self_prompts"
     ]
    }
   },
   {
    "name": "web_search",
    "description": "Perform an agentic web search using Claude's internal web search capabilities",
    "input_schema": {
     "type": "object",
     "properties": {
      "question": {
       "type": "string"
      }
     },
     "required": [
      "question"
     ]
    }
   },
   {
    "name": "python_edit",
    "description": "    Edit Python code using pytest-style scope syntax and optional expression matching.\n\n    Parameters:\n    -----------\n    target_scope : str\n        Location to edit in pytest-style scope syntax:\n        - \"file.py\" (whole file)\n        - \"file.py::MyClass\" (class)\n        - \"file.py::my_function\" (function)\n        - \"file.py::MyClass::method\" (method)\n\n    code : str\n        Python code. This code will replace the entire target scope by default,\n        or insert after (if coscope_with is specified). Code is indented automatically\n        to match to scope.\n\n    coscope_with : str, optional\n        If specified, code is inserted in the same scope as the specified code,\n        and immediately after it. Uses the same syntax as target_scope with\n        optional expression at the end in quotes.\n\n\n            Scope:\n                - a\n                - b\n\n            - python edit called with coscope_with = \"a\", code = \"- c\" -\n\n            Scope:\n                - a\n                - c\n                - b\n\n        - \"__FILE_START__\" (special token for file beginning)\n        - \"MyClass::method\" (insert after this method within the target scope)\n        - '\"expression\"' (insert after a line matching this expression)\n    delete_a_lot : bool, optional\n- \"__FILE_END__\" (special token for file end)\n        Safety parameter. Must be True to allow operations that delete more than 100 lines.\n        Helps prevent accidental file overwrites. Default False.\n\n    Returns:\n    --------\n    str\n        Description of what was modified or error message",
    "input_schema": {
     "type": "object",
     "properties": {
      "target_scope": {
       "type": "string"
      },
      "code": {
       "type": "string"
      },
      "coscope_with": {
       "type": "string"
      },
      "delete_a_lot": {
       "type": "string"
      }
     },
     "required": [
      "target_scope",
      "code"
     ]
    }
   },
   {
    "name": "python_view",
    "description": "View Python code using pytest-style scope syntax with scope-aware truncation.\n\nParameters:\n-----------\ntarget_scope : str\n    Location to view in pytest-style scope syntax:\n    - \"file.py\" (whole file)\n    - \"file.py::MyClass\" (class)\n    - \"file.py::my_function\" (function)\n    - \"file.py::MyClass::method\" (method)\nmax_lines : int, optional\n    Maximum number of lines in output. If exceeded, deeper scopes will be\n    progressively truncated to create an outline view. Default 500.\n\nReturns:\n--------\nstr\n    The source code at the specified scope, or error message",
    "input_schema": {
     "type": "object",
     "properties": {
      "target_scope": {
       "type": "string"
      },
      "max_lines": {
       "type": "string"
      }
     },
     "required": [
      "target_scope"
     ]
    },
    "cache_control": {
     "type": "ephemeral"
    }
   }
  ],
  "requests": [],
  "results": [],
  "modules": {
   "dynamic_module_5245363458925641105": {
    "name": "dynamic_module_315465908699359385",
    "source": "from bots.utils.helpers import _process_error\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef view(file_path: str, start_line: str=None, end_line: str=None, around_str_match: str=None, dist_from_match: str='10'):\n    \"\"\"\n    Display the contents of a file with line numbers.\n    Parameters:\n    - file_path (str): The path to the file to be viewed.\n    - start_line (int, optional): Starting line number (1-indexed). If None, starts from beginning.\n    - end_line (int, optional): Ending line number (1-indexed). If None, goes to end of file.\n    - around_str_match (str, optional): String to search for. If provided, shows lines around matches.\n    - dist_from_match (int, optional): Number of lines to show before and after each match. Defaults to 10.\n    Returns:\n    A string containing the file contents with line numbers,\n    filtered according to the specified parameters.\n    \"\"\"\n    encodings = ['utf-8', 'utf-16', 'utf-16le', 'ascii', 'cp1252', 'iso-8859-1']\n    start_line = int(start_line) if start_line is not None else None\n    end_line = int(end_line) if end_line is not None else None\n    dist_from_match = int(dist_from_match) if dist_from_match is not None else None\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as file:\n                lines = file.readlines()\n            if around_str_match:\n                matching_line_numbers = []\n                for i, line in enumerate(lines):\n                    if around_str_match in line:\n                        matching_line_numbers.append(i)\n                if not matching_line_numbers:\n                    return f\"No matches found for '{around_str_match}'\"\n                line_ranges = set()\n                for match_line in matching_line_numbers:\n                    start = max(0, match_line - dist_from_match)\n                    end = min(len(lines), match_line + dist_from_match + 1)\n                    for line_num in range(start, end):\n                        line_ranges.add(line_num)\n                sorted_lines = sorted(line_ranges)\n                numbered_lines = []\n                prev_line = None\n                for line_num in sorted_lines:\n                    if prev_line is not None and line_num > prev_line + 1:\n                        numbered_lines.append('...')\n                    numbered_lines.append(f'{line_num + 1}:{lines[line_num].rstrip()}')\n                    prev_line = line_num\n                return '\\n'.join(numbered_lines)\n            else:\n                start_idx = start_line - 1 if start_line else 0\n                end_idx = end_line if end_line else len(lines)\n                start_idx = max(0, start_idx)\n                end_idx = min(len(lines), end_idx)\n                if start_idx >= len(lines):\n                    return f'Error: start_line ({start_line}) exceeds file length ({len(lines)} lines)'\n                selected_lines = lines[start_idx:end_idx]\n                numbered_lines = [f'{i + start_idx + 1}:{line.rstrip()}' for i, line in enumerate(selected_lines)]\n                return '\\n'.join(numbered_lines)\n        except UnicodeDecodeError:\n            continue\n    return f\"Error: Unable to read file with any of the attempted encodings: {', '.join(encodings)}\"",
    "file_path": "dynamic_module_5245363458925641105",
    "code_hash": "1a69cab053a4e4dd873489585f35e953",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "difflib": {
      "__module_type__": true,
      "name": "difflib",
      "spec": "difflib"
     },
     "_read_file_bom_safe": {
      "__helper_func__": true,
      "pickled": "gASVMQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SME19yZWFkX2ZpbGVfYm9tX3NhZmWUk5Qu",
      "name": "_read_file_bom_safe"
     },
     "_write_file_bom_safe": {
      "__helper_func__": true,
      "pickled": "gASVMgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFF93cml0ZV9maWxlX2JvbV9zYWZllJOULg==",
      "name": "_write_file_bom_safe"
     },
     "_remove_bom_from_content": {
      "__helper_func__": true,
      "pickled": "gASVNgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGF9yZW1vdmVfYm9tX2Zyb21fY29udGVudJSTlC4=",
      "name": "_remove_bom_from_content"
     },
     "_find_match_with_hierarchy": {
      "__helper_func__": true,
      "pickled": "gASVOAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGl9maW5kX21hdGNoX3dpdGhfaGllcmFyY2h5lJOULg==",
      "name": "_find_match_with_hierarchy"
     },
     "_check_exact_match_at_position": {
      "__helper_func__": true,
      "pickled": "gASVPAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMHl9jaGVja19leGFjdF9tYXRjaF9hdF9wb3NpdGlvbpSTlC4=",
      "name": "_check_exact_match_at_position"
     },
     "_find_exact_match_anywhere": {
      "__helper_func__": true,
      "pickled": "gASVOAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGl9maW5kX2V4YWN0X21hdGNoX2FueXdoZXJllJOULg==",
      "name": "_find_exact_match_anywhere"
     },
     "_check_whitespace_match_at_position": {
      "__helper_func__": true,
      "pickled": "gASVQQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMI19jaGVja193aGl0ZXNwYWNlX21hdGNoX2F0X3Bvc2l0aW9ulJOULg==",
      "name": "_check_whitespace_match_at_position"
     },
     "_find_whitespace_match_anywhere": {
      "__helper_func__": true,
      "pickled": "gASVPQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMH19maW5kX3doaXRlc3BhY2VfbWF0Y2hfYW55d2hlcmWUk5Qu",
      "name": "_find_whitespace_match_anywhere"
     },
     "_adjust_additions_to_context": {
      "__helper_func__": true,
      "pickled": "gASVOgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMHF9hZGp1c3RfYWRkaXRpb25zX3RvX2NvbnRleHSUk5Qu",
      "name": "_adjust_additions_to_context"
     },
     "_find_block_in_content": {
      "__helper_func__": true,
      "pickled": "gASVNAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFl9maW5kX2Jsb2NrX2luX2NvbnRlbnSUk5Qu",
      "name": "_find_block_in_content"
     },
     "_get_line_indentation": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFV9nZXRfbGluZV9pbmRlbnRhdGlvbpSTlC4=",
      "name": "_get_line_indentation"
     },
     "_normalize_path": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMD19ub3JtYWxpemVfcGF0aJSTlC4=",
      "name": "_normalize_path"
     },
     "_normalize_header_lines": {
      "__helper_func__": true,
      "pickled": "gASVNQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMF19ub3JtYWxpemVfaGVhZGVyX2xpbmVzlJOULg==",
      "name": "_normalize_header_lines"
     }
    }
   },
   "dynamic_module_-7790325168722484441": {
    "name": "dynamic_module_-1755968106357101445",
    "source": "from bots.utils.helpers import _process_error\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef view_dir(start_path: str='.', output_file=None, target_extensions: str=\"['py', 'txt', 'md']\", max_lines: int=500):\n    \"\"\"\n    Creates a summary of the directory structure starting from the given path, writing only files\n    with specified extensions and showing venv directories without their contents.\n    Parameters:\n    - start_path (str): The root directory to start scanning from.\n    - output_file (str): The name of the file to optionally write the directory structure to.\n    - target_extensions (str): String representation of a list of file extensions (e.g. \"['py', 'txt']\").\n    - max_lines (int): Maximum number of lines in output. If exceeded, deepest levels are truncated. Default 500.\n    Returns:\n    str: A formatted string containing the directory structure, with each directory and file properly indented.\n    Example output:\n    my_project/\n        venv/\n        module1/\n            script.py\n            README.md\n        module2/\n            utils.py\n    cost: low\n    \"\"\"\n    if max_lines:\n        max_lines = int(max_lines)\n    extensions_list = [ext.strip().strip('\\'\"') for ext in target_extensions.strip('[]').split(',')]\n    extensions_list = ['.' + ext if not ext.startswith('.') else ext for ext in extensions_list]\n    dir_entries = []\n    max_level = 0\n    for root, dirs, files in os.walk(start_path):\n        level = root.replace(start_path, '').count(os.sep)\n        indent = '    ' * level\n        basename = os.path.basename(root)\n        is_venv = basename in ['venv', 'env', '.env'] or 'pyvenv.cfg' in files\n        if is_venv:\n            dir_entries.append((level, f'{indent}{basename}/'))\n            max_level = max(max_level, level)\n            dirs[:] = []\n            continue\n        has_relevant_files = False\n        for _, _, fs in os.walk(root):\n            if any((f.endswith(tuple(extensions_list)) for f in fs)):\n                has_relevant_files = True\n                break\n        if has_relevant_files:\n            dir_entries.append((level, f'{indent}{basename}/'))\n            max_level = max(max_level, level)\n            subindent = '    ' * (level + 1)\n            for file in files:\n                if file.endswith(tuple(extensions_list)):\n                    dir_entries.append((level + 1, f'{subindent}{file}'))\n                    max_level = max(max_level, level + 1)\n    current_level_limit = max_level\n    while current_level_limit >= 0:\n        filtered_entries = [(level, line) for level, line in dir_entries if level <= current_level_limit]\n        truncated_dirs = set()\n        for level, line in dir_entries:\n            if level > current_level_limit and level == current_level_limit + 1:\n                parent_level = level - 1\n                for parent_level_check, parent_line in filtered_entries:\n                    if parent_level_check == parent_level and parent_line.endswith('/'):\n                        truncated_dirs.add(parent_line)\n        final_entries = []\n        for level, line in filtered_entries:\n            final_entries.append(line)\n            if line in truncated_dirs:\n                truncation_indent = '    ' * (level + 1)\n                final_entries.append(f'{truncation_indent}...')\n        if len(final_entries) <= max_lines:\n            output_text = final_entries\n            break\n        current_level_limit -= 1\n    else:\n        output_text = [f'Project too large (>{max_lines} lines). Showing root level only:', f'{os.path.basename(start_path) or start_path}/', '    ...']\n    if output_file is not None:\n        with open(output_file, 'w') as file:\n            file.write('\\n'.join(output_text))\n    return '\\n'.join(output_text)",
    "file_path": "dynamic_module_-7790325168722484441",
    "code_hash": "be7fcc0369eec8cba8010d0867b80cd3",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "difflib": {
      "__module_type__": true,
      "name": "difflib",
      "spec": "difflib"
     },
     "_read_file_bom_safe": {
      "__helper_func__": true,
      "pickled": "gASVMQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SME19yZWFkX2ZpbGVfYm9tX3NhZmWUk5Qu",
      "name": "_read_file_bom_safe"
     },
     "_write_file_bom_safe": {
      "__helper_func__": true,
      "pickled": "gASVMgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFF93cml0ZV9maWxlX2JvbV9zYWZllJOULg==",
      "name": "_write_file_bom_safe"
     },
     "_remove_bom_from_content": {
      "__helper_func__": true,
      "pickled": "gASVNgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGF9yZW1vdmVfYm9tX2Zyb21fY29udGVudJSTlC4=",
      "name": "_remove_bom_from_content"
     },
     "_find_match_with_hierarchy": {
      "__helper_func__": true,
      "pickled": "gASVOAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGl9maW5kX21hdGNoX3dpdGhfaGllcmFyY2h5lJOULg==",
      "name": "_find_match_with_hierarchy"
     },
     "_check_exact_match_at_position": {
      "__helper_func__": true,
      "pickled": "gASVPAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMHl9jaGVja19leGFjdF9tYXRjaF9hdF9wb3NpdGlvbpSTlC4=",
      "name": "_check_exact_match_at_position"
     },
     "_find_exact_match_anywhere": {
      "__helper_func__": true,
      "pickled": "gASVOAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMGl9maW5kX2V4YWN0X21hdGNoX2FueXdoZXJllJOULg==",
      "name": "_find_exact_match_anywhere"
     },
     "_check_whitespace_match_at_position": {
      "__helper_func__": true,
      "pickled": "gASVQQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMI19jaGVja193aGl0ZXNwYWNlX21hdGNoX2F0X3Bvc2l0aW9ulJOULg==",
      "name": "_check_whitespace_match_at_position"
     },
     "_find_whitespace_match_anywhere": {
      "__helper_func__": true,
      "pickled": "gASVPQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMH19maW5kX3doaXRlc3BhY2VfbWF0Y2hfYW55d2hlcmWUk5Qu",
      "name": "_find_whitespace_match_anywhere"
     },
     "_adjust_additions_to_context": {
      "__helper_func__": true,
      "pickled": "gASVOgAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMHF9hZGp1c3RfYWRkaXRpb25zX3RvX2NvbnRleHSUk5Qu",
      "name": "_adjust_additions_to_context"
     },
     "_find_block_in_content": {
      "__helper_func__": true,
      "pickled": "gASVNAAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFl9maW5kX2Jsb2NrX2luX2NvbnRlbnSUk5Qu",
      "name": "_find_block_in_content"
     },
     "_get_line_indentation": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMFV9nZXRfbGluZV9pbmRlbnRhdGlvbpSTlC4=",
      "name": "_get_line_indentation"
     },
     "_normalize_path": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMD19ub3JtYWxpemVfcGF0aJSTlC4=",
      "name": "_normalize_path"
     },
     "_normalize_header_lines": {
      "__helper_func__": true,
      "pickled": "gASVNQAAAAAAAACMFWJvdHMudG9vbHMuY29kZV90b29sc5SMF19ub3JtYWxpemVfaGVhZGVyX2xpbmVzlJOULg==",
      "name": "_normalize_header_lines"
     }
    }
   },
   "dynamic_module_5152119155670391445": {
    "name": "dynamic_module_8296891091829095731",
    "source": "from bots.utils.helpers import _process_error\nfrom typing import Any, Callable\nfrom functools import wraps\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\ndef log_errors(func: Callable) -> Callable:\n    \"\"\"Decorator that logs errors to a file when a function raises an exception or returns an error message.\n\n    This decorator captures both exceptions and return values that contain \"Error\" in the first few words,\n    logging them to 'error_log.txt' in the same directory as this decorators file.\n\n    Parameters:\n        func (Callable): The function to wrap with error logging capabilities\n\n    Returns:\n        Callable: A wrapped version of the function that logs errors\n\n    Example:\n        @log_errors\n        def risky_function():\n            # Any exceptions or \"Error\" messages will be logged\n            return \"Error: Something went wrong\"\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        try:\n            result = func(*args, **kwargs)\n\n            # Check if result is a string that starts with \"Tool Failed:\" (the specific format from handle_errors)\n            if isinstance(result, str) and result.startswith(\"Tool Failed:\"):\n                _log_error_to_file(func.__name__, f\"Function returned error message: {result}\", args, kwargs)\n\n            return result\n\n        except Exception as e:\n            # Log the exception\n            error_msg = f\"Exception in {func.__name__}: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\n            _log_error_to_file(func.__name__, error_msg, args, kwargs)\n            # Re-raise the exception\n            raise\n\n    return wrapper\n\n@log_errors\n@toolify()\ndef execute_powershell(command: str, output_length_limit: str='1000', timeout: str='60') -> str:\n    \"\"\"\n    Executes PowerShell commands in a stateful environment with automatic BOM removal\n\n    Use when you need to run PowerShell commands and capture their output. If\n    you have other tools available, you should use this as a fallback when the\n    other tools fail. Coerces to utf-8 (no BOM).\n\n    Potential use cases:\n    - git commands\n    - gh cli\n    - other cli (which you may need to install using this tool)\n\n    Parameters:\n    - command (str): PowerShell command to execute.\n    - output_length_limit (int, optional): Maximum number of lines in the output.\n      If set, output exceeding this limit will be truncated. Default 200.\n\n    Returns:\n        str: The complete output from the command execution\n    \"\"\"\n    manager = PowerShellManager.get_instance()\n    output = ''.join(manager.execute(command, int(output_length_limit), float(timeout)))\n    return output",
    "file_path": "dynamic_module_5152119155670391445",
    "code_hash": "5c06ee0b81212ae0afa6700e2d86fe1f",
    "globals": {
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "codecs": {
      "__module_type__": true,
      "name": "codecs",
      "spec": "codecs"
     },
     "glob": {
      "__module_type__": true,
      "name": "glob",
      "spec": "glob"
     },
     "subprocess": {
      "__module_type__": true,
      "name": "subprocess",
      "spec": "subprocess"
     },
     "threading": {
      "__module_type__": true,
      "name": "threading",
      "spec": "threading"
     },
     "time": {
      "__module_type__": true,
      "name": "time",
      "spec": "time"
     },
     "_get_active_sessions": {
      "__helper_func__": true,
      "pickled": "gASVNgAAAAAAAACMGWJvdHMudG9vbHMudGVybWluYWxfdG9vbHOUjBRfZ2V0X2FjdGl2ZV9zZXNzaW9uc5STlC4=",
      "name": "_get_active_sessions"
     },
     "_process_commands": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMGWJvdHMudG9vbHMudGVybWluYWxfdG9vbHOUjBFfcHJvY2Vzc19jb21tYW5kc5STlC4=",
      "name": "_process_commands"
     },
     "_remove_bom_from_current_directory": {
      "__helper_func__": true,
      "pickled": "gASVRAAAAAAAAACMGWJvdHMudG9vbHMudGVybWluYWxfdG9vbHOUjCJfcmVtb3ZlX2JvbV9mcm9tX2N1cnJlbnRfZGlyZWN0b3J5lJOULg==",
      "name": "_remove_bom_from_current_directory"
     }
    }
   },
   "dynamic_module_458812868585942248": {
    "name": "dynamic_module_2393089463902401205",
    "source": "from bots.utils.helpers import _process_error\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef execute_python(code: str, timeout: int=300) -> str:\n    \"\"\"\n    Executes python code in a stateless environment with cross-platform\n    timeout handling.\n    Parameters:\n    - code (str): Syntactically correct python code\n    - timeout (int): Maximum execution time in seconds (default: 300)\n    Returns stdout or an error message.\n    cost: varies\n    \"\"\"\n\n    def create_wrapper_ast():\n        wrapper_code = textwrap.dedent('\\n            import os\\n            import sys\\n            import traceback\\n            import time\\n            sys.path.append(os.path.dirname(os.path.dirname(\\n                os.path.abspath(__file__))))\\n            if sys.platform == \\'win32\\':\\n                import codecs\\n                sys.stdout = codecs.getwriter(\\'utf-8\\')(\\n                    sys.stdout.buffer, \\'strict\\')\\n                sys.stderr = codecs.getwriter(\\'utf-8\\')(\\n                    sys.stderr.buffer, \\'strict\\')\\n            def main():\\n                pass  # Placeholder for user code\\n            if __name__ == \\'__main__\\':\\n                try:\\n                    main()\\n                except Exception as error:\\n                    print(f\"An error occurred: {str(error)}\", file=sys.stderr)\\n                    traceback.print_exc(file=sys.stderr)\\n                    sys.exit(1)\\n            ')\n        return ast.parse(wrapper_code)\n\n    def insert_code_into_wrapper(wrapper_ast, code_ast, timeout_value):\n\n        def is_main_func(node):\n            return isinstance(node, ast.FunctionDef) and node.name == 'main'\n        nodes = wrapper_ast.body\n        main_func = next((node for node in nodes if is_main_func(node)))\n        main_func.body = code_ast.body\n        return wrapper_ast\n    if not isinstance(timeout, int) or timeout <= 0:\n        raise ValueError('Timeout must be a positive integer')\n    clean_code = textwrap.dedent(code).strip()\n    code_ast = ast.parse(clean_code)\n    wrapper_ast = create_wrapper_ast()\n    combined_ast = insert_code_into_wrapper(wrapper_ast, code_ast, timeout)\n    final_code = _py_ast_to_source(combined_ast)\n    temp_file_name = os.path.join(tempfile.gettempdir(), _get_unique_filename('temp_script', 'py'))\n    with open(temp_file_name, 'w', encoding='utf-8') as temp_file:\n        temp_file.write(final_code)\n        temp_file.flush()\n        os.fsync(temp_file.fileno())\n    env = os.environ.copy()\n    env['PYTHONIOENCODING'] = 'utf-8'\n    process = subprocess.Popen(['python', temp_file_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0, env=env)\n    stdout, stderr = process.communicate(timeout=timeout)\n    if os.path.exists(temp_file_name):\n        os.remove(temp_file_name)\n    if process.returncode != 0:\n        return stderr or 'Process failed with no error message'\n    return stdout + stderr\n\ndef _get_unique_filename(base_name, extension=''):\n    \"\"\"Generate a unique filename using process ID and timestamp.\"\"\"\n    timestamp = int(time.time() * 1000)\n    pid = os.getpid()\n    if extension and (not extension.startswith('.')):\n        extension = '.' + extension\n    return f'{base_name}_{pid}_{timestamp}{extension}'",
    "file_path": "dynamic_module_458812868585942248",
    "code_hash": "4be9bde13f302a570977216c740ce4d2",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "subprocess": {
      "__module_type__": true,
      "name": "subprocess",
      "spec": "subprocess"
     },
     "tempfile": {
      "__module_type__": true,
      "name": "tempfile",
      "spec": "tempfile"
     },
     "time": {
      "__module_type__": true,
      "name": "time",
      "spec": "time"
     },
     "_py_ast_to_source": {
      "__helper_func__": true,
      "pickled": "gASVLAAAAAAAAACMEmJvdHMudXRpbHMuaGVscGVyc5SMEV9weV9hc3RfdG9fc291cmNllJOULg==",
      "name": "_py_ast_to_source"
     }
    }
   },
   "dynamic_module_6791270121340974809": {
    "name": "dynamic_module_6807399256454285814",
    "source": "import os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom bots.flows import functional_prompts as fp\nfrom bots.foundation.base import Bot\nimport traceback\nfrom bots.flows.recombinators import recombinators as recomb\nfrom bots.utils.helpers import _process_error\nfrom typing import List, Optional\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef branch_self(self_prompts: str, allow_work: str='False', parallel: str='False', recombine: str='concatenate') -> str:\n    \"\"\"Create multiple conversation branches to explore different approaches or tackle separate tasks.\n\n    Simplified approach that creates isolated conversation branches:\n    1. Save current bot state to temporary file\n    2. Execute each branch with independent bot instances\n    3. Stitch results back into main conversation tree\n    Args:\n        self_prompts (str): List of prompts as a string array, like ['task 1', 'task 2', 'task 3']\n                           Each prompt becomes a separate conversation branch\n        allow_work (str): 'True' to let each branch use tools and continue working until done\n                         'False' (default) for single-response branches\n        parallel (str): 'True' to let branches work in parallel, 'False' for sequential (default).\n        recombine (str): One of ('none', 'concatenate', 'llm_merge', 'llm_vote', 'llm_judge'), default 'concatenate'.\n                         Combines the final messages from each branch using that method.\n\n    Returns:\n        str: Success message with branch count, or error details if something went wrong\n    \"\"\"\n    import os\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    from bots.flows import functional_prompts as fp\n    from bots.foundation.base import Bot\n    bot = _get_calling_bot()\n    if not bot:\n        return 'Error: Could not find calling bot'\n    allow_work = allow_work.lower() == 'true'\n    parallel = parallel.lower() == 'true'\n    recombine = recombine.lower()\n    valid_recombine_options = ['none', 'concatenate', 'llm_judge', 'llm_vote', 'llm_merge']\n    if recombine not in valid_recombine_options:\n        return f'Error: Invalid recombine option. Valid: {valid_recombine_options}'\n    try:\n        message_list = _process_string_array(self_prompts)\n        if not message_list:\n            return 'Error: No valid messages provided'\n    except Exception as e:\n        return f'Error parsing prompts: {str(e)}'\n    prefixed_prompts = [f'(self-prompt): {prompt}' for prompt in message_list]\n    try:\n        original_node = bot.conversation\n        original_autosave = bot.autosave\n        bot.autosave = False\n        if original_node.tool_calls:\n            dummy_results = []\n            for tool_call in original_node.tool_calls:\n                if tool_call.get('name') == 'branch_self':\n                    dummy_result = {'tool_use_id': tool_call['id'], 'content': 'Branching in progress...'}\n                    dummy_results.append(dummy_result)\n            if dummy_results:\n                original_results = getattr(original_node, 'tool_results', [])\n                original_node._add_tool_results(dummy_results)\n                temp_id = str(uuid.uuid4())[:8]\n                temp_file = f'branch_self_{temp_id}.bot'\n                bot.save(temp_file)\n                original_node.tool_results = original_results\n            else:\n                temp_id = str(uuid.uuid4())[:8]\n                temp_file = f'branch_self_{temp_id}.bot'\n                bot.save(temp_file)\n        else:\n            temp_id = str(uuid.uuid4())[:8]\n            temp_file = f'branch_self_{temp_id}.bot'\n            bot.save(temp_file)\n\n        def execute_branch(prompt, parent_bot_node):\n            \"\"\"Execute a single branch with the given prompt.\"\"\"\n            try:\n                branch_bot = Bot.load(temp_file)\n                branch_bot.autosave = False\n                branching_node = branch_bot.conversation\n                if hasattr(branch_bot, 'tool_handler') and branch_bot.tool_handler:\n                    branch_bot.tool_handler.clear()\n                if allow_work:\n                    response = branch_bot.respond(prompt)\n                    while not fp.conditions.tool_not_used(branch_bot):\n                        response = branch_bot.respond('ok')\n                else:\n                    response = branch_bot.respond(prompt)\n                parent_bot_node.replies.extend(branching_node.replies)\n                for node in branching_node.replies:\n                    node.parent = parent_bot_node\n                return (response, branch_bot.conversation)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                return (None, None)\n        responses = []\n        branch_nodes = []\n        if parallel:\n            with ThreadPoolExecutor() as executor:\n                futures = [executor.submit(execute_branch, prompt, original_node) for prompt in prefixed_prompts]\n                for future in as_completed(futures):\n                    response, node = future.result()\n                    responses.append(response)\n                    branch_nodes.append(node)\n        else:\n            for prompt in prefixed_prompts:\n                response, node = execute_branch(prompt, original_node)\n                responses.append(response)\n                branch_nodes.append(node)\n        bot.autosave = original_autosave\n        try:\n            os.remove(temp_file)\n        except OSError:\n            pass\n        success_count = sum((1 for r in responses if r is not None))\n        combined = 'No recombinator selected'\n        if recombine != 'none' and responses:\n            valid_responses = [r for r in responses if r is not None]\n            if valid_responses:\n                from bots.flows.recombinators import recombinators as recomb\n                if recombine == 'concatenate':\n                    combined, _ = recomb.concatenate(valid_responses, [])\n                elif recombine == 'llm_judge':\n                    combined, _ = recomb.llm_judge(valid_responses, [], judge_bot=bot)\n                elif recombine == 'llm_vote':\n                    combined, _ = recomb.llm_vote(valid_responses, [], vote_bot=bot)\n                elif recombine == 'llm_merge':\n                    combined, _ = recomb.llm_merge(valid_responses, [], merge_bot=bot)\n                else:\n                    combined = 'Unknown recombination method'\n        exec_type = 'parallel' if parallel else 'sequential'\n        work_type = 'iterative' if allow_work else 'single-response'\n        result_content = f'Successfully completed {success_count}/{len(message_list)} {exec_type} {work_type} branches. Recombination result:\\n\\n{combined}'\n        return result_content\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return f'Error in branch_self: {str(e)}'\n\ndef _get_calling_bot() -> Optional[Bot]:\n    \"\"\"Helper function to get a reference to the calling bot.\n    Returns:\n        Optional[Bot]: Reference to the calling bot or None if not found\n    \"\"\"\n    frame = inspect.currentframe()\n    while frame:\n        if frame.f_code.co_name == '_cvsn_respond' and 'self' in frame.f_locals:\n            potential_bot = frame.f_locals['self']\n            if isinstance(potential_bot, Bot):\n                return potential_bot\n        frame = frame.f_back\n    return None\n\ndef _process_string_array(input_str: str) -> List[str]:\n    \"\"\"Parse a string representation of an array into a list of strings.\n    Only works with properly formatted Python list literals.\n    Args:\n        input_str (str): String representation of a Python list literal\n    Returns:\n        List[str]: List of parsed strings\n    Raises:\n        ValueError: If the input is not a valid Python list literal\n    \"\"\"\n    result = ast.literal_eval(input_str)\n    if not isinstance(result, list) or not all((isinstance(x, str) for x in result)):\n        raise ValueError('Input must evaluate to a list of strings')\n    return result",
    "file_path": "dynamic_module_6791270121340974809",
    "code_hash": "246de2e7c30345618d3cb0748630bada",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "json": {
      "__module_type__": true,
      "name": "json",
      "spec": "json"
     },
     "uuid": {
      "__module_type__": true,
      "name": "uuid",
      "spec": "uuid"
     },
     "_modify_own_settings": {
      "__helper_func__": true,
      "pickled": "gASVMgAAAAAAAACMFWJvdHMudG9vbHMuc2VsZl90b29sc5SMFF9tb2RpZnlfb3duX3NldHRpbmdzlJOULg==",
      "name": "_modify_own_settings"
     },
     "_verbose_callback": {
      "__helper_func__": true,
      "pickled": "gASVLwAAAAAAAACMFWJvdHMudG9vbHMuc2VsZl90b29sc5SMEV92ZXJib3NlX2NhbGxiYWNrlJOULg==",
      "name": "_verbose_callback"
     },
     "_remove_dummy_from_tree": {
      "__helper_func__": true,
      "pickled": "gASVNQAAAAAAAACMFWJvdHMudG9vbHMuc2VsZl90b29sc5SMF19yZW1vdmVfZHVtbXlfZnJvbV90cmVllJOULg==",
      "name": "_remove_dummy_from_tree"
     }
    }
   },
   "dynamic_module_3081067974357524236": {
    "name": "dynamic_module_-946339205871689114",
    "source": "from bots.utils.helpers import _process_error\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify(\"Perform an agentic web search using Claude's internal web search capabilities\")\ndef web_search(question: str) -> str:\n    \"\"\"Perform an intelligent web search and return organized results.\n\n    This tool uses Claude's built-in web search functionality to perform searches\n    and returns organized, pertinent information extracted from the results.\n\n    Args:\n        question (str): The search goal framed as a question.\n        Good examples:\n            \"Who is Ben Rinauto?\"\n            \"What is the connection between Paris, France and Buffalo, NY?\"\n        Bad examples:\n            \"Ben R wrestling\"\n            \"paris buffalo\"\n\n    Returns:\n        str: Organized search results with key information extracted\n    \"\"\"\n    try:\n        _validate_question_format(question)\n        api_key = os.getenv('ANTHROPIC_API_KEY')\n        if not api_key:\n            return 'Error: ANTHROPIC_API_KEY environment variable not set'\n        client = anthropic.Anthropic(api_key=api_key)\n        tools = [{'type': 'web_search_20250305', 'name': 'web_search', 'max_uses': 10}]\n        search_prompt = f'I need you to search the web answers to this question: \"{question}\"\\n\\n        Please use the web search tool to find current, relevant information to precisely and\\n        accurately answer this question.'\n        response = client.messages.create(model='claude-opus-4-1', max_tokens=16384, temperature=0.3, tools=tools, messages=[{'role': 'user', 'content': search_prompt}])\n\n        def extract_clean_content(response):\n            \"\"\"Extract only the essential content from web search response.\"\"\"\n            result = []\n            result.append('=== WEB SEARCH RESULTS ===\\n')\n            result.append(f'Search: \"{question}\"\\n')\n            if hasattr(response, 'content') and response.content:\n                text_responses = []\n                search_results = []\n                for block in response.content:\n                    if hasattr(block, 'type') and block.type == 'text' and hasattr(block, 'text') and block.text:\n                        text_responses.append(block.text)\n                    elif hasattr(block, 'type') and block.type == 'web_search_tool_result':\n                        if hasattr(block, 'content') and block.content:\n                            for item in block.content:\n                                if isinstance(item, dict) and item.get('type') == 'web_search_result':\n                                    search_results.append({'title': item.get('title', 'No title'), 'url': item.get('url', 'No URL'), 'age': item.get('page_age', 'Unknown age')})\n                if text_responses:\n                    result.append(\"=== CLAUDE'S ANALYSIS ===\\n\")\n                    for i, text in enumerate(text_responses, 1):\n                        result.append(f'{i}. {text}\\n')\n                    result.append('')\n                if search_results:\n                    result.append('=== SOURCES FOUND ===\\n')\n                    for i, item in enumerate(search_results, 1):\n                        result.append(f\"{i}. {item['title']}\")\n                        result.append(f\"   {item['url']}\")\n                        if item['age'] and item['age'] != 'Unknown age':\n                            result.append(f\"   ({item['age']})\")\n                        result.append('')\n                result.append(f\"Search performed: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M UTC')}\")\n            return '\\n'.join(result)\n        clean_content = extract_clean_content(response)\n        return clean_content\n    except Exception as e:\n        return f'Web search failed: {str(e)}'\n\ndef _validate_question_format(query: str) -> bool:\n    \"\"\"Use a simple Haiku bot to validate if input is formatted as a question.\n\n    Args:\n        query (str): The input query to validate\n\n    Returns:\n        bool: True if formatted as a question, False otherwise\n\n    Raises:\n        ValueError: If query is not formatted as a question\n    \"\"\"\n    try:\n        api_key = os.getenv('ANTHROPIC_API_KEY')\n        if not api_key:\n            return True\n        client = anthropic.Anthropic(api_key=api_key)\n        validation_prompt = f'Is this text formatted as a question? Answer only Y or N.\\n\\nText: \"{query}\"\\n\\nAnswer:'\n        response = client.messages.create(model='claude-3-haiku-20240307', max_tokens=10, temperature=0.0, messages=[{'role': 'user', 'content': validation_prompt}])\n        answer = response.content[0].text.strip().upper()\n        if answer.startswith('N'):\n            raise ValueError(\"Input queries must be formatted as questions. Good examples: 'Who is...?', 'What is...?', 'How does...?' Bad examples: 'Ben R wrestling', 'paris buffalo'The more information, and the more specific, the better. You are asking an AI to answer a question, not performing a traditional web keyword search.\")\n        return True\n    except ValueError:\n        raise\n    except Exception:\n        return True",
    "file_path": "dynamic_module_3081067974357524236",
    "code_hash": "da6cd7163e652f16f8b0ad248f04a418",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "anthropic": {
      "__module_type__": true,
      "name": "anthropic",
      "spec": "anthropic"
     }
    }
   },
   "dynamic_module_7219771540807829092": {
    "name": "dynamic_module_-2154381783122948247",
    "source": "import textwrap\nfrom bots.utils.helpers import _process_error\nfrom typing import List\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef python_edit(target_scope: str, code: str, *, coscope_with: str=None, delete_a_lot: bool=False) -> str:\n    \"\"\"\n        Edit Python code using pytest-style scope syntax and optional expression matching.\n\n        Parameters:\n        -----------\n        target_scope : str\n            Location to edit in pytest-style scope syntax:\n            - \"file.py\" (whole file)\n            - \"file.py::MyClass\" (class)\n            - \"file.py::my_function\" (function)\n            - \"file.py::MyClass::method\" (method)\n\n        code : str\n            Python code. This code will replace the entire target scope by default,\n            or insert after (if coscope_with is specified). Code is indented automatically\n            to match to scope.\n\n        coscope_with : str, optional\n            If specified, code is inserted in the same scope as the specified code,\n            and immediately after it. Uses the same syntax as target_scope with\n            optional expression at the end in quotes.\n\n\n                Scope:\n                    - a\n                    - b\n\n                - python edit called with coscope_with = \"a\", code = \"- c\" -\n\n                Scope:\n                    - a\n                    - c\n                    - b\n\n            - \"__FILE_START__\" (special token for file beginning)\n            - \"MyClass::method\" (insert after this method within the target scope)\n            - '\"expression\"' (insert after a line matching this expression)\n        delete_a_lot : bool, optional\n    - \"__FILE_END__\" (special token for file end)\n            Safety parameter. Must be True to allow operations that delete more than 100 lines.\n            Helps prevent accidental file overwrites. Default False.\n\n        Returns:\n        --------\n        str\n            Description of what was modified or error message\n    \"\"\"\n    try:\n        file_path, *path_elements = target_scope.split('::')\n        if not file_path.endswith('.py'):\n            return _process_error(ValueError(f'File path must end with .py: {file_path}'))\n        for element in path_elements:\n            if not element.isidentifier():\n                return _process_error(ValueError(f'Invalid identifier in path: {element}'))\n        abs_path = _make_file(file_path)\n        try:\n            original_content = _read_file_bom_safe(abs_path)\n            was_originally_empty = not original_content.strip()\n        except Exception as e:\n            return _process_error(ValueError(f'Error reading file {abs_path}: {str(e)}'))\n        try:\n            if original_content.strip():\n                tree = cst.parse_module(original_content)\n            else:\n                tree = cst.parse_module('')\n        except Exception as e:\n            return _process_error(ValueError(f'Error parsing file {abs_path}: {str(e)}'))\n        try:\n            import textwrap\n            cleaned_code = textwrap.dedent(code).strip()\n            if not cleaned_code:\n                if coscope_with:\n                    return _process_error(ValueError('Cannot use empty code with insert_after - nothing to insert'))\n                return _handle_deletion(abs_path, target_scope, path_elements, original_content, tree, delete_a_lot)\n            elif was_originally_empty and (not path_elements):\n                _write_file_bom_safe(abs_path, cleaned_code)\n                return f\"Code added to '{abs_path}'.\"\n            new_module = None\n            if cleaned_code:\n                try:\n                    new_module = cst.parse_module(cleaned_code)\n                except Exception as e:\n                    if cleaned_code.strip().startswith('#'):\n                        comment_stmt = _create_statement_with_comment(cleaned_code.strip())\n                        new_module = cst.Module(body=[comment_stmt])\n                    else:\n                        return _process_error(ValueError(f'Error parsing new code: {str(e)}'))\n        except Exception as e:\n            return _process_error(ValueError(f'Error processing new code: {str(e)}'))\n        if coscope_with == '__FILE_START__':\n            return _handle_file_start_insertion(abs_path, tree, new_module)\n        elif coscope_with == '__FILE_END__':\n            return _handle_file_end_insertion(abs_path, tree, new_module)\n        elif not path_elements:\n            if coscope_with:\n                return _handle_file_level_insertion(abs_path, tree, new_module, coscope_with)\n            else:\n                lines_to_delete = _count_lines_to_be_deleted(original_content, cleaned_code)\n                if lines_to_delete > 100 and (not delete_a_lot):\n                    return _process_error(ValueError(f'Safety check: this operation would delete {lines_to_delete} lines. ' + \"If intentional, set delete_a_lot=True. Consider using 'insert_after' parameter to add code without deleting existing content.\"))\n                _write_file_bom_safe(abs_path, cleaned_code)\n                return f\"Code replaced at file level in '{abs_path}'.\"\n        else:\n            replacer = ScopeReplacer(path_elements, new_module, coscope_with, tree)\n            modified_tree = tree.visit(replacer)\n            if not replacer.modified:\n                if coscope_with:\n                    return _process_error(ValueError(f'Insert point not found: {coscope_with}'))\n                else:\n                    return _process_error(ValueError(f'Target scope not found: {target_scope}'))\n            _write_file_bom_safe(abs_path, modified_tree.code)\n            if coscope_with:\n                return f\"Code inserted after '{coscope_with}' in '{abs_path}'.\"\n            else:\n                return f\"Code replaced at '{target_scope}'.\"\n    except Exception as e:\n        return _process_error(e)\n\ndef _handle_file_end_insertion(abs_path: str, tree: cst.Module, new_module: cst.Module) -> str:\n    \"\"\"Handle insertion at the end of a file.\"\"\"\n    if len(new_module.body) == 0 and new_module.code.strip():\n        comment_stmt = _create_statement_with_comment(new_module.code.strip())\n        new_body = list(tree.body) + [comment_stmt]\n    else:\n        new_body = list(tree.body) + list(new_module.body)\n    modified_tree = tree.with_changes(body=new_body)\n    final_code = modified_tree.code\n    if final_code and (not final_code.endswith('\\n')):\n        final_code += '\\n'\n    _write_file_bom_safe(abs_path, final_code)\n    return f\"Code inserted at end of '{abs_path}'.\"\n\ndef _handle_file_start_insertion(abs_path: str, tree: cst.Module, new_module: cst.Module) -> str:\n    \"\"\"Handle insertion at the beginning of a file.\"\"\"\n    new_body = list(new_module.body) + list(tree.body)\n    modified_tree = tree.with_changes(body=new_body)\n    _write_file_bom_safe(abs_path, modified_tree.code)\n    return f\"Code inserted at start of '{abs_path}'.\"\n\ndef _count_lines_to_be_deleted(original_content: str, new_content: str) -> int:\n    \"\"\"\n    Count how many lines would be deleted by comparing original vs new content.\n\n    Parameters:\n    -----------\n    original_content : str\n        The original file content\n    new_content : str\n        The new content that would replace it\n\n    Returns:\n    --------\n    int\n        Number of lines that would be deleted\n    \"\"\"\n    original_lines = len(original_content.splitlines()) if original_content.strip() else 0\n    new_lines = len(new_content.splitlines()) if new_content.strip() else 0\n    return max(0, original_lines - new_lines)\n\ndef _create_statement_with_comment(comment_text: str, indent_level: int=0) -> cst.SimpleStatementLine:\n    \"\"\"\n    Create a statement that contains just a comment.\n    Since LibCST requires statements to have actual code, we create a pass statement\n    with a trailing comment.\n    \"\"\"\n    lines = comment_text.strip().split('\\n')\n    if len(lines) > 1:\n        comment_text = lines[0]\n    comment_text = comment_text.strip()\n    if comment_text.startswith('#'):\n        comment_text = comment_text[1:].strip()\n    comment = cst.Comment(f'# {comment_text}')\n    return cst.SimpleStatementLine(body=[cst.Pass()], trailing_whitespace=cst.TrailingWhitespace(whitespace=cst.SimpleWhitespace('  '), comment=comment))\n\ndef _write_file_bom_safe(file_path: str, content: str) -> None:\n    \"\"\"Write a file with BOM protection.\"\"\"\n    ends_with_newline = content.endswith('\\n')\n    clean_content = clean_unicode_string(content)\n    if ends_with_newline and (not clean_content.endswith('\\n')):\n        clean_content += '\\n'\n    with open(file_path, 'w', encoding='utf-8') as file:\n        file.write(clean_content)\n\ndef _handle_file_level_insertion(abs_path: str, tree: cst.Module, new_module: cst.Module, insert_after: str) -> str:\n    \"\"\"Handle insertion at file level after a specific pattern.\"\"\"\n    pattern = insert_after.strip()\n    if pattern.startswith('\"') and pattern.endswith('\"') or (pattern.startswith(\"'\") and pattern.endswith(\"'\")):\n        pattern = pattern[1:-1]\n\n    class FileOnlyInserter(cst.CSTTransformer):\n\n        def __init__(self, pattern: str, new_nodes: List[cst.CSTNode]):\n            self.pattern = pattern.strip()\n            self.new_nodes = new_nodes\n            self.modified = False\n\n        def leave_Module(self, original_node: cst.Module, updated_node: cst.Module) -> cst.Module:\n            new_body = []\n            for stmt in updated_node.body:\n                new_body.append(stmt)\n                match_found = False\n                if isinstance(stmt, (cst.FunctionDef, cst.ClassDef)) and stmt.name.value == self.pattern:\n                    match_found = True\n                else:\n                    try:\n                        stmt_code = updated_node.code_for_node(stmt).strip()\n                        if self.pattern in stmt_code:\n                            match_found = True\n                    except Exception:\n                        pass\n                if match_found:\n                    new_body.extend(self.new_nodes)\n                    self.modified = True\n            if self.modified:\n                return updated_node.with_changes(body=new_body)\n            return updated_node\n    inserter = FileOnlyInserter(pattern, new_module.body)\n    modified_tree = tree.visit(inserter)\n    if not inserter.modified:\n        return _process_error(ValueError(f'Insert point not found at file level: {insert_after}'))\n    _write_file_bom_safe(abs_path, modified_tree.code)\n    return f\"Code inserted after '{insert_after}' in '{abs_path}'.\"\n\ndef _read_file_bom_safe(file_path: str) -> str:\n    \"\"\"Read a file with BOM protection.\"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return clean_unicode_string(content)\n\ndef _handle_deletion(abs_path: str, target_scope: str, path_elements: list, original_content: str, tree: cst.Module, delete_a_lot: bool=False) -> str:\n    \"\"\"Handle deletion of a target section when empty code is provided.\"\"\"\n    try:\n        lines_to_delete = _count_lines_to_be_deleted(original_content, '')\n        if lines_to_delete > 100 and (not delete_a_lot):\n            return _process_error(ValueError(f'Safety check: this operation would delete {lines_to_delete} lines. ' + \"If intentional, set delete_a_lot=True. Consider using 'insert_after' parameter to add code without deleting existing content.\"))\n        if not path_elements:\n            _write_file_bom_safe(abs_path, '')\n            return f\"File '{abs_path}' cleared (deleted all content).\"\n        else:\n            deleter = ScopeDeleter(path_elements)\n            modified_tree = tree.visit(deleter)\n            if not deleter.modified:\n                return _process_error(ValueError(f'Target scope not found for deletion: {target_scope}'))\n            _write_file_bom_safe(abs_path, modified_tree.code)\n            return f\"Deleted scope '{target_scope}' from '{abs_path}'.\"\n    except Exception as e:\n        return _process_error(e)\n\ndef _make_file(file_path: str) -> str:\n    \"\"\"\n    Create a file and its parent directories if they don't exist.\n\n    Parameters:\n    -----------\n    file_path : str\n        Path to the file to create\n\n    Returns:\n    --------\n    str\n        Absolute path to the created/existing file\n\n    Raises:\n    -------\n    ValueError\n        If there's an error creating the file or directories\n    \"\"\"\n    if not file_path:\n        raise ValueError('File path cannot be empty')\n    abs_path = os.path.abspath(file_path)\n    dir_path = os.path.dirname(abs_path)\n    if dir_path:\n        try:\n            os.makedirs(dir_path, exist_ok=True)\n        except Exception as e:\n            raise ValueError(f'Error creating directories {dir_path}: {str(e)}')\n    if not os.path.exists(abs_path):\n        try:\n            with open(abs_path, 'w', encoding='utf-8') as f:\n                f.write('')\n        except Exception as e:\n            raise ValueError(f'Error creating file {abs_path}: {str(e)}')\n    return abs_path",
    "file_path": "dynamic_module_7219771540807829092",
    "code_hash": "d32c06f3c3098327ee487e0a7b8369ad",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "cst": {
      "__module_type__": true,
      "name": "libcst",
      "spec": "libcst"
     },
     "_process_error": {
      "__helper_func__": true,
      "pickled": "gASVKQAAAAAAAACMEmJvdHMudXRpbHMuaGVscGVyc5SMDl9wcm9jZXNzX2Vycm9ylJOULg==",
      "name": "_process_error"
     },
     "_py_ast_to_source": {
      "__helper_func__": true,
      "pickled": "gASVLAAAAAAAAACMEmJvdHMudXRpbHMuaGVscGVyc5SMEV9weV9hc3RfdG9fc291cmNllJOULg==",
      "name": "_py_ast_to_source"
     },
     "_apply_scope_aware_truncation": {
      "__helper_func__": true,
      "pickled": "gASVPAAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjB1fYXBwbHlfc2NvcGVfYXdhcmVfdHJ1bmNhdGlvbpSTlC4=",
      "name": "_apply_scope_aware_truncation"
     },
     "_collect_scope_entries": {
      "__helper_func__": true,
      "pickled": "gASVNQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBZfY29sbGVjdF9zY29wZV9lbnRyaWVzlJOULg==",
      "name": "_collect_scope_entries"
     },
     "_create_outline_view": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBRfY3JlYXRlX291dGxpbmVfdmlld5STlC4=",
      "name": "_create_outline_view"
     }
    }
   },
   "dynamic_module_-2060446122594093304": {
    "name": "dynamic_module_-4485579323350813411",
    "source": "import ast\nfrom bots.utils.helpers import _process_error\n\ndef toolify(description: str = None):\n    \"\"\"\n    Convert any function into a bot tool with string-in, string-out interface.\n\n    - Converts string inputs to proper types using type hints\n    - Ensures string output (JSON for complex types)\n    - Wraps in error handling (returns error strings, never raises)\n    - Enhances docstring if description provided\n\n    Args:\n        description (str, optional): Override the function's docstring\n\n    Example:\n        @toolify()\n        def add_numbers(x: int, y: int = 0) -> int:\n            return x + y\n\n        @toolify(\"Calculate the area of a circle\")\n        def area(radius: float) -> float:\n            return 3.14159 * radius * radius\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                # Convert string inputs to proper types using type hints\n                converted_args, converted_kwargs = _convert_tool_inputs(func, args, kwargs)\n\n                # Call original function\n                result = func(*converted_args, **converted_kwargs)\n\n                # Convert result to string\n                return _convert_tool_output(result)\n\n            except KeyboardInterrupt as e:\n                # Convert KeyboardInterrupt to ToolExecutionError to prevent it from\n                # bubbling up to CLI and being treated as user Ctrl+C\n                from bots.utils.helpers import _process_error\n\n                tool_error = ToolExecutionError(f\"Tool execution interrupted: {str(e)}\")\n                return _process_error(tool_error)\n\n            except TypeError as e:\n                # Check if this is a missing required argument error\n                error_msg = str(e)\n                if \"missing\" in error_msg and \"required\" in error_msg and \"argument\" in error_msg:\n                    # Add special message for output token limitation\n                    enhanced_error = TypeError(\n                        f\"{error_msg} - this is usually due to running out of output tokens \"\n                        f\"before finishing the message. Try making the function parameters shorter.\"\n                    )\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(enhanced_error)\n                else:\n                    from bots.utils.helpers import _process_error\n\n                    return _process_error(e)\n            except Exception as e:\n                from bots.utils.helpers import _process_error\n\n                return _process_error(e)\n\n        # Update docstring if description provided\n        if description:\n            wrapper.__doc__ = description\n        elif not wrapper.__doc__:\n            # Generate minimal docstring from function name\n            wrapper.__doc__ = f\"Execute {func.__name__.replace('_', ' ')}\"\n\n        return wrapper\n\n    return decorator\n\n@toolify()\ndef python_view(target_scope: str, max_lines: str='500') -> str:\n    \"\"\"\n    View Python code using pytest-style scope syntax with scope-aware truncation.\n\n    Parameters:\n    -----------\n    target_scope : str\n        Location to view in pytest-style scope syntax:\n        - \"file.py\" (whole file)\n        - \"file.py::MyClass\" (class)\n        - \"file.py::my_function\" (function)\n        - \"file.py::MyClass::method\" (method)\n    max_lines : int, optional\n        Maximum number of lines in output. If exceeded, deeper scopes will be\n        progressively truncated to create an outline view. Default 500.\n\n    Returns:\n    --------\n    str\n        The source code at the specified scope, or error message\n    \"\"\"\n    max_lines = int(max_lines)\n    try:\n        file_path, *path_elements = target_scope.split('::')\n        if not file_path.endswith('.py'):\n            return _process_error(ValueError(f'File path must end with .py: {file_path}'))\n        for element in path_elements:\n            if not element.isidentifier():\n                return _process_error(ValueError(f'Invalid identifier in path: {element}'))\n        abs_path = os.path.abspath(file_path)\n        if not os.path.exists(abs_path):\n            return _process_error(FileNotFoundError(f'File not found: {abs_path}'))\n        try:\n            source_code = _read_file_bom_safe(abs_path)\n            if not source_code.strip():\n                return f\"File '{abs_path}' is empty.\"\n        except Exception as e:\n            return _process_error(ValueError(f'Error reading file {abs_path}: {str(e)}'))\n        if not path_elements:\n            if max_lines and max_lines > 0:\n                return _apply_scope_aware_truncation(source_code, max_lines)\n            return source_code\n        try:\n            wrapper = cst.MetadataWrapper(cst.parse_module(source_code))\n        except Exception as e:\n            return _process_error(ValueError(f'Error parsing file {abs_path}: {str(e)}'))\n        finder = ScopeFinder(path_elements)\n        wrapper.visit(finder)\n        if not finder.target_node:\n            return _process_error(ValueError(f'Target scope not found: {target_scope}'))\n        result_code = wrapper.module.code_for_node(finder.target_node)\n        if max_lines and max_lines > 0:\n            return _apply_scope_aware_truncation(result_code, max_lines)\n        return result_code\n    except Exception as e:\n        return _process_error(e)\n\ndef _apply_scope_aware_truncation(source_code: str, max_lines: int) -> str:\n    \"\"\"Apply scope-aware truncation to Python source code.\"\"\"\n    if not source_code.strip():\n        return source_code\n    lines = source_code.splitlines()\n    if len(lines) <= max_lines:\n        return source_code\n    import ast\n    tree = ast.parse(source_code)\n    scope_entries = []\n    _collect_scope_entries(tree, scope_entries, 0)\n    max_depth = max((entry['depth'] for entry in scope_entries), default=0)\n    for current_depth_limit in range(max_depth, -1, -1):\n        result_lines = _create_outline_view(scope_entries, current_depth_limit, lines)\n        if len(result_lines) <= max_lines:\n            return '\\n'.join(result_lines)\n    result_lines = _create_outline_view(scope_entries, 0, lines)\n    if len(result_lines) > max_lines:\n        result_lines = result_lines[:max_lines]\n    return '\\n'.join(result_lines)\n\ndef _read_file_bom_safe(file_path: str) -> str:\n    \"\"\"Read a file with BOM protection.\"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return clean_unicode_string(content)",
    "file_path": "dynamic_module_-2060446122594093304",
    "code_hash": "449fe7af816bb40e0b1e069d1b65eed0",
    "globals": {
     "_convert_tool_inputs": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX2lucHV0c5STlC4=",
      "name": "_convert_tool_inputs"
     },
     "_convert_tool_output": {
      "__helper_func__": true,
      "pickled": "gASVMAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBRfY29udmVydF90b29sX291dHB1dJSTlC4=",
      "name": "_convert_tool_output"
     },
     "ast": {
      "__module_type__": true,
      "name": "ast",
      "spec": "ast"
     },
     "datetime": {
      "__module_type__": true,
      "name": "datetime",
      "spec": "datetime"
     },
     "functools": {
      "__module_type__": true,
      "name": "functools",
      "spec": "functools"
     },
     "inspect": {
      "__module_type__": true,
      "name": "inspect",
      "spec": "inspect"
     },
     "logging": {
      "__module_type__": true,
      "name": "logging",
      "spec": "logging"
     },
     "os": {
      "__module_type__": true,
      "name": "os",
      "spec": "os"
     },
     "sys": {
      "__module_type__": true,
      "name": "sys",
      "spec": "sys"
     },
     "textwrap": {
      "__module_type__": true,
      "name": "textwrap",
      "spec": "textwrap"
     },
     "traceback": {
      "__module_type__": true,
      "name": "traceback",
      "spec": "traceback"
     },
     "_get_context": {
      "__helper_func__": true,
      "pickled": "gASVKAAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjAxfZ2V0X2NvbnRleHSUk5Qu",
      "name": "_get_context"
     },
     "_get_py_interface": {
      "__helper_func__": true,
      "pickled": "gASVLQAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBFfZ2V0X3B5X2ludGVyZmFjZZSTlC4=",
      "name": "_get_py_interface"
     },
     "_convert_string_to_type": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBdfY29udmVydF9zdHJpbmdfdG9fdHlwZZSTlC4=",
      "name": "_convert_string_to_type"
     },
     "_log_error_to_file": {
      "__helper_func__": true,
      "pickled": "gASVLgAAAAAAAACME2JvdHMuZGV2LmRlY29yYXRvcnOUjBJfbG9nX2Vycm9yX3RvX2ZpbGWUk5Qu",
      "name": "_log_error_to_file"
     },
     "cst": {
      "__module_type__": true,
      "name": "libcst",
      "spec": "libcst"
     },
     "_process_error": {
      "__helper_func__": true,
      "pickled": "gASVKQAAAAAAAACMEmJvdHMudXRpbHMuaGVscGVyc5SMDl9wcm9jZXNzX2Vycm9ylJOULg==",
      "name": "_process_error"
     },
     "_py_ast_to_source": {
      "__helper_func__": true,
      "pickled": "gASVLAAAAAAAAACMEmJvdHMudXRpbHMuaGVscGVyc5SMEV9weV9hc3RfdG9fc291cmNllJOULg==",
      "name": "_py_ast_to_source"
     },
     "_make_file": {
      "__helper_func__": true,
      "pickled": "gASVKQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjApfbWFrZV9maWxllJOULg==",
      "name": "_make_file"
     },
     "_write_file_bom_safe": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBRfd3JpdGVfZmlsZV9ib21fc2FmZZSTlC4=",
      "name": "_write_file_bom_safe"
     },
     "_count_lines_to_be_deleted": {
      "__helper_func__": true,
      "pickled": "gASVOQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBpfY291bnRfbGluZXNfdG9fYmVfZGVsZXRlZJSTlC4=",
      "name": "_count_lines_to_be_deleted"
     },
     "_handle_file_end_insertion": {
      "__helper_func__": true,
      "pickled": "gASVOQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBpfaGFuZGxlX2ZpbGVfZW5kX2luc2VydGlvbpSTlC4=",
      "name": "_handle_file_end_insertion"
     },
     "_create_statement_with_comment": {
      "__helper_func__": true,
      "pickled": "gASVPQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjB5fY3JlYXRlX3N0YXRlbWVudF93aXRoX2NvbW1lbnSUk5Qu",
      "name": "_create_statement_with_comment"
     },
     "_collect_scope_entries": {
      "__helper_func__": true,
      "pickled": "gASVNQAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBZfY29sbGVjdF9zY29wZV9lbnRyaWVzlJOULg==",
      "name": "_collect_scope_entries"
     },
     "_create_outline_view": {
      "__helper_func__": true,
      "pickled": "gASVMwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBRfY3JlYXRlX291dGxpbmVfdmlld5STlC4=",
      "name": "_create_outline_view"
     },
     "_handle_file_start_insertion": {
      "__helper_func__": true,
      "pickled": "gASVOwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBxfaGFuZGxlX2ZpbGVfc3RhcnRfaW5zZXJ0aW9ulJOULg==",
      "name": "_handle_file_start_insertion"
     },
     "_handle_file_level_insertion": {
      "__helper_func__": true,
      "pickled": "gASVOwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBxfaGFuZGxlX2ZpbGVfbGV2ZWxfaW5zZXJ0aW9ulJOULg==",
      "name": "_handle_file_level_insertion"
     },
     "_handle_deletion": {
      "__helper_func__": true,
      "pickled": "gASVLwAAAAAAAACMFmJvdHMudG9vbHMucHl0aG9uX2VkaXSUjBBfaGFuZGxlX2RlbGV0aW9ulJOULg==",
      "name": "_handle_deletion"
     }
    }
   }
  },
  "function_paths": {
   "view": "dynamic_module_5245363458925641105",
   "view_dir": "dynamic_module_-7790325168722484441",
   "execute_powershell": "dynamic_module_5152119155670391445",
   "execute_python": "dynamic_module_458812868585942248",
   "branch_self": "dynamic_module_6791270121340974809",
   "web_search": "dynamic_module_3081067974357524236",
   "python_edit": "dynamic_module_7219771540807829092",
   "python_view": "dynamic_module_-2060446122594093304"
  }
 },
 "autosave": true,
 "bot_class": "AnthropicBot"
}